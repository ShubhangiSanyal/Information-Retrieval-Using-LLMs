Okay, so I want to start by showing you some python code about how decision trees are used in the Scikitlearn library. So it should tell you at what level comfort you need to know about decision. So let me just try. And. So is this readable or should I make it a little bigger? It's visible, sir. It's readable, sir. Okay, so this is, if you remember when we were looking at the second lecture of decision trees about dealing with continuous attributes, we saw this decision tree which used this iris data set. So it turns out the iris data set is actually predecided pre available in this thing. So the first thing to note is that we are using library called scikitlearn, which is abbreviated sklearn. Lost my. So Sklearn is what we're using, and this code wants sklearn of a particular. So I'll share this code with you. And this is from this book, which is there in the list of references. The last book is Aurelian Jerome hands on machine learning with scikitlearn. So you can go and look it up. So this code, the full code, is really, I've extracted some parts of it. So the other thing it uses is numpy because we will frequently use arrays and all that. And then there are some random numbers which are needed. So it seeds it with a predictable thing. So it uses 42. So if, you know, if you have read hitchhiker's guide to the galaxy, you know why it uses 42, but otherwise it's just some random seed. And now this is some stuff which we are going to ignore. But essentially they're going to be a lots of things which will Matplotlib will be used to plot various figures at various points. And this is related to that also where to save. So this is some setup which is not relevant to decision. This is more to do with organizing how and visualize things and save those figures for later. Okay, so now this is the real part. So, so Scikitlearn has some built in data sets and one of them is this iris library. So the way to get the iris library into your data set is to import this function called load iris. And Scikitlearn has a bunch of models predefined. So what we have been looking at is decision trees as classifiers. So we are going to import from the subset of models of form tree. We are going to import this model called decision tree classifier. So the first thing that we do is we actually load this data set. So we call this load iris function, and then we are going to remember, the iris data set had two measures. It had the petal and the sepal, length and width. So it had four quantities. But we decided we will only use two quantities. So this is what this is doing. It's taking this iris data set and it is looking at essentially columns, indices. Two and three are the siple and two, three are the petal. So it's taking all the rows and it's taking the third and fourth columns. So we're throwing away two columns. So we are making it, instead of a four column array, we are making it into a two column array. And in this data. So when you load the data set, it automatically produces these two subparts, iris data and iris target. So target is the classification variable. Remember, in this case, the classification variable is a three way thing, right? There are three types of irises that we saw, so it will be essentially a number, zero, one. And now to get this decision tree classifier, we have to take this function that we have imported. So we say take a decision tree classifier. We have to provide it with, again some random state because it uses some randomization inside it. And now this is something which we have not talked about. So we discussed that we will build a decision tree normally until we either achieve some pure leaf nodes where the classification is uniform, or we run out of attributes. Now this is a numerical case, so we are not going to run out of attributes because we are going to run out eventually when we run out of data points. But we will end up with these very long slicing, the width between 1.5 and 1.71.5 and 1.6 and so on. So what you can do is you can tell the decision C classifier not to expand beyond a certain depth. And here it has fixed it to be depth too. So we will come back to this at a later lecture, which is, this is related to this generalization question. That is, if you build a very detailed tree that asks a lot of questions, then you start asking questions which are very specific to the training data and you end up with what is called an overfitted model, a model which is following your training data too closely and is therefore picking up some peculiarities which don't necessarily exist within. So one of the things we mentioned in passing was that we like short trees for two reasons. One is because they are easier to explain. The second thing, which I claim without any justification, is that they generalize better. So here, priority, they are saying, let us not construct deep trees. So whatever tree we construct, we are going to stop it when it reaches. So it's a two step process, right? So what you do is you first say what are the parameters for the decision tree classifier? So he says, I want a decision tree classifier to be set up with this random state 42 and which will not grow to more than depth two. And then I have to actually construct the classifier for a particular data set. So that's the next thing. So I use this fit function, right? In some sense this creates a decision tree object with certain operating parameters, and then you pass it the training data in the form of the input and the output x and y, and say fit. So the important thing to see is that this is all you need to do to build a decision tree in python. If you have the data, what you need to do manually is to probably decide. So the question is, can we specify a certain threshold? A certain threshold of what? Yes sir. So there is also stopping criterion where we can stop. Correct. So you can also specify criterion saying that, saying that stop when your improvement is less than so much and so on. All these things have a lot of options. So I don't know offhand what the syntax for that is, but certainly you should be able to do that. So you need to look up the scikitlearn documentation for this classifier to see what are all the different ways in which. So what are all the different options that you can pass at the time of constructing the tree. 

So this is something that certainly should be what is the, but the main thing I wanted to emphasize is that modulo understanding the parameters and what you can do with it, you actually don't have to do any of the encoding that we discussed in terms of finding the impurity and then maximizing it all that is actually something that is taken care of by the library. So all that discussion that we had about how a decision tree is constructed, choosing the best split for the numerical values, then comparing the impurity gain for all the things, picking the best one and so on, all that is done automatically. So you really never have to implement that. So the reason that it's good to know how all these things work is because indirectly that impacts how these parameters that you are allowed to control will work. So you have to have a little bit of an idea about what's going on behind the algorithm in order to make best use of it when it is not producing an answer which is acceptable to you. But as far as using it per se, all you need to do is set up the appropriate. So here it's a decision tree, but pretty much the same. So these two steps that you see here, these two steps. So this is a pretty canonical style of programming. In this scikitler, you import the right type of classifier, you build a classifier of that type with the parameters that you think are reasonable for your application, and then you pass it the training data and say fit. Now here, one thing we have not done, for instance, is to segregate out, we talked about training and test data. We have not done that. So that's also something that you can do. Before you set it up, you might get all the iris data and then you might want to keep part of it aside. So we will see as we go along how to do all these things. But for the moment, it's taking all the iris data which has come from the iris data set and using it to train the decision. That's also. So now this is some visualization stuff, which I'm not going to explain, but as a result of this whole thing. So the thing that has been fit before is now available. So I can pass that reclassifier to this graph thing. There is some issue which I have not debugged yet as to why it is not drawing things correctly. It's something to do with magnifying the screen. But anyway, we saw this picture in the slides, so this picture is actually produced by the python. So this is how the classifier looks in a human readable form. So as we said there, what you can see is that it's using internally this Genie index. So it's using that as the format there. And it's telling you at each point some information about how the data. Remember, we had three classes, 50 50, and then once we did the first split, we got this 500 zero and then all that. So if you remember those pictures, which I. So this was that histogram, remember? So this bottom most thing was the first type, the Cetosa, and then we had these other two things, this verseicolor and Virginica. So the first split was this petal length at some 2.45. And the second split effectively did not affect this part because this part was already, the left hand side was already pure. And then the right hand side we further split on the y axis at 1.75, I think. So that's what's happening. So 2.45, if you're on the left of 2.45, nothing more, you already have pure split on the right of 2.45. Then you further split that region into two by drawing a horizontal line and splitting it into two vertical parts. And at every point, there's a corresponding split. So 00:50 50 now goes as 00:49 five and zero, 145. So it has a nontrivial GD index. But because we said so, it's not because it decided to stop, but because we told it to stop. We said, do not expand history beyond depth two. Here we have reached depth two. Depth two means I've asked two questions. It won't ask any. If I said depth three, it would probably gone almost. So this is just a kind of. So notice that that classifier tells us now, if I give you a new pair of petal length and petal width, right? If I give you some combination of numbers, which is a hypothetical flower, it will follow this tree and it will put into one of these three classes. So these classes, these boundaries are clearly horizontal and vertical lines. They really divide the space of all points in this region. That is petal length between, say, one and seven, and petal width between zero and three, it divides this space into these regions. So the first line is this vertical thing here. And then the second one is this horizontal line here. So every point known or unknown which falls into this yellow region will be classified as setosa. Everything known or unknown. If I produce a new point, even if it's not a real flower, but something which I just fictitiously manufacture, if the combination fits in this green zone, it'll be called the last one Virginia. And if it is in the purple zone or the blue zone, it will be called. This complicated code here is doing is it's just reverse engineering these geometric boundaries that the decision trees decisions correspond. Right? So these decisions define some geometric lines in the space of all possible inputs. And this is just helping you to visualize them, right. It's nothing more than that. So these are all byproducts. The actual classification is only those two lines of code. After that, you might want to see the data in different formats to interpret it. That is really not part of the classification, it's more the visualization part. And there are various visualization packages which you can use. So the way that it actually works internally is that if I give it some new input, give it some five. So remember that these are not exact, right? So there is some doubt because of the misclassification of the two categories on the right hand side. So if I give it something which is of the form here, like five, this one, if I give it an input of the form five, comma 1.5, then because the petal width is five, it is clearly not yellow, but because it is 1.5, it is in this zone here. There is some ambiguity. So just purely based on the numerical values associated with these pure categories, this 54 and this 46. So there is a probability that it is verse color, which is 49 by 54. So 49 by 54 is about 90%, right? Because 54, if I divide by ten, is 5.4, so it will be a little bit more than 90%. And similarly here, so remember, the boundary is 1.75, so if it is less than 1.75, it should be green. But there is a probability that it is wrong, which I get. So that is what's basically coming out here. So you can actually get the fact that with probability 90%, it is coming out to be the second category. But there is a small balance, residual probability, which is coming there. And these are pure, these are not real probabilities. These are just the misclassification ratios applied to those categories. 

Right. If I am here, then I cannot be 100%. So if it were the case that at that second level they would pure, then I will get zero, 10, or one. But because those last two boundaries are not 100%, then internally, what the question is, how is the model calculating probability? I'm just saying it's exactly calculating it the way I said. So if it falls into. So if I say five, 1.5, and I follow the tree, it is bigger than 2.45, so it goes down the right, and then it is smaller than 1.75, so it comes down the left. So if it comes here with 1.75, then with probability. And this is where the thing is. So, with probability, 49 by 54, it is versicolor. So 54 -5.4 is 48.6, so 48.6 would be 90%, so 49 is slightly more than 90%, so 49 by 54 is slightly more than 90%. So it's saying, if it comes here, then with 90% probability, just as a ratio of the majority, the probability is that, and therefore the possible probability of it being wrong is five by 54. So that is the way it works. Is that clear? So that's exactly that number. So this 0907, if you work it out, should be. I don't want to mess around with this thing, but if you calculate it, 49 by 54 should be 0907 and five by 54 should be zero, nine. So these two add up to one, because it is clearly not zero. It's not clearly not the first case. The probabilities are just arithmetic expressions of the ratio. And if some data fall into one category, then it's certain that probability will only that it will not change, right? Yeah. So if that thing had said 54 out of 54 of that type, it will be one. So basically this is just the ratio of the majority. So we talked about misclassification as being the ratio of the minority. This is just a ratio of the majority interpreted. So with 90% probability it's going to be the majority class, and with 9% or whatever probability is going to be the minority class. And that's just that. 49 by 54 and five by 54. That's okay, good. So it's not really a probability, it's just a frequency count. If I come there, then 49 out of 54 times I have observed this and five out of 54 times I've observed that. And then of course it has to give an answer and that's what basically the prediction is. So this is really the other side of it. So fit was a function that we saw earlier. So fit created the model. We passed it an x and a Y. So X was a table of inputs and Y was a vector of output predictions which were learned. So that is the training data, and it produced a model which we visualized there. Now, when I want to actually run this model, what I have to do is I have to give it an input and ask it what do you think is the output? So things, effectively, the output will be ambiguous because there will always be some degree of mixing between the classes. So what it will do is it'll first compute according to what. In this decision tree, it will basically follow the input to the leaf. And in that leaf it will look at the distribution of nodes and it will assign a number to each one so that they are normalized to add up to one. And after I normalize and add up to one, I take the highest probability one as the answer. So if you now insist that this tree should tell you one of the three classes is obviously going to predict the class with the highest probability. So it gives me one. The reason it gives me one is that the three classes are encoded as zero, one, two. So one is really the second class. The reason it's an array is because normally I predict not one value, but I predict a collection of values. So I pass it an array of inputs and I get back an array of outputs. So here I pass it a trivial array. So I pass it an array. There's an outer square bracket and the inner thing is an array of inputs, right? It's a two dimensional object. So normally I would two column matrix to predict, saying, here is a set of rows for which I want the output and this will produce a column of outputs. So that's why we have this double bracket. So you have the fit and then you have the predict. And the predict is the outcome of choosing the highest probability. So if you really want to see how close, I mean, for instance, if you get a one, then how likely was it that it could have been a two instead? That is not visible here, because you just blindly get the highest probability. You don't know what is the difference between the highest probability and the second highest probability. So if you look at this predict, underscore the probability of the prediction, then you will get the actual differences. So you know in this case that it is reasonably correct, right? Whereas if the ratios had been some zero point 51 and 00:49 then you would have not taken this prediction with that much confidence. You would have said, okay, maybe it is not really that any question about this. Okay, so now one of the problems with decision trees, which you will see later on, is also that a lot of the construction of the tree depends on these statistics. Because we said that we look at which attribute has the maximum discrepancy, whether it's entropy or genie index or whatever, which one of them will improve it the most. And that improvement, or lack of improvement really depends on the distribution of the different inputs at that point in my table. So if there is a borderline case, it could be that by shifting one value here or there, the attribute which has the highest improvement might shift from one to the other. And then I ask a different question. The moment I ask a different question, the tree becomes radically different because the question pattern changes and the split will change and so on. So here is to illustrate that what they have done is they have first, so y equal to one. Remember that the categories are zero, one and two. So they are picked out from the input those. So remember the petal has width and length, right? So the length and width. So after, initially we had sepal length and width, petal length and width, we reduced those four columns to two columns. So now we have petal length and width. So the second column, or rather the column index, one is the width. So this is picking out the second column of x where y is one, and this column value is maximum. So this is basically finding out that this particular flower, so remember that the width was varying between zero and three. But we are saying in this purple region, not in the purple region rather, but among all the flowers, which are the training data labeled blue, because this y is the training data value, what is the widest flower and that is this. 

So this blue flower here where you can see the cursor just at this corner here where I'm rotating the cursor. So this has width 1.8. So that's what we're identifying. We're identifying among the training data. So the training data where y is equal to one, where the label class label was one among those points, which is the one where the second column of the input, the width is maximum. Right? So now what you do is you basically remove it. So you say that I want a training set in which I don't have 1.8, so I have either class is equal to two, y is equal to two, or I don't have 1.8. So basically this has all the inputs except that one. If there were more than one, all the ones which are of class one and 1.8 get eliminated. So I'm basically effectively removing this one data point from my training set. Understand what I'm doing? So I'm taking the old training set of 150 flowers, and I'm removing this one tree by identifying which is the widest. So remember that last boundary was horizontal, so it was separating less wide from more wide. So I'm taking the one which is at the bottom category but closest to the boundary or in fact above the boundary and eliminating it from my data set. So I build now a new classifier. So I create a new decision, the same type, but I call it classifier tweak. So this is just a new object, and I fit it to this new data set, which is X minus at one input and Y minus this one. So you understood what I've done. I've basically taken the old data set and removed one extreme point from one of the categories, the widest blue flower. And now what happens is that I end up with a very different decision tree. So this is, remember, this is the second level visualization. So I'm not showing you the tree, I'm showing you the outcome of the tree. So effectively what this tree is doing is that, remember the earlier thing, I would separate out the yellow from the blue by either drawing a vertical line here or by drawing a horizontal line as this one has drawn. So the earlier thing, we found it natural to draw a vertical line, but somehow by changing, this is the mysterious thing, by changing some point which is quite far away. It has nothing to do with this yellow set at all, by changing something very far away. Again, the idea is that the solid line is the first question and the dashed line is the second question. So mysteriously, the discrepancy which happens here at this point, which is the second question, which affects the second question, is actually changing the first question. So this is why these decision trees are rather unstable classifier. Because with a little bit of perturbation of the input, the tree can change predictable and dramatic way. So what this is now doing is it's first asking whether the width is more than whatever, zero eight or something here, right? And if it is less than zero eight, it's saying it's yellow. And if it's more than zero eight, then it's asking again about 1.75. And then, so the two data sets which differ only in this one input, produce very different decision. So that's what this is clear. Is it necessary to only remove one point if there were more? It's not designed to remove only one point. It could remove more points. It's designed to remove every point, right? Which. So first of all, it says either the width is not 1.8 or the category is two. So if you look at this line, so it says either the width is not 1.8 or the category is two. Now, in terms of all the yellow flowers, you know that their width is well below 1.8. So they are all going to satisfy the width not equal to 1.8. Among the blue flowers, we are eliminating the. We know there is one which is 1.8 and that will get eliminated. Among the green flowers. It's possible that there is something which is 1.8, but because y is equal to two, it will get retaked all the blue flowers which have 1.8, but I believe there's only one in the data set. So suppose instead of the point that we removed in this time, if we removed a different point, would we get a different decision tree? For that, it's not necessary. You will get a decision tree. It's different. But in general, the removing and modifying the data set in a very small way can produce it. So it's not guaranteed. It will produce. So you can experiment with it. So you could, for instance, in the same data set, probably that's a good experiment for you to use, which is that. Notice that we have two types of discrepancies here, right? So we have this horizontal line and we have this one blue thing which is above the horizontal. So what if you remove the one or two of the bottom green things which come below the line, these are also wrong. Right? So the blue thing above the line was wrong in some sense. The green triangles below the line are also wrong. So instead of removing the widest blue, you can try to remove the narrowest green or the two narrowest greens or something and see what happens. So you just have to copy paste this code and just change the condition and all the visualization. You can just run it as it is and see, I have not done it, but I think you might get something different. So the predict outcome is not predictable. I cannot guarantee you that one or the other will produce a different output, but it can happen. And this is generally something which one has to be mindful of when you're dealing with decision. One reason that this is important is remember that we talked about this cross validation. So what do we do in cross validation or even in normal validation? In normal validation or in cross validation. In either case, you pull out some random data, set data points from your input and build a tree on the rest. Now the question is, the model that you built from the rest, is it a stable structure in the sense that is it sensitive to how you chose your training and test data? And this example suggests that you are likely to get different models from different subsets of training data. So you then have to worry about what is your final model going to be. So if you test it on a particular test set, then maybe if you test it on a different test set, you might get an equivalent model, but it might have a very different structure, and maybe for other reasons, that structure might be more natural. So this is a generic problem with this kind of tree model, that it is very sensitive to perturbations in the data. We will look at a more familiar problem, which you are generally aware of called regression. Right? So in regression, what do we do? We take a bunch of points, we try to fit a line to it. Now, if I take one point in that set and remove it, or if I shift it a little bit, the line will perhaps change, but it will change in a very minor way, right? The slope will change slightly. So small changes in input produce small changes in output. So that is a kind of stable kind of situation. So this is something called variance. So how much variance is there in your model? So, variance is basically a property which says, is it the case? So if you have low variance, then small changes in input produce small changes in output. But decision tree, by definition, is a kind of discrete kind of thing. There's no way to slightly change a tree. You change a question and the tree changes drastically. So it has high variance, whereas a line fitting kind of algorithm will have a low variance, because if you change a few points, you will get a slightly different line but the line will be recognizably similar to the other line. Does that answer your question? Yes, sir. Thank you, sir. Okay, so one of the things that you can do. So one of the classic data sets, which is difficult to classify by many models, is what are called these either half moons or bananas. 

So just imagine if you will just ignore the light, right? So you have this one yellow crescent of bananas and you have a blue crescent of bananas. They kind of interlocking. So you see the picture, right? So this is a very typical situation where it is hard to build good classifiers, or it's a good test of how your classifier works. There is this built in function in scikitlearn, which is called make moons. So you can make moons and you can provide that with some cellular blue. So I must warn you that I don't know exactly what all these things do, but essentially it's clear it's going to produce 100 data points, and implicitly 100 data points are going to be split 50 50. But this noise is going to somehow tell you how much they have this kind of intersection property, right? So in the middle of the blues, do you have a yellow. In the middle of the yellow, do you have blues. So what is the probability that these things are not separable by a nice boundary? So now what they are doing here is that they are basically taking this set of moons, except ym. So ym, it comes out on the moon's, the moon's data set basically produces these two sets of points of different categories intersecting. These two crescents are intersecting. So now you build these two classifiers. And here somebody asked earlier about stopping criteria. So here they're using a different stopping criterion. So here they are saying that if the number of samples that you reach in a node is less than four, then stop. So there's no limit on the size of the tree as such. But if you get a very small partition, then you just don't try to refine it further so you don't come down to zero, one and all that. So you just say the number of samples per leaf node should be at least four. So there are these two classifiers. So one which is without such restriction and one which is with such restriction. So CLF one is the unrestricted tree classifier. CLF two is the restricted one. This is just to show you this overfitting thing. So see, the right hand side is with samples of at least four in every leaf. And this produces this reasonable boundary which you, I mean, you can see that it sort of looks like the squareish shapes look like two intersecting pieces of some kind of a jigsaw, right? So you have a yellow kind of piece here and you have a green piece here. And of course, there is this hole here, which could be yellow or could be green, but it turns out to be green in this case. Now, if you allow to go arbitrarily, then it starts finding these very, very. So if you have not noticed, there is this small yellow thing on the bottom right, which is misclassified in this cruder model. So the model which is forced to stop, whereas this model is not allowed, not constrained by that, it's allowed to construct a leaf, which consists of just that one yellow piece. So you can build this bottom right rectangle whose only job is to isolate that yellow piece. So from all the blue part there or the green part there, it asks a couple of questions. It asks one vertical question and one horizontal question and isolates that. Similarly, there is a blue point in the middle here, right there on the right hand side, you can see where my cursor is, that blue part. So here it basically create just to pull that blue point out. So this is, again, kind of an experimental or empirical justification of the argument that smaller trees will give you more manageable and more understandable marks. The tree on the right sort of makes mistakes, but you can kind of figure out why it is drawing the lines where they are. And there are going to be a few anomalies with it. The left hand side is arguably got fewer anomalies, but it has created these strange islands of yellow at the bottom and the spike of green in the middle and all that, which may not make much sense. So this is as far as constraining the thing. Now, if I take this. So now we go back to our, this iris data set. So iris data set had capital X as the input. That was our original input. So now if I take that capital X, that capital X and I rotate it, so if I rotate it, I get a data set which looks like this. So I've just taken that same data set and just rotate it right, so that they lie in one direction rather than diagonal. And again, by this rotation, you end up with a very different classifier compared to the classifier that we had when it was unrotated. So there's no fundamental change in the geometry of the points. I just rotated them, but I've rotated them, and I'm using the same axes, right? So I'm using the same x one, X two. But I've kind of interpreted these points differently because I've rotated them and I get it differently. So all of these are just showing the susceptibility of the tree building process to the actual distribution of the data points. So if we remember, we deleted one extreme item, we got a very different tree, then we looked at that moon thing and we said, okay, if you don't constrain it, you get these very high dimensional trees, which don't make much sense. And here we are saying that one other thing, instead of deleting a point, if we transform it geometrically into an equivalent, geometrically equivalent shape, but the axes are now oriented differently with respect to the geometry of the points, I get very different. So these are the kinds of experiments you can do. So building the tree is trivial, but the interesting thing is to then see what you can do, the data, and seeing how stable the tree is. So these are the kinds of things that typically one does with these libraries. And I think this is, again, some other rotation. So let's not worry about this one. This is, again, some rotation example. So I just wanted to show you some code before we come back to theory. So, is there any further questions? So, at the moment, I'm not expecting you to do anything with this code other than to understand that a, it is relatively painless to invoke a model in scikitlearn. And if you have this kind of visualization code, either you write it yourself or somebody gives it to you. You can then interpret your models visually and then experiment with it by changing things and seeing what happens. And that gives you a fair amount of insight both into your data and into how the model is working. So it gives you some indications about the limitations of that particular. Okay, so now I'm going to do something hopefully not unfamiliar to you. Right? At the beginning, when we talked about supervised learning, we said that we are trying to predict something, and those predictions can be in two types. So one type is what we have seen in the decision tree, which is a classification. So the predicted value is one from a small range of possibilities. So we have been looking at a binary case, but even at an iris data set, there was a three way split. But we are choosing a class from a finite set. The other type of problem, which is very common, is to predict a numerical value. So we were talking in terms of predicting the marks of a student based on their board exam prelims and so on. So here is an example of that. So, supposing you want to predict the price of a house based on its square foot area. So you have, in this case, the training data will consist of the input is the house price for some given houses where you know the living area and you know the price. So these, now you would plot them. In this particular case, it's a two dimensional plot. The x axis is the living area and the y axis is the price. And then you would like to, for example, say that if I give you up there, if I give you house which is of this size, then what would be the corresponding position? So I give you an x and what is the y? So you essentially want to map this data to some kind of a prediction function, which, given the living area, will produce a reasonable estimate of the price. So here is some more data. So supposing you expand that thing to say that you have two inputs, you have not just the living area, but you also have something about the number of bedrooms, and then you give price. So now I have something which is a three dimensional scatter plot. So I have x y or x one x, and my output is a function of x one and x two. So as we know, the simplest thing you can do is try and fit a linear function, right? So you can fit some theta one times x one plus theta two times x two plus theta zero. And try to see what are the values of theta zero, theta one and theta two, which best approximate the training date. So that's what we are going to talk about. So these are these linear predictions. So you have a bunch of input features, this case x one and x two. X one is the living area, x two is the number of bedrooms. And these are numeric values. 

And from this you want to construct a function, a linear function of x one and x two, whose output will give you the desired value, which is the price. So because you have one coefficient with each of the terms plus this constant thing, if you have two inputs, you have three coefficients that you have to compute. So in general, you will have some k, such things. So these are usually called features, these inputs. And you always have to deal with this stupid thing. So to make it uniform, it's nicer if I can associate with theta naught, that constant term, a corresponding value x zero, which is an input. But I know that the theta zero always occurs as it is. So I can imagine that my input looks like. So I can basically pad out my input so I can take this input x one to x k and rewrite it as one comma x one, x two up to x k. So now if I take the theta as theta naught, theta one up to theta k and I do this dot product, then I'll get theta zero times one, which is theta zero plus theta one times x one plus theta two. So I will end up being able to write my output. So this is my prediction. So theta is the thing that I need to determine. So for a given choice of theta, that is a given sequence or vector of coefficients, the prediction it makes is just this summation, right? Summation zero to k, theta I x I. So I can put it into a single summation because I have by convention inserted this x naught. But I have made sure that in my input data. So remember, these x I's are actually not unknowns. These x I's known. So these x I's are what you get. The theta I's are the unknowns that you are trying to figure out. But in terms of the theta I's you can predict for a given xi. If you have computed some theta I's, then the answer is summation theta I x. So therefore the target that we have is clearly to find the best line. So let's assume we have a simple case of only one variable. So we have x one, x two, and so on. So we have n points. In reality, each of these x size is a vector of k, and the y, by convention, is always the label. So in the earlier thing, when we looked at that scikitlearn decision tree thing, we used capital x to denote the input matrix and y to denote the column matrix of output. So this is a normal standard convention in supervised learning that x's denote the inputs of the training data and y denotes the output label. In this case, it's not a label, but it's an output value. So in order to compute the best fit, we need to measure in some sense how far we are away from the answers that we want. So how far is our prediction away from the true answer? So of course, the usual problem is there that we don't know the true answer. But here, notice that unlike our decision tree, we are not likely to actually, because these values are all not quite lying on a line. It's not that we are actually going to get a line that passes through all the points that we started with. So in a decision tree, we are more or less tailoring our decision tree to give the correct answer for the given. So for overwhelmingly large values, large fraction of the training data, the decision tree will give the correct answer. So measuring the accuracy with respect to the training data will typically not give us very much information about how good the tree is on unseen data, whereas here, because we are kind of assuming that the data is slightly scattered and we will formalize this a little later on, we can measure our prediction with respect to the actual training data. So for each xi, we can first compute what our current model tells us and subtract what I know is the answer for that, which is yi. And now, this could be positive or negative, but I want to measure the error. So I could take absolute value, or in this case, I'm going to square it so I get a positive quantity. So we are taking the sum of the squares of the errors at the individual training points for this model that we have built at this current theta, which is a function h theta, which is just that linear function. Right. And so we are taking this square of the differences, we are adding this half term, which is just for convenience. We will see later on, it doesn't matter because, of course, if you scale by half, it's still going to be monotonic. So if you have more error scale by half, it'll still be more. So this is essentially the sum squared error, modulo that half. So we are taking the square of each individual error and summing it across all the training. If I divide by the number of training samples, then this will become the mean squared error. So technically, I think this should be a one. Make it correct. So we are going to use sum squared error. Why we are going to use sum squared error we'll later on. But we could have used absolute value. We could have used any reasonable function which accumulates, in some sense, the error that we have made. And we want to eventually, our goal is to make this error small. So this error is usually called, it is the cost that we incur. But traditionally it is called the loss function. So the loss function is a way of describing how far away our model is from the true values in our training data. So in a numerical setting, this makes a lot of sense. If I, then typically the loss will be zero or one. Either I get it right or I get it wrong, say a binary class. If the output should be zero and I predict zero, then I have zero loss. If the output should be zero and I predict one, then my loss is one or vice versa. So that's called a zero one loss. So zero one loss is a little bit harder to justify because you're just counting. In some sense, the fraction of it's more or less that what we called earlier, the impurity. How many are we getting wrong? Whereas here it's a more nuanced thing. It's not counting exactly how many, we might be getting all of them wrong, but by a small amount, and that's okay. But if you get a few of them wrong by zero, and we get a lot of them wrong. So for instance, if I were to draw a line somehow passing through, I don't know if there are three, but supposing there were three points, supposing it was possible to actually hit three points in this exact. But then in the process some of these errors became larger, then I would pay a different price, not because I've got more of them right, but because the sum square error is increased or decreased. So I'm not really counting how many I got right, whereas in a zero one case, if I did this loss, I would typically be counting how many I got right, so I would find a best fit. And by this preceding argument, best fit corresponds to minimizing this discrepancy between our training data and the computed values. So we are going to minimize this sum square. So remember that we expand every input with this extra zero, right? So we had this extra zero at the beginning. So every input of k attributes has an extra zero attribute which is always set to one, so that when it multiplies by the constant term, it will just give me theta. So each x I, each input x I is a row vector. So I can now write my entire training data as this famous matrix x, where each row is one input. So I have n inputs one to n. Each row is one input. So I get n inputs n rows, and my output is a column of numbers in this case. So each yi is a corresponding output value associated with this particular equipment. And these are the values that I'm trying to estimate the k plus one coefficients to go with the constant term and the k variables. So I want to estimate theta, which is theta zero to theta k. So I will write this. Now if I write, think of it as a column vector, so then it is theta transpose. So now remember our sum square error thing. So it says I compute h of x, I subtract the value y, I square it and add it up and then divide by two at. But what is h of x I h of x I is just this times. So I have one x one x k n theta naught theta one. This is theta zero. So the prediction for the ith input is just that Rho multiplied by my predicted coefficients. So I can write it as x times theta. So this is a column of predictions. 

So I can take my column of predictions and subtract the column of expected values. And to square it, I just do this transpose, right? So this will give me a point wise squaring, right? Squaring of each term. So I have, for example, if this is so, this is my difference, right, between for y one and the predictive value for y two. So now if I take delta y one, delta y two, so this is a transpose multiplied by delta y one, delta y two, I get delta y one squared plus delta y two squared, which is what I want, right? I want the difference for each height squared. So that's why I'm saying take the difference and multiply the difference by pre multiplied by its transpose. So instead of this, I do this. So this is now an alternative representation in terms of these x, y and theta vectors of my loss function. My loss function is just the sum square error. Now I want to minimize this loss function. So I want to minimize this. So I want to say find with respect to the, so the variables, now remember, the important thing to remember is these x's and y's are not variables. The x's and y's are actual concrete numbers which are given to me in the training data. The variables are the theta. That's what I'm trying to find. So how does this vary as a function of theta? And I want to find a setting for theta which minimizes this value. So I want to find in some sense a derivative of this j with respect to theta to be zero to get an extreme value. So if I differentiate this and set it to zero, then I have to do some working out to calculate the differentiation. So I take this whole, so this is just j, theta is just this. So I want to take the derivative of this rather with respect to all the thetas and set it to zero. So if I expand this whole thing out, I get this long x theta times x theta. But then this is a transpose, remember, x theta transpose times x theta is theta. Just remember this. So the first x theta gets reversed and transpose. So I get theta transpose x transpose times x theta. Then I get theta transpose x transpose times y. Then I get y times x theta, y transpose times x theta and y transpose times y with the appropriate signs. I'm just multiplying that thing out by pulling the transpose into these two terms. So when I bring the transpose into x theta, I get theta transpose x transpose. Now these two are actually the same thing. So you can check this. Both of these correspond to multiplying y by the so therefore I can merge these two into this. So this is just some simple algebraic manipulation. So you can go and look it up and verify it for yourself as you go along. And now if I differentiate this with respect to theta, then it will turn out that you want this quantity to be equal to zero. So this term will disappear because there is no theta in it. This term will produce x transpose Y. And this is where this half helps, right? And this is going to produce twice of. So this is more or less like theta squared. So it's going to produce two theta x x two theta x squared, and that's two gets cancelled. So that's how you get this. So now you set this equal to zero and you isolate the theta. So you take x transpose y to the other side, and then you pre multiply by x, transpose x, and you'll get theta. So this gives us actually an explicit solution for the best theta. And this explicit solution. So this explicit solution depends on the input, right? Because these terms. So if I change my x and y, I will get a different theta, which is what you expect, right? The parameters associated with the line should depend on the points that you provide to produce the line. But whatever x and y, you give me, if you give me any concrete set of points, this is guaranteed to produce the coefficients that best fit that set of points. And this is called a normal equation. So really for this linear prediction problem, we have a closed form solution. So as a learning problem, it's fairly straightforward in that sense, it doesn't require any fancy computing in terms of finding the solution. We don't have to. Like in the decision tree, we were trying to find out how the smallest model and all that, because we said we cannot compute the best possible model because it's computationally intractable, whereas here it looks like we don't have that problem, we have a closed form solution. So there is a fixed theta, which is the best answer, and we can always find. So the problem here is that this is actually, given the scale at which we are going to operate, we are going to find problems. So if we have something like 10,000 points already, we are going to struggle to do this matrix inversion. And also we need to check that x transpose x is invertible. And ten to the four is a relatively small data set. So the real challenge here is that of actual computation, not the theoretical computation, but the practical computation of this quantity as a matrix operation is not really feasible for the kind of data sets that one is expecting to see. So the other approach, which is a more computational approach, is to do this iterative. So you make a guess. So, for instance, initially you plot a line which seems reasonable. Now you compute the error. So what is the error? The error consists of taking every point and measuring its vertical distance from the line. Right, the distance. This is y I minus h. Doesn't matter which way you would add it, the other way. So h theta of x I minus y I. That is just this quantity. Y I is the actual value. H theta of x I is what the green line tells you it is. So now you use this to improve your line. So you start, you have these big lines. You try to make the discrepancy smaller by adjusting your parameters. So you keep doing this until you find something which is the best. So you iteratively approximate that normal equation. So the question now becomes one of computing the best adjustment. How do we adjust? So, this is a general procedure which is called gradient descent. So the gradient is now, again, like we said before, we have a loss or cost associated with any approximation that we build. So it is sensitive to its parameters. So we want to know, in terms of each of the component parameters, theta. 

So what does slope tell you? It basically tells you, if I change x by a certain amount, how much will y change? That's the slope. If I just draw a simple line, if I move this by delta x, then how much is this delta? So that's what slope is telling. So now you are kind of generalizing this thing that in each parameter, if I change theta I a small amount, how does it affect this total loss function? Now, remember, the loss function is an explicit function. We know the loss function. We've already written it out when we differentiated it to get the normal equation. So the loss function can be explicitly computed. So we can actually compute this. So the gradient tells us something. And now what we do is we say, okay, if the gradient says that if I move in this direction, the loss will increase, then I move in the opposite direction. So you adjust. So the gradient is always the direction of increase, somehow, by convention. So you go in the opposite direction by a small amount. So you compute the gradient with respect to theta I, and you go a small step in that direction, and that is this step parameter called alpha. So you adjust each parameter in a direction. So here is a picture of this thing, right? So you start somewhere. So these vertical hills are supposed to indicate the loss function. And you want to come down to a thing here, and there is a kind of a strange surface whose shape you cannot really calculate because it's quite complex. And what you want to do is find a way down to the lowest point. So you look around you. So this is literally what gradient descent is doing. You look around you and you say, in which direction should I move? So that I go downwards. So it's a greedy strategy in that sense. Just like the decision tree was a greedy strategy. You say, here are a bunch of attributes I can pick. Which one do I pick so that I make the most improvement. So here you're saying which direction should I move so that I go downwards? And then you make a small step in that direction. Now how much you make a small step will be of course an important question, which will come up to again and again. But you make a step in that direction, then you reassess, okay, now I have reached a new point by perturbing these parameters by moving in the right direction. Now what's happening? So you can again recompute the gradient. Again you move a small step and keep. So this is called gradient. So for a single training sample, let's see how you do it. So if you take a single training sample, then this is our formula for j theta. The loss function is theta of that. So I just have one sample, then I can expand it out. And then because of the square and all that, this is just saying the derivative of f of x whole squared is two times f x times f. So you work it out and you get this. So this is just coming from the fact that it's theta of x is this, we're just saying that d by d theta. D by d theta. So you're just using the simple derivative rule to come from here to here. And here you're just substituting. And then if you check this, this term has one component for every parameter theta. But I'm only interested in the theta I parameter. So all the other non theta I parameters will disappear and I will only get theta I, x minus y. And if I do, d of d, theta I of that, all the others are independent of theta I. So they'll disappear. I'll just get x. I remember that x I's and yis are constant. Theta is the variable. So that's how I get explicitly compute. This is what I meant by saying I can explicitly compute the gradient and then I go in the opposite direction. So you can do this over the entire training set. You can work it out. So this is what is called batch gradient descent, right? So you compute this h theta for the entire training sec, right? So this gives you overall a loss for the entire training sec. Now you take this and you adjust each parameter based on this loss, and you'll get a new theta. Again. You will now compute this for the entire training set. So you'll get a new set of outputs again. You will compute the loss, and so on. So you keep repeating. And now remember, what we are ideally trying to get at is whatever answer we would have got from that normal equation. But as usual, numerically this may not converge fast, so you might stop because you're not making much progress. So, convergence could either mean that you actually get a parameter for which the next iteration doesn't change anything, or you get a set of parameters for which the next improvement is small. So that's something which is normally there. You normally aim for the best theta, but for as good a theta as you can compute within a reasonable number of equations. Sorry if I missed something, but what is alpha here? What? Alpha. Alpha. Alpha is something which you have to fix. So we will not discuss it right now. We will come back to this when we look at more complex models where we apply gradient descent. But it is really something which tells us by how much we should adjust the parameter, I mean the value in the direction. So if it is very small, then you will make very small improvements. But if it is very large, then sometimes what will happen is that you will overshoot, you will overcompensate. So just to give a picture, so supposing you're trying to come here to this point you're trying to hit, this is your optimal parameter. Now here it might tell you that you have to go down, right? So that means you have to go to the right in some sense, not down, but you have to go right to go down, but you overshoot. And you overshoot so much that you end up actually going there. You see what I'm saying? The gradient is saying that if I go left, my error increases. If I go right, my error, how much should I go right? I should go right by some multiple of alpha. But if alpha is very large, then it might take me overshoot beyond the minimum point to another point on the opposite side of the slope. Now it tells me that, oh, I should go left. But now, again, because alpha is large and the slope is increased, alpha times the slope is a large value. And I'll come here. So if you pick too large and alpha, then you will possibly diverge. If I pick too small and alpha, on the other hand, I will converge very slowly, right? So I will make incremental, many small steps towards it, and then it will take me very long time. So choosing the right alpha is a little bit of an empirical thing so that we are not going to get into. But usually you choose a small. The question about convergence. Convergence is just saying that you want to get to the minimum. So the minimum is, there is an absolute minimum or theoretical minimum, you know, because if you could do that matrix transpose in matrix inversion, you can actually calculate your target and not do it iterative. 

So in our terms, what it means is that I reach a theta for which the gradient is zero in any direction. If I go, the loss is going to increase. There is no good way to move. So that is the next iteration, which should be the same as this iteration. But that would mean I've actually found this magical point. So in practice, what I will do is if I'm near the gradient, so let me do it this way. So supposing I'm near this. So supposing I'm here and I make a small step and I come here and I find that the difference between the two is very small. So I'll calculate the loss at the next step compared to the loss at the previous step. And if the difference in the loss function is below a certain threshold, which I will set according to my choice, then I will say I've converged. So either I converge because I'm not making progress or because I've actually hit the limit, which is very. So the other variation of this is that you don't actually. So in this batch gradient descent, what we are doing is we are evaluating the function that we have estimated on the entire data set before we make the next update. Right. So we have to choose an initial theta, which itself I've not told you how to do. You choose an initial estimate for the parameters. Then you have to run that thing on the entire data set, compute the loss on the entire data set, and then adjust the gradient of that loss. On the other hand, because the loss plays a role because of this. Right? So the actual values are needed here. So you need the actual values, you need the actual value in the adjustment. So the other option is that you do it as you're going along. So every time you evaluate one point, you compute the loss and then you adjust, and then you get a new theta. Then you evaluate one more point to compute the loss and all. But you don't do it predictably, you do it randomly. So you pick up a random data point, make an adjustment based on the value at that data point and so on. So this is called stochastic gradient descent. And another way in between, which we will see later, is you take small batches. So here we batch gradient descent. You take the full. So if you have, say, 1 million inputs, you will evaluate your function on all 1 million inputs before you make one step of adjustment. In stochastic gradient descent, you will look at one input and then do it. Now, the problem with that is that that one input that you get might be one, which is kind of very extreme, and so it might give you a wild swing, and then you might oscillate a lot more. So it might take you longer to converge, but faster to compute. So there's a trade off between doing each iteration. You are making a lot of adjustments fast, but those adjustments are less controlled. The other way in between is take small batches. If you have 1 million things, you break it up into 1000 batches. So you do 1000 points. So you get some averaging out of the behavior of the function, make an adjustment, then you do another 1000, make an adjustment and so on. So all these things are actually used, we will see later in neural networks and all that. But in this linear prediction also, that's where they arise. I mean, that's the origin of this. So this is, of course, as you have all probably read. So this strategy is conventionally called linear regression, but it's basically this iterative linear prediction, which you can do theoretically by a matrix operation, but computationally, it's more feasible to do it in this. So I'll stop here, and we'll continue next time to discuss why we are using some square error and also what to do when you have nonlinear things, how to deal with nonlinear approximations. Having done decision trees, I think at the next class, at the end of the class, the last 15 have a small moodle quiz covering up to last week's decision tree lecture. So it'll be about ten to 15 minutes at the end of the class. Okay, see you then. Bye.