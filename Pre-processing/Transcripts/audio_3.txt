So we were looking at this market basket analysis, and towards the end of the discussion on association rule rules, if everybody who buys X is also likely to buy y. We said that one application of this association rule idea is to have these so called class association rules. So the right hand side was a category. We looked at an example with documents and we said, if a document has these words, it is also likely to belong to this category. So that brings us to the topic that is one of the central ones in machine learning, which is the idea of supervised learning. So in supervised learning, we have a set of items, and each item belongs to some category or class, and we distinguish these items in terms of some attributes. So you can think about this, as we will see shortly, you can think about these attributes as columns, right? So the attributes will be things like name, age, date of birth, whatever, income, stuff like that. So each row in the table will be one item, and there will be a special column which indicates what category, depending on the classification problem that we are dealing with, what category it is. And the goal of supervised learning is to now construct, given a new row, a new item, a new combination of these attributes, which is not seen before, try to predict what would be the most appropriate category to assign. So these items which are given with the labels. So these are called labels. So these items are called training data. So we have labeled training data, which consists of some information which has been collected in the past, some historical information about items and their classification. And we want to find a model that generalizes the training data. So since normally this categorization corresponds to some kind of classification, this is typically called a classification problem. So classification problem could be a binary problem. It could be, yes. No. This mail is junk. This mail is not junk. It could also be multi way, especially when we are thinking of what we did last time with the class association rules. For instance, where you're trying to assign a topic, topic will be one of many different types of topics. So there may be four or five or six or ten, depending on the diversity of topics that you want to include. But of course, you can always do multi way classification by doing it two at a time. You can say, is it category one or is it not category one? Then if it's not category one, you can say, is it category two or is it not category? So you can basically simulate by a cascading choice, right? So you could say not C one, then not C one. You would say C two or not C two and C two. You would say C three, and maybe there are four classes. So if it's not C one, not C two, not C three, then it must be C four. So by doing a sequence of these classifications, you can do it. Now, of course, this is not necessarily the most efficient way. It would be nice if you could actually build a one shot classifier to do all these four things in one time. But from a theoretical perspective, to understand how these models work, we will start by assuming that all our classification is binary. That is, we want to classify something as being class one or not class one. And later on, we'll talk about extending it. So, as I said, the best way to think about this training data, especially when the training data is uniform, is uniform, meaning all the items are similar, is in terms of a table. So the rows in the table are the items. So here we have 15 items, and the columns are the attributes. So here we have five columns, and the fifth column is special. So this column is actually the category. So it's either yes or no. So it is what's called the class label. And the other four items describe the data. So this is supposed to be some hypothetical small for a loan data set. So here what we have done is collected four types of information about each applicant. Their age, whether they have a job, whether they own, possess a house, and what is their credit rating. Credit rating is a kind of long term evaluation of this person's past history in terms of finance and repaying loans, et cetera. So one of the things that you can see here is that some of these values, so it is very clear that these are boolean. So there's no problem. Either you own a house or you don't own a house. Either you have a job or you don't have a job. So these are poor faults. But here, for instance, we have this aggregation, right? So instead of saying precisely that somebody is 45 years old or somebody is 72 years old or somebody is 32 years old, we have grouped these age values, which are logically numbers, into these three categories. So we will talk about this little later in this discussion about how you can go from numerical values to these categorical values. But for the moment, it's simpler to work with these categories because you only have a kind of finite range of choices. 

So here there are three choices. For the first column, the age could be one of three categories. Young, middle, and similarly have multiple categories. We have fair, we have good, we have excellent. So we have fair, good and excellent. So there are three values. So the first and the fourth column are attributes which have this three choices. In general, you could have some finite number of choices. And the second and third column are actually boolean values. So again, you have finite number of choices. Just the choices are two or false. And so now what we want to do is really build a model which kind of tells us if I get a new row which says somebody is, for example, old and does not have a job and does not own a house. So this, for example, is a row which is not old, does not have a job, and does not own a house. Now, you can see that in this particular segment, which is old, there is somebody. So the last row actually has, maybe we say, does have a job and does. So this is not right. So if you look at these five rows, which correspond to old, the combination true. True is not. There's. Maybe this person has a fair credit rating. Now, the question is, as per this historical, whatever decision process this data set has been following, would this person have got a loan on? So that's really the prediction problem. So I give you a new item which corresponds to a new combination of values in these columns, and then I want you to predict the class column. So, just going forward, we will need this information. So there are 15 rows, and if you count the yeses, 123-45-6789 so there are nine yes in this specific data set, there are nine yeses and six no's. So, I mean, if you think that this is a representative sample of the applications processed by whichever bank or organization is handing out these loans, then you might infer from this that more than 50% of people who apply get a loan. So maybe there is some other criterion which prevents people who are not going to get a loan from actually being considered. Maybe this is after some. But here it really looks like some 60%, nine out of 15 or 60% get a loan. So this is one of the issues that we have to deal with, which is that this training examples that we are going to base our model on should be somehow indicative of what we are going to predict. If we are going to take data pertaining to one group of individuals and extrapolate it to a completely different profile of individuals, then it's unlikely that this model that we built has any relevance. So there is this fundamental assumption. So we are not going to make this assumption mathematically precise in this course. So, in the advanced machine learning course, we will talk about a kind of theory of learning in which you can make this thing rigorous. But we are going to assume that there is a correlation between the distribution of training examples and the distribution of unseen data. So these are probabilistic, of course. Even if you have a similar set of data, you could draw samples which omit a few cases and have a few other cases. So it's always going to be a kind of probabilistic argument. But what you're saying is that this is a representative sample. So you're not saying that the examples have every possible situation that you are likely to use for judgment, but you're really talking at the level of probabilities. You're saying that the probability distribution of the training is somehow identical to that of the unseen data on which you are going to make the prediction. So this is going to be your training and this is going to be your prediction. So this is a fundamental assumption. If this assumption does not hold, then whatever you try to do doesn't really have any basis. So this is something that we have to assume, and we will talk about later on what this assumption means. Maybe not in this course, but in later courses. But for now, let us assume this. So the next question is, what does it mean to learn? Right? So, as we saw in this loan data set, 60% of the loans were granted, so nine by 15. So if I were to just say yes, supposing I don't have an interesting model and I just say yes all the time, every answer that every person who comes, I just say yes, give this person. And if the previous assumption is actually valid, that the distribution of training examples and the distribution of unseen data are actually the same, then if I used to give 60% of the training people loans, then giving 60% of the unseen data loans is something that I would expect to do. So if I just keep on saying yes, then 60% of the time I would be correct, which might suggest that you're doing better than half, but actually the answer is biased, so you're not doing any better than random guessing. It's like if I toss a fair coin and I keep saying heads all the time, then I should expect to call it right 50% of the time. So doing 50% of the time in a fair situation is no better than random guessing in this kind of situation, where you know that the answer is biased because the trading data tells you that you have more guess than no, then if you go for the majority answer, then you will obviously do better than 50%, but you're doing no better than. In some sense, this is the same as random, so you should actually do better than that. 

And ideally your performance should improve with more training data, which will not happen. In this trivial example, if I give you 30 cases instead of 15 cases in the previous. If I give you 15 more rows, and again they have 60%, your performance will still be the same. But maybe you see different combinations in those 15 new examples, which should give you a better note. So we really can think about learning as doing better than random guessing. Now, another big issue with this whole problem is that what we are building the model on and what we are going to test it on are two different things. So we would like a model which does well with respect to the training data. So if you think about it as a situation where you want to minimize the errors, some kind of quantity, right? So you have a model which makes some predictions with respect to your training data, and you have the actual training data answers. So in a way, you would like to make these match as closely as possible. So it's a kind of optimization problem. You want to minimize the discrepancy between what your model does and what your training data tells you is the true value. So this is all possible for the training data because we know the answers. But when I move to a situation where I'm predicting the answers, the whole situation changes. Because if I could tell you what the right answers were, I wouldn't have to use this approach. So if I had an algorithmic way of telling you that this person should get a loan and that person should not get a loan, then I did not need not build a data driven model for it. I could just apply whatever algorithm the algorithm might just tell me that if this person owns a house, give them a loan, or this person has a job with a salary beyond something, give them a loan. So this would be a very direct way of characterizing these things. And for these, I will not have to look at past patterns or past data in order to do something. So since I don't have the correct answers, what do I say? Whether the algorithm that I have come up with, that is the model that I have built, is it a good one or it's not a good one, right? So there's a fundamental problem here. So I just want to emphasize that normally when I build anything using software, you build any kind of a program, you have an expectation that this will, if given this input, will give you that corresponding output. So there is a correct output that you expect for every input, and then you can validate whether or not the program does it by feeding it different inputs of different types and seeing that it gives the expected output on a. But here the problem is that this expected output is not something that you know how to compute, and yet you need to make some statement, obviously, as to whether the model is good or bad, because if you can't validate that your model is doing well, you have no basis to use it. So this will be another issue that we will take up. How do we deal with this problem of evaluating performance when actually we don't have a yardstick measuring standard in some sense? So please stop me at any time. So if you think something, I'm going to get into a specific module very soon. So what we are going to look at are different. Start with very simple models. So what I'm going to do today is something called a decision tree. Then we will look at probabilistic models, and then we will start looking at models which are based on a geometric idea. So you think of these items or these columns as coordinates. So you can think of these items as points in some high dimensional space, and you're trying to actually separate them by something which separates categories a from category, not a. So that will get us to more complicated models, starting with something called a support vector machine. And then later on, of course, the model that we most commonly find these days, which is neural networks. So these are the different types of models that we will consider during the course of this particular DM. So the kinds of issues that we need to address in the context of these models is how to evaluate them. The fact that there are many models already tells you that there is no best model. So for different purposes, different models are preferred. Then you need to solve this generalization. So, generalization basically says that the models do well on unseen data. See, here is a trivial model, which is a. So, I just memorized my training data, right? So I just take that table that I gave you before those 15 rows, and I just memorized it. So I just put it into, say, for want of anything better, I put into a python dictionary where the first four columns are the key, and the value that I store with that key is the class label, which is a table text. So now, whenever you give me something from the training data, I just look it up in the dictionary, and I always give you the right answer. So, in this sense, I can minimize my discrepancy of the model with respect to the training data very trivially, by just exhaustively tabulating the answer. So I just mug it up in a very literal sense. But now, if you give me a new combination which is not there in this table, which I have not memorized, I don't have any principle way of giving you chance. So this model, in the terminology of machine learning, does not generalize very well. So generalize in this sense means that if I make some number of mistakes on my training data, and if you give me some random new data, I should make a correspondingly similar ratio of mistakes, which, of course, comes back to this question of how do I know which mistakes are made? Process is part of that. But this is the fundamental thing, right? I want to trade off this optimization. I don't want to make it fit my training data very closely, because then maybe it is learning things which are peculiar to the sample, which I have, which are not representative of the general case. So I might want to intentionally, in some sense, make mistakes so that I generalize well. And then, of course, there are these problems of the training data itself. Many of these models, especially the more complicated models like neural networks, they require a large amount of training data, because essentially, as we will see, the process of learning is one of setting parameters. And the more parameters you have in your model, the more complicated your model is, the more data you need to adjust those parameters well. So how do you get this training data? Is another question. 

Okay, so, let's look at decision. So, here is our table. So, just to remain so, our table has these 15 rows and it has these four columns. So, if you have ever played this game called 20 questions. So 20 questions, typically, you play with one or a group of people, and one person guesses a name, name of a person and person, or the rest of the group has to figure out who this person is by asking questions. And the questions will have some limited format, and typically they commonly answer yes or no. So you might ask, first question might be, is the person female? Then if the answer is yes, then you know that you have to focus all your questions, assuming that the person, that the hidden name is a female's name. Otherwise you will guess a male's name. So, depending on the first answer, you will ask a second question. The second question may be, is this person alive? Then answer may be no. So now you know that you're talking about a dead woman. And then you might ask, was this person in entertainment show business? So these are the kinds of ways in which you narrow down the space. So you start the space of every possible person whom you could be guessing. And then by cutting it down, by asking these questions, you cut down the possibilities to narrow it. So you do the same thing here, right? So you say, okay, maybe it is that the age has some implication on the category. So you ask, what is the age of the person, and depending on the answer, you will ask a second question. So, for instance, if the person is young, then maybe the correct thing to ask is whether the person is working or not. Maybe they have not earned enough money to buy a house yet, but at least they should be working. Seems to be reasonable, because if you look at this, if I look at young, and if I look at those who are working, they are given a look, and those who are not working are not given. So this seems to be actually something. But of course, this is something I've done by inspection. But this is the kind of thing that we want to do automatically. So we want to know which is the first question to ask, which is the second question to ask, which is the third question to ask, and so on. So here is a decision tree. A decision tree starts by asking a question. So the first question that is asked here is, what is the age? And there are three possible answers, because that column had three answers. So I'm asking it of a particular individual in the training data. So I'm asking these questions of these specific 15 books. So if I get the answer young, then the next question I ask, as we indicated, is whether a person has a job. If they get the answer middle, this person is middle aged, then I get asked the question whether this person owns a house or not. And if they are old, by whatever old means, I will ask whether they have a cult. Now, these, in turn, have their own answers, right? So if the person is young and has a job, then it narrows down to two rows. So this says that two out of two rows which follow this trajectory, out of the 15 rows that I start with, there are two rows which follow this path. And both those things come out to be yes. If I follow this path, those who are young and don't have a job, three of them follow that path, and all three of them turn out to be no. Similarly, if somebody's middle aged and owns a house, it turns out there are three of them and they have yes. And there are two which have, without a house, middle age. Both are no answer. So this is our model. So when do you stop? You stop. When you reach this situation, we will come to it formally, but once I have asked enough questions to be sure of the answer, I don't need to ask any more questions. So in the case of the 20 questions thing, once you are clear who the individual is, you will say, I guess that the individual is so and so. Now here what you're saying is I have asked enough questions to narrow it down, so that anybody who satisfies the criteria I have discovered is going to be always given alone or is always going to be denied alone. I don't need to know any more for this combination of attributes, I don't need any more information. So you query the attribute, and as we discussed in the context of that 20 question series, what it does is it really partition data? So based on that first question there, I decide I'll end up partitioning this training data into three sets. Then I will look at a particular partition. So I look at this set, and as we said, by asking a question about has job, I partition that further. Similarly, if I ask this set and I ask the partition about own the house again, I get these three rows, three rows below with the house, and two rows above which don't have a house. So at each step I start with a bunch of data items. The answer that I get to a question focuses on those data items where the answer is that line, so it will partition. Because the answer has to be exactly one of the options, there are no overlaps. Then I can look at the smaller data set. It'll have one less column in some sense, because I've already fixed one column's value. I've already decided that in this five rows, I already fixed that the age is young, so there's no point in asking the age again. So now I'm only looking at these three columns. So in these three columns I should ask a question, and I choose whether they have a job or not. So you partition the training data based on the answer, and then you choose a new question on this reduced set of rows. And this new question will depend on which set of rows you're looking at. And you keep repeating this until you reach a partition where every row says yes or every row says no. That's the ideal situation that you hope to get to in a decision. The question that we have to ask is, how do you do this? How do you actually decide which question to ask Next? So, as we saw, the questions are adaptive. 

So it means that in this path I ask a different question in this path, I ask a different question in this part. So depending on the answers I have seen so far, the next question that I ask will be different across these different paths. So I'm not always going to ask the same second question. The second question depends on the answer to the first question, the third question depends on the answer to the first two questions, and so on. So, to formalize this as an algorithm, so we have a set of attributes so these are all the columns other than the class label, which is not counted as an attribute. These are all the column names in my table. I pick some attribute, one of the columns, and now I will create these subtables, these children, corresponding to the value of that column, and then I will recursively do this. I will take those rows which fall into that subtable, and I will again, now I have attributes which exclude the one which I chose, because the one that I chose is already uniform for that particular set of rows, right? So I have all the young ones, all the middle aged ones, all the old ones. No point asking the age again. So I stop when the current node has a uniform label. That's what happens here when I reach a subtable in which everything is yes or everything is no. But there is another option that could happen, which is that I might reach a situation where. So, remember, how many questions can I ask? The number of columns, exactly. So this is at most, this is all the information that I have. I can't ask anything more than what is given to me in the table. So I can ask one column at a time. But the moment I ask a column, that value of that column is now zero, because the utility of that column is zero, because I've already asked the question. So in the new table, which I have, that column value has been fixed, no point asking it again. So you never need to ask the same question twice in some sense, and therefore you can only ask as many questions as there are columns. So now it is conceivable that this was not three by three, but it was, say, two by two. So that is, among people who were young and who did not have a job, there were three rows, but two of them they got no, and one of them they got yes. So then I had to ask one more question. Now, it is conceivable that the next row goes to one out of two and so on. Now here, because the numbers are such, it's not really going to happen, but it could be that you run out of questions. So at each point one attribute is getting knocked off. So at some point the set of attributes is going to get exhausted, because I've used up all the questions that I could possibly ask, and yet I have not reached a situation where everything in that group of rows that I have left is all yes or all no. So why would that happen? Can you imagine? So what this is saying is, so let's assume there are three, four columns, right? So I have one row which says, so these are the concrete values, and it says yes. So I've asked whether a one is equal to x one. Then I asked whether a two is equal to x two. Then I asked whether a three is equal to x three, and so on. Right? So I've done this. So my four attributes are used up and I have this. But I also have, for the same thing, at least one other row which has the same values for these four attributes. But the answer is different. So can you suggest why this might happen? Because you're really saying that from our perspective, these two items are identical because they have the same four attributes, but one has been given alone and one has not been given. There may be another attribute. Exactly. So the point is that we are not claiming that, and we cannot claim in general that the attributes that we capture in this table or in this training data are exhaustively the ones which determine the answer. So just in terms of loans, or even more apt for many of you, is consider a job interview, right? So you would ideally like that a job interview determines the job or not based on your qualifications. The qualifications could be your performance in your previous degrees and in CMI degree, on your performance in some maybe selection test, that the company gives you some questions that you answer in the interview, and so on. But as if any of you have ever given an interview, you also do other things, right? So one thing is you tend to try and possibly dress up more neatly than you would if you were just going to class. Why would you do that? Because clearly, even though this is not something that is going, nobody is going to write down on your application form, that person came with crumpled clothes, that's not going to be recorded anywhere. Or if you happen to have an interview in which you maybe said something inappropriate or rude, in the perception of the interview, that may not be recorded. So there may be different factors which are not recorded in these attributes which are actually influencing this. So you cannot claim that these things are exhaustive. So essentially, when you have this kind of thing, there is, as whoever pointed out, there is some kind of implicit missing criterion, but you have no idea what that is. So you have to accept that this will happen. So in such a situation, also you have to stop and say something. So your decision tree, finally, what is the goal of decision tree is going to say? If this is the combination of attributes that I see in an item, is my answer yes? Or if I am this case, the answer is clear. Whatever label I have reached uniformly in that group, that is the answer. So if I have, as we said, if the person is young and has a job, I will say yes, I don't have to say anything else. If the person is middle aged and does not own a house, I will just say no. But if there is a mixed thing, then how do I choose? So there is no good way to choose. So all we can say is that at that point, hopefully there is some uneven distribution. So maybe there are a few unusual cases who had all the correct attributes as far as what is written down, but due to some mess up and some hidden kind of features, they didn't get it. So you could basically just take the majority, right? So supposing you do all this and you come to a situation where you have eight customers who have similar attributes, and five of them are given a loan and three are not given, then if you get a new case which looks like this, you will say, okay, five out of eight were given. So let me predict that the new case will be given, because that's a nature. So that's really the only sensible thing you can do in the absence of any other information. So that's typically what a decision three algorithm. So it'll ask question by question. The real we have to figure out is this part, which is how do you pick the next attribute to question. But once you pick an attribute, then the next step is automatic. It will filter out the rows according to the values attribute. And then you have to ask the same algorithm applied on each of those subtables, and these subtables be destroyed. And when you stop, you ask whether it's uniform or whether there's a majority. So this is what we were discussing. So a non uniform leaf node where I have run out of questions, but I have different classes. So basically I have identical combination of attributes, but different classes is something which indicates that our data is somehow incomplete. The attributes are not capturing all the criteria which are actually used for the classification. There are some hidden attributes which are not being captured, and this happens quite often. So we can't derive this. This might happen for a variety of reasons. It might happen in these kind of manmade scenarios, like giving loans. But even when you're trying to classify, this is happening around us. Every example can be traced to COVID, but right now we see that, right? So there are lots of situations where we have to realize that different people with the same bit different symptoms. So conversely, two people with the same symptoms don't necessarily. So there are asymptomatic COVID patients. There are people who have a sore throat and a cold and a headache who don't necessarily have COVID. So I cannot just say that if you have these symptoms, you must have COVID, and if you don't have these symptoms, you cannot have COVID. Right. 

So clearly there is something going on in the background, and that background in our case is revealed through a test. Even tests have errors and all that. So it's really a problem that we have to grapple with in real life. And we will just assume for now that when we reach such a situation, we just take a majority decision. But realistically, if we get too many of these situations, and it's very difficult to decide, we have to ask whether the data has actually been collected correctly. So as we said, the main issue is really how to ask the questions and what this means. Sorry, sir, I have a question. Yeah. Sir, once we know that the data is young, we are making a prediction and it come out that young. So like other attribute is not relevant other than it has job or not, and then we are not going to look at other attributes. Yeah, but remember that which attributes to look like vary from one path to another. So maybe in this case, right, so what we are saying is that if they are young, we only need the job, right? But if they are middle aged, we need to know the house. So for different combinations, so they may not occur together. I agree, in any decision, but each of these plays a role somewhere. Now, if there is another attribute which never plays a role, as we will see the next tree as that example. Right? So if you look at this, this is a different tree for the same data set. So instead of first asking the age, we ask whether you own a house or not. And it actually turns out that everybody in this data set who owns a house, six. Out of those, remember we said there were nine yeses and six no's overall, but out of those nine yeses, six of them own a house and all six of them get yes. And nobody owns a house who's told no. And if you don't own a house, then if you have a job, you get it. If you don't have a job. So in some sense, if you don't have a job and you don't own a house, you're not going to get a loan. If you have either of the other two, you're going to get a loan. Now, in this situation, this attribute doesn't appear at all. So then you could argue that that attribute is irrelevant to this thing, and maybe you could flag it and say that maybe you need not. That's possible. So we need to make like deployment tree too, in order to. Sorry, I didn't hear you. Can you say louder? We need to take all the possible tree. That could happen. Is it not necessarily all the possible trees? So that is some issue that we have to deal with. So what we are going to try and build, we are not going to build all the possible trees because that would be too many trees. So we want to build a good tree as far as possible. And then we have to ask, what is a good tree? And with respect to that good tree, what we are saying is there is a kind of ranking of attributes. So even in this order, the tree will tell us a little bit about which attribute is more important. So there is a kind of implicit ranking which is happening, and an attribute which does not appear at all in our tree that we construct. If it turns out that otherwise it's a good tree, then we could argue that that attribute has no significance. But even otherwise, one of the things, byproducts of decision tree will be a kind of implicit importance, ranking among the attributes, which ones are more significant. So not all columns are equally important. So these are all things that have come up. So maybe when we come to it later on we'll discuss it. But these questions that you're asking, sir, when we form this decision tree, is it important or in some sense beneficial if we know the data? Insight of the data. Insight of the data, meaning what? As in the relevance of each column? Yes, normally that is, I mean, I cannot say it will always help. Normally it will help. But on the other hand, you have to assume that you do not have. So what you're really asking in a more general setting is that if a person who is kind of aware of the context, who's a specialist or a domain expert in this, so say somebody who has been associated with this, whatever, this bank, this particular bank, which is giving out loans, if they build the tree, they might make choices which are given based on their expertise. Whereas what we are doing is trying to do something blind without understanding, except for the data that's given to us. And generally, yes, it is true, but the other problem with that is that in general, one cannot really. So in specific situations, this does help. So if you have domain knowledge, you should use. But sometimes the domain knowledge is very hard to capture in terms of the data, and it's not easy to give an example in this tabular form, but it is very easy to give examples in terms of images, as it doesn't require a great expert to distinguish dogs from cats. So anybody who is above a certain age can look at a picture and tell you whether it's a dog or a cat. But if I ask you as a domain expert to give me an algorithm to distinguish a dog picture from a cat picture, or one which is neither of the two, it's not very easy to quantify that. And there the attributes in a picture are typically just if you think of a picture as a digital picture, it's a bunch of pixels and their colors and their arrangement, their relative arrangement. So even I'm saying, even if you have some domain knowledge, it's not very clear that the person's domain knowledge is aligned to the way the data is available. So what we are going to be doing is agnostic to the domain. So we are not going to believe anything or assume anything about the domain, except that we will want to validate at the end somehow that what we are doing is right. But in practice what you're saying makes sense that really to do supervised learning, it's much more realistic to do it in collaboration with somebody who understands a domain rather than just doing it blindly. Because also when you do it blindly, you might come up with solutions which make no sense. You might actually identify features which contribute to a decision purely by statistical coincidence, and no human would actually do that. So there's another issue which comes up of interpretability or explainability of your answers. So there are various factors which get into this. It's only not only accuracy which matters, but also whether your model is sensible in something, so that you can justify it. So these will all come as we go along. So let's proceed with this. So the first thing is that because the choice of attribute to question will determine the structure of the tree. So here are two different trees. So this is the tree we started with, and in a sense this tree, if you remember that table, consists of asking the first column first. So the first column in our table was age. So we asked age, and then maybe we adapted our question based on age, whereas here we have directly jumped to own a house. The question to ask is which of these trees intuitively is better secondary? What is the definition of better over here, a shorter tree or a tree which predicts better? So here both trees are predicting. Exactly. So there is no mistake, at least with respect to the training data. Of course we have not seen what it does with the unknown data, but if you are just looking at its behavior on the training data, all of these things reach nodes in which I don't have any confusion, any majority to choose. So in that sense they are equivalent in terms of the prediction. So therefore, the question is that if they're equivalent in terms of prediction, whatever that means, then the question is somehow. So it's not even shorter, right? Because this is also like. 

Depends on what shorter means. But I ask two questions here. I ask two questions here. So, normally, the height of a tree is the longest path. So in the longest path, I ask two questions, but clearly this is fewer questions overall. And there are some situations where I can get an answer in one question and so on. So for many reasons, the secondary looks simple. So generally we prefer small trees. So there are many reasons for this. The one reason is it's perhaps easier to explain when I have fewer questions to answer. So there is this general principle which is called. So this is from the philosophy of science. So William of Auckham was a philosopher from several hundred years ago. So his general principle was that if you have two explanations for something and one is simpler than the other explanation, then the simpler explanation is a better one. And this has been a guiding principle for a long time now. It's not always the case that the simpler explanations are correct. So, do you know an example from science? Newton and quantum mechanics. Right? So newtonian mechanics works, but it doesn't really do a good job at atomic levels. So you have to go to quantum mechanics, but under certain assumptions. At macro level, you would not want to do quantum mechanics for measuring how to predict, to put a satellite in orbit or something like that. At that level, you will just be doing newtonian mechanics. So it depends also on what kind of problem you're trying to solve. But in general, this is the principle that smaller explanations are better. And one of the kind of intuitive reasons for that is that smaller explanations are easier to convey what they are doing. So this explainability. So another kind of illustration of this is supposing you go to a doctor and the doctor asks you to take a battery of tests. So you come back, I'm sure many of you have done this. So you get some medical report and you say, this is this. So you will have some, depending on how many tests you have done, you will have anything ranging from five to 15 to 20 numbers on that piece of paper which the lab has produced. Right? Now, if the doctor were to say, okay, looking at this piece of paper, I declare that you have XYZ condition. It will be very hard for you to expect to believe that. What the doctor will say is that, look at this number. Look at that number. So those two numbers are outside the range that I would normally expect, and they typically indicate something. And therefore, I believe you have. So you narrow down the suspects. So it's the same as saying that you have a complex explanation which says, test whatever. Number one is this and number two is this, and number three is this, and number 15 is this, and therefore. Or you say, number two is this and number seven is that. And therefore, in any context, generally, a smaller model is usually better, but everything is. And this is a common feature of anything we do, machine learning. Nothing is. You cannot say it universally, as I said, like newtonian mechanics and quantum mechanics, you could always have a situation where the more complicated model is required. So, yes, certainly small trees are more explainable. It'll turn out that small trees also solve the other problem that we mentioned earlier about generalization. So, question is whether or not this model is better for the data you have not seen compared to the data that you are training on. So is it too specific for the training data that you are building. So if you ask more questions, then you are really focusing on more individual eto synchronicity or complications in the data that you see. That might be an isolated case, and it may not appear in general, so maybe you should ignore it. I will show you later on an example of this. But the problem is that as a computational task, so I have not yet told you how to do this, but if you wanted to make this your goal, build the smallest tree, which, among other things, gives you this kind of a nice boundary of uniform answers. The problem is that this is a task which is what is known in computer science as NP complete. Now, you don't need to know what this is, but if you are interested, I will put up a reference. But this is basically saying that sort of the only way to guarantee that you'll get the smallest tree is to try every tree, and there are going to be enormously number, exponentially number of large number of trees, because I can choose anything I want for the first attribute, and then I can choose anything I want for the second question on each path and so on. So I have to try out all these combinations and then evaluate which one comes out smallest. And short of doing that, assuming that this p is not equal to NP, there is no guarantee or no known way of doing it. So this is a little bit of a disappointment that we cannot actually hope to find the smallest tree in a reasonably efficient way. And this doesn't matter how you define smallest, right? So smallest could be in terms of how many questions you ask, in terms of the depth, number of nodes, that are there in the tree. It could be any of these. So instead of this, what we are going to do is to try and approximate this by choosing the next question as best we can. So this is what is known as a greedy status. So you have a number of choices to make. You have a number of columns to choose from. You look at the column, which appears to give you the best benefit right now, and then this is not always going to give you an overall optimum strategy. Sometimes you need to make a suboptimal choice now to get a better choice later on. But we are going to follow this greedy thing where we will have some criterion to choose one attribute and we will apply it one by one at each step. So what is this greedy strategy? See, first, to understand the greedy strategy, we have to understand what we are trying to get to. So we are trying to get to this situation as fast as possible, right? So what do we want? We want to reach a partition in which everything is yes or everything is no. As soon as we get to that, we can stop asking questions. So that is our goal in some sense. So we start with our training data and we want to ask questions so that every path terminates as quickly. These uniform partitions. A partition which is uniform is something that we will call pure. It is either pure yes or pure no. Remember, we are just assuming everything is binary, so we can just assume the categories are yes or no, right? Whatever they are supposed to be, it's yes or no. So our goal is to achieve all yes or all no as fast as possible, no matter what trajectory we choose. So given this answer, I need to choose the next question so that it will converge to all yes or all no as fast as possible. On the other hand, if I reach a node where I do not have all yes or all no thing impure, then at that point, if I have to stop and make a prediction, then I will choose the majority. In the worst case, it could be the last node. I don't have any more questions to ask. Then I have no other choice. But if you stop me midway, supposing you stop me here, I found out that this person is young, right? So what should I answer? Supposing I'm not allowed to ask one more question, right? So I've just done this and I've come here no, because there are three no's and there are two yeses, so it is no's and two yeses. So I should answer no, because that's a majority. Whereas here, if I say the person is middle, aged. Then without asking any more questions, I say three yes and two no, and I'll say yes. And if the person is really old, then I have four yes and one no. So with more certainty I can say, so I could stop this algorithm and answer even earlier. 

So we have this impurity, right, the majority value, and then we have the minority value. And we will basically want to minimize the minority to zero. If the impurity becomes zero, then I have a pure node. Okay? So our goal is really to go from some. So I start with obviously some kind of impure node, and I want to go to a pure node along each path. So the idea is to keep purifying in some sense, this node, by reducing the impurity as much as possible. So that is the heuristic. The heuristic is to try and make the resulting partition nodes that I create as pure as possible. And so I choose a question according to that. So I have an impurity here. So what is the impurity here? I had nine yes and six no. So the impurity is six by 15, which is 0.4, right? So the impurity is the ratio of the minority to the total. So it's two by five, which is zero, four. What is the impurity? Here it is again, two by five. The minority is two out of a total of five. What is the impurity here again? Now notice it's minority. It's not saying yes or no, right? It's saying which of the two answers is the minority and how many of those are there as a fraction. So again, the impurity is zero. And here impurity is one out of five. Okay? But now I started with an impurity of zero four. And I produced three nodes with impurity, zero four point. Let me look at this situation. I started here again with the same impurity .4 because I had the same starting point. And now impurity on the left hand side is at this point. How much is impurity here? Zero. Right? I have no minority. What is impurity here? Look at the bottom. There are nine things. One third and three are wrong. So it's one third. So we can say 00:33 just for sake of. So now the problem is, how do I compare this combination with this combination, which is better? So that's why we now say you take the weighted average. So we say that here I have one third of the nodes end up with zero four impurity, one third of them end up with zero four impurity, and one third of them end up with zero. Here I have six by 15, right? So two fifths of the nodes end up with zero four impurity, zero impurity and three fifths of the node end up with. So this, maybe it is better to write it as a fraction. So this now resolves to zero plus one fifth. So this gives me zero. This plus this plus this is zero plus one third times three fifths is one fifth. On the other hand, here I have two by 15, plus two by 15 plus one by 15 is equal to five by 15 equal to one five equal to zero point. So what I'm saying is, now I've started with an impurity of zero four. If I ask the age question, then my resulting weighted average impurity reduces to 00:33 but if I ask the own house question, it reduces to zero two. So the reduction or the purification process is moving faster in the own house case. So if I have to choose between age and own house, own house is a better question to ask right at the beginning. So I will ask this of all the four options. So, initially I have four options. So there is nothing magical about this. I ask for each question I can evaluate, because the training data is completely known to me. So I can ask each question and see how the data splits. And the data will split into some ratios. The ratios need not be equal. That's why you're taking the weighted average. So we are saying in one third of the cases, this happens, in two thirds of the cases that happens, and so on. So you take this weighted average, you get the average impurity after you ask this question. And what you want is that average impurity after you ask this question should be as low as possible. So you choose, among all the attributes which you could ask, you choose the one which reduces that thing as much as possible. So any question about this. So we have to check for every possible order of asking. I'm only asking one question at a time. So for the first question, I'm saying I will ask all four. I have four columns in this case, so I will apply this at each question separately. So initially I have four. So I will ask for each of these four. What's going to happen if I ask this question as the first question? So, notice that I'm not really asking this question. I've not asked this question, right, really, I'm just saying that if I ask at the top level itself, maybe I should. So what we are saying is that if I ask age, then I end up with. If I ask house, it end up with six years or end up with three years, six years. So similarly, I would ask what happens if I ask what is it thing, John. So there will be something. And if I ask credit. So I will evaluate these four things. So I will now take a weighted average. So the weighted average is saying here each is five. So it's one third, one third, one third. So I take the average, I take the impurity of each combination, times that as the weighted average. Here I take, in this particular second one, I take six by 15. So two fifths and three fifths. So again, if I look at job, we'll have to see if I look at the job column. So look at the job column. False, false, false. 56789. So there are nine where the job will be false, and there are six where the job is true. So whatever impurity comes out of the true set will be six by 15. And whatever impurity comes out of the false set will multiply with nine by 15. I'll add it up and I will get the net impurity of that. So that's how I do it. So I will do it for all these four cases, and then I will base on, based on the answer I get for that. So based on the answer for that, I will choose one of these. Now I will come to the next question. So now I have only three columns. So I will choose among those three columns on the current impurity, what happens. So it will depend on each situation what happens, because I'll be working with different subsets of. So yes, it is exhaustive, but it is not something, I'm doing it one level at a time and one node at a time. So we choose the minimum one. So that's basically the idea. But it turns out that this is not the best heuristic to use, and that's only by the fact that it's not the best heuristic to use is more by people trying to build these trees and finding that this doesn't necessarily do a good job. It's not, theoretically, not easy to explain why this is bad, but there is a certain logic to it, which is that this misclassification rate is linear. What it means is that if this is the, so this is the fraction of so the two classes, let me say this is yes. Everything is either yes or no. If I tell you that 60% of the columns are yes or rows are yes, then you know that the remaining 40% are no, right? So you know the other side. So the other side is fixed. So if I have zero yeses or if I have all yeses, right? If 100% or a fraction of one, then the misclassification rate is zero, because the minority case must be zero. It must be one. 

I mean, basically, if this is the case, then the minority is. So I'm plotting the fraction of things where the answer, the class is one. The class is yes. So if the fraction is zero, that means this is the minority. The majority is the other class, and therefore, there is no misclassification. If everything is C equal to one, then there is nothing of the other class. And in between, as this grows, the misclassification rate grows. When both are 50 50, then that's the maximum. Right? So I have. Maybe I should blow this up. So this is the picture, right? So at 0.5, when half the rows are one and half the rows are zero, the misclassification is 50%. And then the answer flips, which is the majority, and which is the minority flips. So, once the ratio of category one crosses 50%, this becomes a majority and the other one becomes a minority. And as the number of rows with one grows, the minority shifts. So you understand this linear curve, right? So this is basically talking. So the y axis is the misclassification rate, and the maximum misclassification rate is half, because at most, the minority can be half. If it is more than half, it's a majority. Okay, so, empirically, it turns out that it is better to have a function which looks like this, where if you deviate from a pure state, so notice that either everything is zero or everything is one. So if you deviate from the pure state, your misclassification, or whatever penalty you are associating, should grow faster than linear. That's what you really want. Now, why? As I said, this is basically an empirical observation that if you can design such a function where you can. So what are you trying to do? You're just trying to associate a quantity given the split of yes and no, you're trying to associate a numerical quantity with that which is different from just the ratio of the minority. The ratio of the minority has this linear thing. You're trying to find a different function which gives you a better nonlinear curve. So it turns out that there are two such functions which have been studied. So one comes from information theory and it's called entropy, and the other one comes from economics and it's called genie index. So let's just quickly look at what these are. So entropy comes from information theory. So it comes from a fundamental piece of work by Claude Shannon related to how many bits you need to transmit a message. So what Shannon said is that you need to really look at the relative frequency. So if you are taking a typical message, which is, say, written in a natural language like English, and you have to encode each letter as bits, then you should take the frequent letters and give them shorter encoders. Because there are more E's and t's going in your message than there are, say, Z's and Q's. So you use longer sequences for z and Q, shorter sequences for E and T. And overall, your message length will be smaller. So that was really the motivation. So, formally, he tried to compute the other way around. What is the minimum number of bits that you will need to send a message? And that is the centropy. That is how the more random your message is. So if your message is perfectly predictable, if the outcome of the message is known, you don't have to send any information at all. So nowadays, because I think cell phones are cheaper, but one doesn't do it anymore. But this infamous missed call. So everybody knows about the missed call. So what is the missed call? Missed call is a prearranged signal, right? It's a message whose answer is known. It says, I am at the gate or I have reached or something. So when you go there, send me a missed call. When you arrive at the door, send me a missed call where the existence of the message is all that you need. You don't need to know what the message contains. There's no information to be conveyed because the answer is the message content is fixed. There's no randomness. Whereas if I'm trying to tell you the sequence of ten coin tosses, then I have to tell you the sequence. Because there is no way, if the coin tosses are truly random, that I could ever compress it into a smaller. So, this is what Shannon was trying to capture. So, here we are dealing with two cases, zero and one. So what we do is we split the n columns, n rows, rather as n zero and n one, right? So n is equal to n zero plus n one, the number of rows in which the class is zero. And the number of rows in which the class is one. And I interpret these ratios as probabilities. So, p zero and p one. The p zero is the probability of zero, which is a number of zero rows. Number of rows where the last column is zero. P one is the probability of one number of rows where the last column is one. And now you just have to swallow this. This is Shannon's formula for input. It's p zero log, p zero log to the base two p one log p one. And the reason you have a minus sign here is that, remember that p zero, p one are smaller than one. So log of p zero, log of p one, they're going to be negative numbers because two to the zero is one. So if you're less than one, it's some one by two to the something. So it's two to the minus k. 

So I add up this two quantities and I negate it, and you get a curve that looks like this. So this is our old linear curve, which goes up to half and then comes down, and this is this thing. Now, the only difference is that of course it's going to go up to one. And the reason it goes up to one is that when p zero and p one is half, I'm going to end up with half log half plus half, log half, and log of half is what, two to the minus one. So this is minus half, plus minus one, which is one. So what Shannon is saying is that when both outcomes are equally likely, when p zero and p one are both halves, then you have maximum randomness, and the randomness otherwise drops off like this. Now, you have a small problem, which is, what do you say for the randomness at the pure state? At the pure state, one of these probabilities is zero, because either p zero is zero or p one is zero, because I have zero rules of zero, zero rules of one. So you have to technically evaluate zero log zero, and zero log zero, as you probably know from calculus, is not a great thing because log of zero is not defined. You have to do some limits and all that. But for entropy calculation, zero, log zero is just defined to be zero. So log of one is going to be zero. So if I'm at an extreme case, either p zero is zero and p one is one, or p one is zero and p zero is one. So one of the two will become zero because log of one is zero. The other two will become zero because zero, log zero is declared to be zero. Anyway, the important thing for us is that this is the curve of the entropy. And independently, and quite unrelatedly, somebody called corrodo jenny, who was a genie who was an economist, was talking about inequality. So he wanted to measure inequality. How unequal? So, supposing something is distributed in society, how do you measure whether it's being distributed? So wealth. So if the wealth is distributed, so that, for example, you have, many times you have seen this, right? 50% of the wealth is with 3% of the population or something. So then the society is very unequal. So this is the kind of thing he was trying to measure. So in his case, again, he used the same thing. You have two categories, and you want to measure the inequality. If n zero is equal to n one, that means something is equal is very large compared to the other is unequal. But his quantity was expressed as one minus p zero squared plus p one squared. Again, you will notice that if this is half and this is half, then I get one fourth plus one four, and I get one minus this, which is equal to half. And you can check that this is the largest value that you get. So in genie index, if the two values are half, then you get an index of half, and otherwise it will turn out to be much smaller than that. So again, you can plot the genie index curve. And now these are the curves. So this is our old friend. This is entropy. I mean, sorry, this is our misclassification, linear. This green dashed line is genie impurity, and this is entropy. But entropy goes up to one, which is out of scale. So we have the entropy, we get this red. So, notice that the entropy line is slightly steeper than the genie impurity line. And our logic earlier was that somehow nonlinear is better empirically, whatever that means. So empirically just means that somebody built these systems and found that if they use these kind of things, it works better. So empirically, entropy is, I mean, theoretically, entropy is slightly steeper, but entropy requires us to compute logs and all that, whereas genie index is just this computing squares of numbers. So genie index is actually easier to compute. And if you look at, as we will see, libraries in Python and all that, typically they use genie index. So although there is a very famous system built using entropy, which was one of the early decision tree systems, the ones that we will practically encounter in Python, libraries actually use the genie index. Basically. I'll stop with this right now. So what we are saying now is that we want to choose the next question to evaluate the next question to ask. So for the next question to ask, we have to say something about the quality of the impurity before and after. So if I just look at the raw ratio of the impurity, I have a motion of just that impurity percentage. But that gives me this linear relationship, which apparently is not that good in practice. So I want a nonlinear way of giving some impurity value to this ratio of yes or no, and then we can use either this entropy idea or this genie index, and it gives me a sharper curve, and this sharper practice turns out to be better. So that's how you build a decision tree. You build a decision tree by applying one of these two nonlinear things. If you come up to the third thing, it's also fine. It's just that these two happen to be standard things. They are both borrowed from other areas where they have a long and established history. So, as I said, genie index comes from economics and entropy comes from information. So we'll stop here. So, any questions? It okay, if there are no questions, then we'll continue with this discussion of decision trees on Thursday when we meet next.