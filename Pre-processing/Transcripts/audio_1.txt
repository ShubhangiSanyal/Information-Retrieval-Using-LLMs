So welcome to this first lecture on this course, DMML. So today I will try to set some context for the course and also tell you a little bit about start the first topic. So I have set up some information just a few minutes back on the moodle page to give you information about the list of topics, roughly that we plan to cover and also about the assessment and all that. So if you have any questions on that, you can look it up and maybe next time we can discuss that. So I won't spend too much time right now on the administrative aspects of the course. We'll just start looking at what we are going to do in this course. So if you look at the title of the course, it clearly says two things. It says data mining and machine learning. In a sense, this is a kind of historical thing. That's how this course was initially created some many years back. So if you want to look at the two parts of the title in some detail. So data mining is a loose term which talks about identifying some hidden patterns in data. So we should think about mining in the sense of real mining. So when you do real mining, you're looking, say, for some precious gold or diamonds or something. So you will be digging through a lot of mud or stone or chipping away at rock in the hope of finding something precious, but you don't know for sure it is there. You might have some guidance. So in that sense, data mining involves going through vast amounts of information which has been collected for various reasons and searching for these nuggets of insight. So there are various other aspects which go into this. One is the fact that this data has to be collected to begin with. So historically, a lot of data was collected manually through forms and so on. And one part of the problem would be that these forms were manually entered by somebody and then converted to electronic forms. So that would be, there would be two levels of potential sources for errors. The person writing down the information and then the person typing in the information. Now, gradually these kind of electronic forms are spilled in directly, so at least the source of the error is reduced to one step. But still, people mistype things. I mean, there are any number of situations where people type their email address wrong and so notifications don't reach them and so on. So there is this data collection. How do you collect the data and how do you clean it? And the third thing is, how do you make it uniform? So when data is being collected by different people, they may collect different things. And for instance, if you look at the government, typically the government collects data in different forms. For instance, there is a public distribution system which the ration shops, so they collect some information about who is collecting ration and who is income levels and so on. Then on the other side, you have, of course, things like electoral information. Then you have property information. Who owns property, who pays electricity bills, who owns vehicles, vehicle registration, driving licenses. Not necessarily the same thing. Not everybody who has a driving license owns a vehicle. So you have all these different sources of information. Now, is it possible really to find out same person in all these? Well, a lot depends on how the information is collected. You have a common format in which you record information about people. Even names in India tend to be spelt in different ways, written in different ways. Sometimes we expand initials, sometimes we suppress them, sometimes we write middle names, sometimes we don't. Sometimes we invert the order, sometimes we don't. Addresses, of course, are written in a million different ways. So there are all kinds of issues with just getting the data to a format where you can work on it. So this is an entirely different ballgame. It's much more kind of. There are lots of tools and techniques to deal with this, but that's not really going to be the focus of the course. So, in a sense, if you want to think about it, this data mining aspect will be almost missing from this course, even though it's part of the title. So what we are really going to look at is the machine learning aspect. So learning means you are trying to understand something that you don't know before. And machine learning suggests that it's done automatically. It's done by now machine as a computer. So there is an algorithm which learns something about, again, it's always with respect to data. So it's something about the data. So what we are trying to do is learn some kind of mathematical models from data. And this falls into two broad categories. So the first category is where you are given examples. So you are told, for instance, this is, for example, supposing you're trying to evaluate whether a painting is painted by a particular type of. So there are these pools of painting, like impressionists and cubists and so on. 

So you might be given a lot of pictures saying, this is cubist, this is not cubist, this is impressionist, and so on. And then from these examples which are given to you with manual information, you are supposed to build a model which will take an unknown painting and tell you from its features which of these categories. So this is a typical supervised learning kind of thing. Unsupervised learning is something which is closer to the data mining situation where you have data and you are looking for patterns, but you don't really have a clear idea beforehand what patterns they are. So a typical example of this might be that a company which is selling something might want to know some information about the demographics of its customers. So what are the groupings? I mean, which age groups? What is the proportion of people who are, say, between 20 and 30 who buy their products, people above 50 who buy their products, and so on. So this kind of thing is unsupervised because you don't know what you're going to get. But after you get this information, maybe you can put it to good use. So these are broadly the two types of things that are there. There is a third type of machine learning called reinforcement learning, which we are not going to talk about at all in this course. So to look at supervised learning a little more detail, we will of course do it in much more detail in the lectures to come. So we are trying to extrapolate. So basically it's a prediction problem, right? We have some historical information and we want to predict something in the future based on this historical information by building some model, some mathematical model using which we can make these predictions. So, for instance, when a school conducts exams before the board exams, one part of it is for practice, to give the students some practice. So these model exams are partly to get students acclimatized to the structure of a board exam, but that they could very well do on their own. But another, perhaps more relevant thing from a school perspective is to try and understand how well their students are likely to do so. The model exam performance can be seen as a predictor of board exam performance. And if there are concerns, for instance, the school feels that some students are not prepared enough and they are going to do badly, they might use the model exam performance as an indicator to kind of counsel the students about postponing their exam. So this is a situation where you're trying to predict something based on something that you have seen. And of course, how do you do this prediction? Well, you presumably know in previous years something about how correlation is there between model exam performance and board exam performance. So that's the model that you want to build. Another thing might be a bank. So bank is handing out loans, so somebody comes to apply for a loan. Now you have a historical policy and historical data about previous customers and whatever profile information you collect about them. So you might collect information about their income and their assets and their liabilities and so on. And based on that, you'll make a decision. So now you have a new customer. You want to know whether, based on this person's profile and the historical policy that you have been following, should this loan application be granted. And of course, today, all around us, one of the things that we are most concerned with is medical diagnosis, which is also in the same spirit. Right? So each time a new strain of COVID comes, it comes with its own symptoms. It takes a while for people to recognize that sometimes it is loss of smell, sometimes it is some stomach problem, sometimes it is just cold and a sore throat. So each time we have to kind of look quickly at people who have symptoms, who diagnose themselves, who get tested, who diagnose positive, and then figure out which symptoms are actually indicating the new brand of COVID So these are all in the same broad category of. So in all of these things, the manually labeled historical data is available. So we know from previous years students how they did in model exams and how they board exams. We know previous customers, what their profile was and whether they were given a loan or not, whether they defaulted on the loan. So not just whether they were given a loan, but also their future behavior. Right? At one point you give a loan, but later on it's not necessary that you made the right decision. So if you didn't give a loan, of course you don't know anything because that customer is not with you anymore. But if you did give a loan and for some reason that loan did not work up the way you wanted, the person defaulted or something, then that's a red flag. So maybe you need to go and check those things. And for medical things, of course, over time, not just recently, everything in medicine is really based on this kind of maintaining of records and looking for these kind of models across patients, across different countries, across generations, people maintain these things to look for these patterns so that we can identify quickly when certain symptoms indicate certain conditions. 

So what we really want to do is this. So this is our golden aim, which is to go from historical data to a model. So we want to build some kind of a model which we can use for prediction. And that's really going to be the focus of what we are going to do in this course. We are going to look at many different types of models that are used for this. So within this, there are two types of predictions that we make. So one type of prediction is actually a numerical value. So we are trying to predict, for example, board marks, an insurance company might want to know how much a house is worth, right? So somebody comes to insure the house. And they say, I want it to be insured for, say, 75 lakhs. And the question is, is this house really worth 75 lakhs or is it overvalued or undervalued? So the insurance company needs a way of looking at various features of the house and determining whether this is an accurate value. So again, it's a numerical question. And of course, for loans, indirectly, what the bank is trying to assess is some kind of net worth. How much does this person own in terms of all the property and bank deposits and shares, and whatever else this person may have, minus whatever the person owes in terms of previous loans and other liabilities, what is the total actual net worth of an individual? And the other type of prediction that we make is something that we encounter quite often in our own life. So one is, of course, email. Junk mail, right? So almost every mail reading program or client or mail software like Gmail has a way of flagging some mail as being junk. So somewhere there is a machine learning model which is looking at different features of the mail and deciding whether this is junk or not. But the learning part of it comes because junk is not really something which is uniform. Not everybody perceives the same mail as junk. Similarly, this insurance thing, if you go to the next step, if somebody actually makes a claim, first we said we want to value the house and decide how much to insure for. But if something is already insured and somebody makes a claim, one of the first things to check for an insurance company is whether this is a genuine claim or not. Is there some fraud involved or is this a genuine claim? So how much time and how much effort do you spend in doing this before you pay out it? If it turns out that you spend a lot of effort and something turns out to be genuine, and then you pay it, you're paying double. It makes a lot of sense. If you spend some effort and time to check a fraud and you save on paying out something which you should not. So there is a trade off there. And this is, again, something where if you had an efficient system which was based on machine learning, you could possibly get some benefits. And finally, banks, we have looked at other examples like loans, but also credit cards. When you apply for a credit card, a bank will typically go by your income and various things and decide whether to give you the card or not. But nowadays, if you get the card, there may still be a further refinement. The banks issue many different kinds of cards. So they have some premium cards and they have some normal cards. So they might decide that based on your income profile or your economic profile, that you are willing to pay something extra for the card to get more benefits. So this categorical thing may not be just two ways. So the idea is that these are kind of more or less binary. Either it is junk or it is not junk. Either you pay or you say it's a fraud, but this is something which is a multicade. So it's not just yes or no, it is no. And then in yes, there are two parts. Another similar thing could be that if I try to tell you whether this news item is sports or arts or it is a book review, so if I look at some news feed which is coming from some press agency, and I want to put it in the right place, I need to know whether it's about politics or sports or something. So, topic classification of documents is also a multi category classification topic. So, as we said, these models that we use are of different kinds, and each of them is, there are two parts to it. There is a generic model, and then there is some specific part of it which needs to be tuned according to the data. That is the learning part. So we have to fit the parameters to the model. So, for instance, the simplest kind of thing that you can do is to kind of, if you have some points, try to fit a line through the. All of you have seen this at various points in school and all that when you are doing experiments. Now, the shape of this line, so the model is a line. So that is known. But what is the unknown? What is the position of this line? What is its y intercept? What is its slope? And this will be determined by the point. So that is the sense in which you learn. The parameter fitting is the learning, as we said before, even junk mail, junk mail is filtered out based on some characteristics. It could be some terms which come in the subject, I have a great offer for you, or my husband has left me a million dollars. 

So these are all signals that probably this is some junk which is coming your way. But there may be more subtle things. Somebody might be talking about events, some kind of pop music events, and another person may be talking about some spiritual gatherings. And one person may be interested in pop music events, and another person may not be. And another person may be interested in spiritual gatherings, and the first person may not be. So then you can train these things. So in that sense, which words signal junk and which words don't signal junk is also a parameter of the model. Just like in this linear fit, the shape of the line is a parameter. So this is the learning part. So what we are going to do is look at different types of models, as I said, and then we will look at this parameter adjustment. How does the data actually determine the model, the concrete model? How do we build the best model that we can get for the given data? That's the algorithm. So we have a kind of model template. So we have a model template on this side, we have training data, and what we have now in between is a machine learning algorithm which comes out and produces a specific model. So in a sense, it's different from a typical program that we write. So typically we have a problem to solve. The problem to solve is given to us in saying, say you want to compute some numerical quantity, you want to compute, say, the log of x. So your problem statement is given x return log of x. And then you come up with some calculational strategy which allows you to compute this quantity, and then you implement it as code. Here it's different. So here you are given some kind of a format. You're saying you want to fit a line, and then you're given data, and then you're supposed to now take this line fitting thing and make the best line out of it. So it's not quite generating a program from a specification, but generating a concrete program from a generic program you can think of a model template as. So I just give you a line here which says a Y equal to Mx plus c. And here I want a specific m zero and C zero, which fits the given data in the best possible way, which is also part of the description of the algorithm. What does it mean for one line to be better than another line? So that's part of it. So the other side of this picture, as I mentioned, is when you don't have this training data, you don't have any examples that people have already labeled, but you are looking for some kind of patterns. So there is no guidance. So the thing about supervised learning is somebody should have gone through and given you this information before. Now, this information could have been collected over time. Maybe it's not manually collected, like the school marks. The school marks over time. Everybody has. I mean, the school keeps a record of what happens in the model exams, what happens in the board exam. So it comes automatically. The bank will be keeping records of all the loans that it bought and what happened to the customers who bought loans. So this historical information is accumulated over time. On the other hand, in some situations, like the example I mentioned about classifying paintings, now, when you go into a museum or you go into some art gallery or something, the painting doesn't have a label on it, saying, this is a painting of this type and this is a painting of that type. So somebody, if you want to fit this training model to it, somebody has to sit and actually label these things for the algorithm. So there is a lot of work in it. So generating a lot of valid training data is itself a kind of computationally intensive task. How do you provide a lot of well trained examples? Because the assumption is that the labels that these training samples have are somehow correct so they can be trusted. Because if you can't trust those labels, and the model that you build will obviously not work. So if you want to write generalization of the labels that people have given you, you have to trust those labels. So that means somebody reasonably conversant with the domain has to be willing to put in the time to label it. Now, if it happens, as I said, because historically you're collecting data, it's fine. But more and more, we are finding that this is not the only source of data. A lot of data now actually comes out of automatic things like we have systems which are producing. For example, if you run a computerized device like a network switch or something like that, all these things generate a lot of diagnostic data. Even things like cars and planes and vehicles, which run with a lot of electronic components, generate a lot of diagnostic data. That's why whenever there's a crash, for instance, people are looking for the same as black box. Now, if you're trying to build a model based on that, then who is going to sit through and go through all this diagnostic data and tell you that, oh, this part of the data suggests that there's a fault here, and that part of the data suggests that there's a fault there. 

That's something that is not very easy. So that's where this unsupervised learning becomes actually important. So, as I said, a prototypical, almost canonical example of this is customer segmentation. So you have different types of people who are selling different products, who are interested in this. As a newspaper, for instance. You want to know what kind of people are reading your newspaper. Now, this has two reasons. One is, of course, that it guides what you write. People target their content. I mean, news is not in some sense an objective quantity anymore. So you can take news and you can interpret it according to what your customers would like. So some of them may be, first of all, politically left wing, right wing, and then old and young. So you might focus on certain types of news. If you are targeting a younger audience, you might spend more parts of your paper on items pertaining to sports and entertainment. And if you are targeting an older audience, you might have more articles about the economy and politics and so on. But the other side is also advertisers. Right? So, advertisers also look at this. So who advertises in your newspaper will be determined by who reads your newspaper. So, for a newspaper, it is important to be able to figure out who their client is. And how do you find this out? Well, you, of course, figure out through some sampling or through some explicit customer information, details about all your customers or some large fraction of your customers. And then you have to look for patterns. You have to look for these groups. So the same thing might happen in a shop. If it's a shop which sells multiple types of things, it could be a clothing shop which addresses different age groups, different genders, different types of clothing, might be sending fashion clothing and sports clothing and baby clothes and various things. Then depending on how many people are visiting of different types, you might realize that one category is taking up a lot of space and it's totally useless, or you're making a lot of money in one category, but it's underrepresented. So, again, this customer segmentation helps. And, of course, something which has become almost a kind of a disease now is that we are no longer seeing entertainment because we would like it, but we are saying it because they know that we like it. Right? So Netflix, everything, Amazon Prime, Z, everybody is now producing tv shows and movies based on what they perceive that people will like. So it's not surprising that most of what they produce is popular with at least one large segment of the audience, because that's how it's designed. But this is a bit strange, if you think about it. But this is based initially on some kind of unsupervised learning. You look for what people know, and then you can also use it to recommend. So when you log into one of these things and it suggests that you watch something, how is it suggesting that? So you have not watched that thing. So there has to be some way of correlating what you like with what that movie is about. And there are two dimensions to this. So I am a user and I am recommended a movie. So why am I recommended this movie? It's because I am like some other user, and there is a movie like this which this person liked. That means I must have some way of computing similarities between users and movies. Or it could be that I am a user. I am like this, I like m prime. And this person liked m. And therefore, if these are equal, then I should also like. So there are many of these kind of pictures that one could draw, but all of it boils down to looking for these groupings. We want to look for groups of users who have similar tastes. We want to look for groups of movies which are similar in nature because similar groups of people watch them. People who watch one also watch the other. So these are some aspects of this unsupervised learning that are used quite a lot. So at the heart of unsupervised learning, one of the most core ideas of unsupervised learning is the idea of clustering. So you want to take a kind of. So this is the top, a kind of generic collection of points. And then you can see in this generic collection that there are some obvious groups, right? So you can see that there are, of course, these three groups here which are quite evident. And then in this most fast thing, there are these two clones, right? And then if you formally run some kind of an unsupervised learning model on this, then it will try to partition this geometric space into groups of points which go together in such. So it will find, in this particular example, these five partitions. And this will also include places where there are no points. So there are no points here and here and so on. But this allows you to do some kind of prediction. If somebody produces a new point in one of these empty spaces, you can sort of assign it to be of the same type as the nearest group that you are from. So in that way, you can use this unsupervised learning as a way of also doing some kind of classification, which is a typically supervised learning task. 

Now, when you do this, you need to also understand something about how the data looks. So there could be some data which is what is called an outlier, which is absolutely against the average. So typical examples of these are the people who are extremely wealthy. So if you are talking about something about to do with income, I mean, some property to do with the income of an individual, then if you have extremely wealthy individuals who are out of the scale in some sense, and then if you include their income in the calculations, then usually everything gets distorted terribly. But unfortunately, until you examine the data, you don't know this distorting factor, and usually you cannot. How do you know whether something is distorted or not? Usually you want to check whether it's very far away from the average, but then you have to compute the average. How do you compute the average if you have very large sense of data. So in some sense, how can you find these anomalous points without actually doing an exhaustive calculation? Can you do it without actually computing the mean and standard deviation of the entire data set? Another thing that happens usually in this kind of machine learning is that we are looking at different attributes. So, as I said, when you do email classification, you might classify an email as junk based on various factors. One is, of course, the words that appear, but the words that appear also appear in two places, in the subject and in the main text. And then there's a question of the attachments. Are there some attachments, large images or not? And then who is it from? Where is it coming from? So each of these is a different dimension to the data. And now if you look at a bank customer who comes for a loan, then you have even more dimensions because you have their income, you have their previous year's income, you have some, whether they own a house, what is the value of the house, and so on. So you have this high dimensional data in which you're trying to do something, and very often it's very hard to work because it just multiplies out into too many possibilities. So one of the things that one can do sometimes is to actually reduce the dimensions. And sometimes when you reduce the dimension, the problem becomes easier. So here is an example. So you have this kind of, imagine this is a kind of a carpet, the thing on the right, like an exercise mat or whatever, you take this carpet, which is colored like this, and then you roll it up. Now, if I give you this rolled up version on the left, then if I ask you to decide, given a point, whether it's green or yellow, it's kind of difficult, because there is a three dimensional twisting of this data, which doesn't make the picture very obvious. Whereas if I'm able to unroll it and produce this two dimensional thing, then there is a very simple line which separates the green from the other. So this is a situation where by reducing the dimension, somehow, you make the problem easier to tackle. But this is an unsupervised thing. Reducing the dimension is something that you do blindly in the hope that it gets you a benefit. So I can take another example of the same type, and I can start with something which actually, in the original thing, is separated by a simple plane, right? So I have this plane here which separates the green from the yellow. But then, believing that it'll be better to work in two dimensions, I actually unroll it. And then I find I have to deal with these multiple bands. So now it's not so obvious anymore because I do have lines, but I have many of them, and I have to decide between which bands it is, green, and which bands it is, this yellow color. So the peril of supervised unsupervised learning is that you don't know that the step that you are taking in advance, you don't know is going to be good or bad because it's unsupervised. You're just taking a guess in some sense, but then you have to have some way of evaluating whether the guess was good or not. And then going back and doing it again may be a different. So this is kind of a quick overview, a summary of the main things that we will be looking at. So, supervised learning and unsupervised learning. So we are looking either to build a predictive model, which is broadly a classification model, or a numerical prediction model. As we said, in case you're trying to predict a number or category, the category is more like a classification problem, right? I give you something and I want you to classify it as yes, no, good, bad, or we have unsupervised learning where we are looking for some kind of structure, and usually this is in the form of clustering, but we also have to deal with these outliers, as I said. And then sometimes you want to simplify the problem by knocking off certain dimensions or unrolling the object or so. So if you look at the popular press and the popular literature, all the focus of machine learning is actually on the first one. 

So supervised learning is by far the more attractive and the more, should I say, attractive, I guess it's the more attractive aspect of machine learning. This is what people are really looking for. But unfortunately, as I said, to do effective supervised learning, you need to actually get a lot of training data. And this is sometimes expensive. Very often it is expensive. So unsupervised learning is often the starting. A few. Couple of years back, there were three turing awards given for deep learning. So one of them was given to Jan Lakun. And his observation is that if you think of this whole thing as a cake, then actually most of the cake is actually unsupervised learning. So supervised learning is just a thin layer on the top, but it's like the icing on the cake. So when we go to a bakery and we see an attractive cake, what we're attracted to is the surface, the icing surface. But if the inside is made of sodas, then the cake will not taste good. So you have to be careful not to be fooled by the icing. So the unsupervised learning is also important. But supervised learning is where all the, in some sense, the jazz is. That's what people spend up spending a lot more time on. And we will also probably spend, in terms of lecture hours, more time on supervised learning than on unsupervised learning. But don't forget that the bread and butter work of machine learning actually rests on a strong foundation of unsupervised learning. Okay, so I'm going to start with something after this. So if there are any questions at this point, I'll just take a minute to pause and ask, then continue. Okay, you. Fine. So now let's start our first topic. So this is a topic which is actually of historical interest because it is how this whole idea about learning from data and data driven decision making, as it's called, came into the forefront. So this is something which was originally suggested in the context of retail people who run supermarkets and department stores, that their various aspects of their business could be improved if they had information about this kind of thing. People who buy one product also tend to buy another product from one side. It could be in terms of deciding which products to focus on, could be deciding how to cross sell, as they say, how to design discounts so that people will buy more. It could also be something as mundane as how to organize the products in your shop. Where do you put. Now, if you go to any big shop, any reasonable size shop, you will find that similar products are in similar places. So that's for a reason that it's the same way, the reason why library books are arranged, typically subject wise. So if you're looking for something and you don't find what you're looking for, then similar things will be nearby. But now, if people who buy X also buy Y, it may be that X and Y are not directly similar, but they are related in some functional way. So it's better to put them. So, for instance, if you go to, say, a supermarket, you might find that things like baking powder and things like that are kept close to sugar and chocolate and things which are used for baking. Now, these are not the same product, but they have similar uses. So this is one thing, of course, there is also a devious situation where if you know that everybody who's going to buy one will also buy the other, they can put them far way because then people are forced to walk through your shop because you know that they've bought baking powder, they're also going to buy something else related to baking, but they know they have to walk to the other end of the shop. But of course, more likely such people will not come to your shop. So it may backfire also. So you should be a little careful about how you do this. So there is a very legendary story related to this. So I just mentioning it because you will come across it in a lot of articles. So there's something about how in the 1990s, some shop discovered a correlation between diapers and beer. So it said that certain times, on certain days, people found that when men went to buy diapers, they also bought beer. A lot of speculation as to what this could mean. And the question is, of course, whether it happened or not. So I would think you should go and read this reference which I've given here. So it actually gives a historical picture of this. But there are various accounts of this, whether it was true or not, whether it actually happened or somebody made it up. But it is certainly referred to in a lot of popular articles and even in some textbooks as to the motivation for doing this kind of market basket analysis. So the claim was that there was actually a shopping chain which discovered this and made use of this. Now, the more I would say, believable end of the story is that somebody found this out. There was no immediate way to make use of this fact. 

Maybe it is true that at certain times, people who buy diapers also buy beer, but it at least gave some confidence to the idea that you can use this kind of information to make decisions. So from that perspective, it possibly had a role to play. But anyway, so these are the origins of this problem, and it's called market basket analysis because it's really talking in this context about shopping baskets. I have a shopping basket when I reach the counter to pay. So question is, what is in that basket? If it has x, does it also have y? So this is a very literal interpretation of markets and baskets. But the notion of markets and baskets can be a little bit more abstract. So, for instance, if you are thinking about, especially nowadays, given the number of elearning platforms, there's a lot of emphasis on trying to structure topics in such a way that students who find difficulty, you can identify why they are facing difficulties. So you want to think of these concepts that you're teaching as these items. And basket is now a set of concepts. And if you look at a concept which is difficult to learn, maybe there is a connection to another concept which is also difficult to learn. So you might want to group this. So people who misunderstand a also misunderstand b. And therefore maybe the problems has to start by fixing b. And another place where this is used is in kind of similarities of documents. So if two documents share words, then you can think they are similar. So you can think of documents. Or if two words appear rather in many documents, you can think they are similar. So if you say that wherever x appears, y also appears, then it may mean that x and y are actually related concepts. So this is the context. So the real question is this, people who buy x also tend to buy y. So this is what we want to formalize and try to do a preliminary round of calculations to see how one might determine. So everything, as we said, is going to be done by first abstracting out the problem into something which is a mathematical model. So here, abstractly, the items that are available in this hypothetical shop come from a set, capital I, and there are a large number of items. Let us assume. So. In this particular thing, we assume that n is large. So imagine a large supermarket. So n could be in tens of thousands, hundreds of thousands. So what is a transaction? A transaction is a set of items that somebody buys. So already, now there are some simplifications. So, first of all, it is a set. So if somebody buys two tubes of toothpaste, we are only going to count it as one item, namely toothpaste. Secondly, what constitutes an item also is up to us. It will depend on. So are we thinking of items in terms of generic things like toothpaste, toothbrush. Are we thinking in terms of brands like Colgate toothbrush, toothbrush of this type? And then are we thinking of, say, in toothpaste, are we distinguishing between 50 grams toothpaste and 100 grams toothpaste as two different items? So depending on the problem you're trying to solve, the nature of what constitutes an item will also change so many of these problems that we do in machine learning. First, you have to understand what it is you're trying to solve. You can't just blindly step in and solve some problem, because the level at which you're solving the problem may not be useful for the person who is facing the problem. So we have to decide what is an item. And then we have made the simplifying assumption that a transaction, we are only going to look at distinct items in the transaction. So we are looking at the shopping basket, we are only looking at different items. We're not looking at multiplicities of items. So now, over a period of time, we collect information about these transactions. So we have a set of transactions, some m transactions, and again, we can assume that M is large. And now we are going to base this as our starting point to kind of figure out this. People who buy X also buy y, so that we now need to formulate. So we want a kind of association. So people who buy x also by Y says that these are, first of all items. So in general, now these are sets of items, not individual items. And very often in this area, these are called item sets. The question about m, no, m is not the number of items in a basket. M is the total number of baskets. So each Ti is a subset. So each Ti is a basket. And I observe many customers. I'm trying to generalize the data from a large number of such customers. So each customer buys a basket, and that is one subset of the items. And I'm looking at a large number of such subsets in order to determine whether or not a kind of observation about x connects to y holds or not. 

So now, yeah, so what I'm saying is that it's not necessarily that only, like people who buy toothbrush also buy toothpaste. It could be something like people who buy toothbrush and toothpaste also buy soap and shampoo, right? So there could be sets on both sides. But these sets, if they are sets, even if they are items, it doesn't make sense to say, people who buy toothpaste also buy toothpaste. Right? So if they're individual items, certainly you would expect x and y to be different items. And if they are sets, you don't want them to have the same element on both sides. You don't want to say, people who buy toothpaste and soap also buy toothpaste and toothbrush and soap. It doesn't make sense. So X and Y can be assumed to be disjoint. And what we want to formalize is the idea that whenever the items in x are in some particular transaction TJ, then it is likely. So what does likely mean that y is this is what we want to do. So the association rule captures this intuition that I have two sets which are disjoint, right? X and y two sets of items. And whenever I see all the items in X in a transaction, I am likely to also see the transaction from Y. Remember, X and Y are two separate sets, so they are two different things which come into the basket. So I need to talk about this likely. Now, clearly, likely is something which I must apply what I think is parameter for likely. It's not something which is universal. So I must give some kind of a ratio. So how often does it happen as a fraction? Right. How many times when I see x, do I see y? That's the first. So this is just saying, what is the frequency of doing this? So is it that 10% of the time when I see x, I see y? Is it 1%? What am I interested in? Am I interested in something which occurs? 10%, 20%, 1%, half percent? I have to give a threshold. And if I give that threshold, then with respect to that threshold, there will be certain x and certain y. The threshold is lower. There will be more x, which satisfies the threshold, because I'm asking something weaker if it is a higher threshold, if I expect 50% of the time, people who buy x by y, then there'll be fewer such x and y. Now, the other part of it is whether this is worth knowing at all. So, supposing I tell you that people who buy Rollsroyce often buy leather seat covers. Now this may indeed be an extremely significant thing in terms of percentages. That is, maybe 90% of people who buy Rollsroyce actually put leather seat covers on their car. But as far as the person selling cars, Rollsroyce are so rare and expensive that it doesn't have any meaningful implications in terms of what you can exploit. So it is a negligible sample in some sense, of the total set of cars which are sold. So any observation I make about Rollsroyce may be just due to the fact that both are expensive items and people who are buying an expensive car are willing to put expensive seat covers and may not have any other indication. So it may not even be worth exploiting. So I need to talk about two thresholds, right? That is, is this pattern worth knowing at all? And if it is worth knowing, is it a pattern at all? How frequently does it happen? So, as I said, these are going to be percentages or frequencies or whatever. So I need to essentially count things. So if I take any subset of items, I can tell you how many times it occurs as a whole in my transaction. So, this is the set of all transactions such that z is entirely there in that transaction. And I want to know how many of them are there. And that is my Z dot count. Remember that m is the total. So z dot count is going to be less than or equal to n. So z is a set of items of. For example, I look at the end of the day or at the end of the month at a supermarket, and I ask you, how many transactions during this month did the person buy toothpaste and toothbrush that's what? Z out of the total. It's not a ratio. It's a total. It's a count. So what do I want to know now? I want to know as a percentage, how often does x imply y? So if I buy X and Y, then I buy x union y. So it's union because these are disjoint. So if my shopping basket has both x and y, there is a tendency to think in terms of intersection. That's why I mentioned it's not that x is there and Y is there. So X intersection. Y is there. So X is a set. Y is a set. But they are disjoint sets. And I want to see both of them. So this is really a disjoint unit. So I want to see baskets which have both x and Y and compare them to baskets which have only x. So these are situations where. So, of course, everything which has x and Y also has x. But there are baskets which have x and Y does not have Y has only x. And I want to know how many of those are there. So that's what this is saying. So we want that the number of baskets which have both x and Y as a ratio of the number of baskets which has x should be above some threshold that we specify. So this is, again, something. So this could be 1%, 10%, 5%, 0%, whatever. So it could be zero, 1.1.3. It's going to be a fraction. 

This is going to be less than one for sure. Less than equal to one at best. Every time people buy x, they buy y. So x union y will appear as many times as x, but cannot appear more times than x because every time x union y appears in the basket, x alone also appears in the basket. Right. If I buy toothbrush and toothpaste, I am definitely going to see a toothbrush. So if I want to know whether toothbrush implies toothpaste, then I look for toothbrush and toothpaste on the top, and I look for just toothbrush at the bottom. But everything which has toothbrush and toothpaste implicitly has a toothbrush. So therefore, the top is going to be no bigger than the bottom. So this ratio is going to be somewhere between zero and one. And I can decide what, for me is a level which I deem to be interesting. And this is called confidence. So I fix this chi. So this is, this greek letter chi looks like an x. So greek letter chi. So I fix a confidence level, and I ask for some x and some y whether this is true. The other part was that whether it's worth doing at all and this worth doing at all is what is called support. So it's like that Rollsroyce and the leather seats. How many of my cars that I actually sold were Rolls Royces with leather seats, because I don't see enough Rolls Royces with leather seats. Then this whole correlation that I'm discovering is not of much use. So here I'm taking the right hand side of my rule. So I have a rule of this one. So, I'm looking at things where this is true, where x and Y are both there, and saying, how many times is it true as a fraction of the total. Remember, m is my total. And I want this again, at most can be one. It could be that every transaction has x union y, but it can't be more than M. So this is again, number between zero and one. And now I want to know whether this is above a certain threshold. So again, these are two independent. If I see at least 1% transactions which have this, then I will say so that is independent of the confidence. So, confidence tells me whether the rule is valid or not. Is it true that x implies y? And support tells me whether that rule is interesting or not. I mean, is it worth my while to actually pay attention to this correlation? So now, the interesting thing about this problem is that if I fix these things, if I fix the items, and if I fix the transactions, and if I fix these two thresholds, then for every x and every y, either this statement is true or it is false, right? Either these two inequalities hold or one of them fails. So there is a kind of correct answer in some sense. So this gives me a fixed set of valid exercises. So there is a solution, which I am trying to find, which is fixed once I fix the basic data. So the basic data is only the items on the transactions, but my parameters, which are user defined parameters, this chi and sigma. Once I fix those, then the answer is no. So this is not, therefore, in the typical sense of a machine learning problem. Because usually in a machine learning problem, the answer is not known. If the answer were known, then you would not need machine learning. If you knew exactly how to tell whether somebody is going to pass their exam. You don't need to build a model, you just use whatever deterministic algorithm they have. Or if you know whether or not this loan is going to default, then you directly decide. The problem is that those things, there is no fixed way of knowing the answer. So it's also difficult, therefore, to tell whether the answer you are predicting is right or wrong. So here it's not yet, first of all, a truly learning problem. It's more an algorithmic problem, but still, it's an interesting problem nonetheless, because I just want to illustrate how it affects the way in which we calculate what might be trivial with small data becomes nontrivial with large data. So this is the problem. So given a set of items, capital n, which is large, and a, given a set of transactions m, which is again large, and given these two ratios between zero and one, find every pair x and y such that x implies y is a valid association. So we can break up this thing into two steps. I mean, we want to first check whether x implies y is worth looking at at all. So we first look at the support part. We want to know whether x, sorry, yeah, we want to know whether the count divided by m is bigger than the support, which is the same as taking this m to the other side and saying whether the count is bigger than a certain fraction of the total. So the first idea is to identify those sets whose count is frequent enough. So these are what are called frequent item sets. Find every z such that z dot count is bigger than or equal to sigma times n, where sigma, remember, is something that we provide. But then we want to know which sets occur at least this many times. So this is the first step. Then after this, we have to then decide whether z breaks up, as after this, we have to decide. That's so once we know that z is worth looking at, then we have to check whether I can break it up into x and y and get a root. So the first problem we are going to look at is just this one. How do you determine all the z whose count is at least this much? 

So how do you do this if I ask you to? For instance, if I'm giving you a stream of numbers, you have seen this kind of thing, which comes sometimes on the news, on tv or something. You see the stock market coming at the bottom, right? You have a row of numbers going past you. So supposing like that, I give you a row of numbers, and I ask you at the end to tell me how many times each number appear. So assuming that you can record it fast enough, one thing you can do is you can maintain a counter for each number that you see. Each time you see a seven, you say, there's one more seven. So you can keep, if you want to do it manually. This is how votes are counted, for example, by hand. So I'll keep 123456. And so on. And then every time I see, I do this, this and I cut it off, this cut it off, and so on, right? So every five. So I can just keep a count. So I have one count per value that I'm tracking. I'm maintaining a kind of frequency count. So think of it as a large list or an array or a dictionary if you want to prefer that. But if it's a list or an array, then essentially you need to have one slot for every entry which could potentially come into your system. And then you need to maintain this counter. So for every z you maintain a counter. So what you do is for each transaction, you go through your transactions, and then for every potential subset that you see in that transaction, you increment account. So this is a standard way of doing this. And then after all the transactions are done, you go through all these counters and keep all those z's whose counter value is larger. So this is straightforward. So what is the catch? Here's anybody take a lot of time. Firstly, what is going to take a lot of time? Counting all the values till all the values are completed. And then we will, I don't know, there will be a lot of, in terms of time, what do you have? Right, so you have a loop which runs over all the transactions. So this is of size n, right. And then within that you have to go through. So we have to assume. So let us assume that each transaction is bounded by some size. So right now we are not looking at the rule part, we are not looking at the decomposition, right. We are only looking at counting which sets as a whole appear sigma m times. So this will be some m times. And what is this? Now if I have each transaction as small m items, then I have in each transaction, I have to look at each subset of it and increment the counter. So there'll be two to the m subset. So it will be m times two to the m. Now this may or may not be large. It depends. So that is one valid concern. But somebody else pointed out in the chat that the other problem is that we have to do this for each z. So z is a subset of subset of I and I is of size n. So we have two to the n potential subsets. That means our counter space, our Dictionary, or our array or whatever we are keeping potentially has to be enormous. It has to be exponential in the number of items. And remember, the number of items is large. So this might be, I mean, if you look, imagine that it's a shopping situation. This might be small. This might be like 1020 kind of. But this is going to be like ten to the power six or ten to the power seven or something like that. So two to the ten to the power six as opposed to two to the ten. See, two to the ten is like thousand, which is not a huge. So the real problem is space rather than time, although of course impacts time also. But the real problem is space here. So the real problem is we have to maintain these two to the number of items counters. And this is in general going to be infeasible. So the real problem is that we don't have enough storage space to keep this count. And if we cannot count this, then we cannot of course apply that criterion. So therefore we cannot check whether a rule is worth exploring or not. So the second problem is still there. We'll come to that separately, which is having found these z's, how to decompose them as x and y. So at the moment, this is where I'll stop for today. So think about this. So we want to somehow do this loop. We want to do this loop, but we cannot afford to keep one counter for every set. So this is what we will do next time. So I'll stop with this. Any questions at this point? I had a question regarding the example that you gave that if there is somebody who buys toll stars, there's a 90% chance. Can you just say that a little loudly? I can't hear you, sir. I had a doubt regarding the example that you gave where you said that the people who buy Rollsroyce, about 90% of them would end up buying a leather. In that example, when we are calculating, say we are calculating m, what would m be in that case? Like when we say x union y. So x would be rollsroyce and y would be leather skates. But what would be. So here now, n would be the total number of whatever par. These are our items, right? This is the size of the set of items. So these are the items that people are buying and say across different car showrooms. 

So they are buying car, but they're also buying these kind of things. They are buying fancy seat covers. They are buying maybe some whatever reflective film for the glass. They are buying some fancy number plate, whatever it is. So this is n is the total number of items which people are buying. Capital MD. And then capital m will be in this particular case, we are going to see across all these shops a number of people buying cars. And along with the cars they buy some other stuff. So we are looking at a large number of sales of cars, across different types of cars and across different dealers and across different customers. So the point will not include the n for nose. That will not include just Rollsroyce. It would include all categories of. Yes, exactly. That's the assumption. Right. So if I'm only looking at Rollsroyce sales, then it's fine. But if you're looking at car sales across a large fraction of cars, then these higher end, very expensive cars will occupy a very small fraction of that spectrum. So it will not make sense to base any policy on this kind of. So in a sense, it's like an outlier we were talking about earlier. You would not base your policy on taxation based on the income of the extremely rich people. You will be looking more at middle. So, sir, in case that. Can we select our n and M based on the model we want? Like in case, for example, if we have something like a factory of somebody who makes car seats is trying to decide what kind of seats he should manufacture, what kind of covers. Sorry. He should manufacture for Rollsroyce. So for him it would be useful. Yeah, absolutely. So there is a context. Yeah, it depends on what is your setting in terms of items and transactions. So if you're restricting your setting to only, for example, high end cars or even only Rolls Royces, then the problem changes. Then of course, as a fraction, the support will go. That's not. No doubt about. So there's really an assumption about what data you are originally starting with. So if you're starting with the full spectrum of data, then you will have this conclusively. We can decide n and M based on what our problem statement is. Well, see, the problem I have phrased now is independent of interpretation. See, that is the whole point of this mathematical thing. I am assuming that somebody has given me a set of items and they've given me some set of transactions. I am not asking you what these transactions mean or what those items mean. So, n and M now for me are just input parameters to my problem. And these are just abstract items. I one, I two. And each transaction is some I seven, I 13, I 15 and so on. But if you are setting up now to solve the problem in a concrete setting, you have to decide what is the set capital a and what is the set capital t. So in that sense it will come. But I'm saying the problem we are trying to solve is not specific to rollsroyce or diapers or whatever. Toothbrush or toothpaste or whatever. It doesn't matter. Yes, sir. I was asking for that specific example. So therefore, I agree that in order to claim something holds or does not hold for a specific example, you have to be precise about what exactly are the interpretations of these values in that example. So unless you specify that, you cannot say that this is unlikely to be true or this is likely to be true, because it will be specific to the interpretation. But the problem now, we have now reduced it to one which is uninterpreted. We are just talking about abstract items. So there was a question in the chat about dimensionality reduction connected to PCA. So, yes, the PCA will be one part of it, but there are also other things that we will see. Sir, I have a question. Yeah. When we model this problem in this way, we didn't capture the frequency of items in a transaction. We were just looking at whether or not that item is present or not. Yeah, but I think for most of these examples, whether someone's buying from a shop or whether it's words in a document, that frequency will be a part of it. Right. Which we are not capturing here. Yeah. So usually this is how mathematical models work. You start with the most basic model, and then you add any features which you think are relevant. You can add them and then see what you need to do to improve the solution, to take that to account. So right now we are solving it in this set situation where the frequency is not counted. But it is true that you might want to do this. There are other things which you will look at later a little bit, and superficially at least. But for saying even this notion of what is interesting is somehow dependent on the items, because if something is being bought on a daily basis, then it might appear in a large fraction of transactions. That doesn't necessarily make it more interesting than something which is bought of higher value. So I'm saying that going back to something, I mean, shops might have different thresholds of interestingness for something like a pressure cooker compared to salt. Okay. The idea that there is a single notion, chi, is also a kind of oversimplification. Similarly for sigma, the fact that a threshold of transactions must appear before you look. The threshold might be much lower, again, for expensive items than for frequent items. So there are lots of variations that you can ask on this problem, and I will mention some, but we will not spend too much time on that. We will just try to solve this basic problem and then learn from it and vote. Right. There was some other question, one secondary. Any other questions? Sir, I have a question. Yeah, so the for loop we are running over here, basically, we are first fixing the z and then we are looking at each transaction, and then we are running the loop on all the subsets of the transaction, right? Because I'm getting confused between these two. Are they same or are they different? You can do it in two different ways. So what I am saying is that I'm overlapping the thing, right? So I'm saying for each p, this is what I'm doing for each transaction, for every subset. But now I can basically change this and say for each, because obviously I don't have to look at z, which is not mentioned in this particular transaction. So instead of looking at every possible subset z, I can look for only those subsets which are there in that transaction, because everything else does not get incremented. Now, what you are asking is the other way around. And this is potentially, I mean, superficially, they are the same thing. You have two loops and you are interchanging the order. But this is potentially much more expensive because there will be a lot of items, subsets which never occur. I mean, for instance, you might be looking for subsets where people are buying two items which are totally of different types of categories, and they might never buy these together. Like for example, a refrigerator and toothpaste, maybe. I don't know if a shop is selling both. Now, in principle, you are looking at every possible subset, and if you're looking at every possible subset, there'll be all kinds of implausible subsets also, which you will count, and you will only discover by looking through all the transactions, they don't. So in some sense, the left hand side loop is more pragmatic because it's only going to. So if you think about it in terms of dictionaries, for instance. So don't think of it in terms of array in a dictionary. You will create a counter for a subset only if you see it in your transaction. Whereas on the right hand side, you are actually creating a subset for everything which could potentially occur, whether or not that appears anywhere in your transaction. So therefore, the left hand side loop, which is what I've kind of written here, is more pragmatic because you're only going to keep track of those z which appear in at least one transaction. You're never going to look at a z which never appears. Whereas if you start by looking at every possible z and putting it outside, then you have a problem. Is that clarify? Yes. Thank you. Okay, so next time we will look at this question and see how to do this calculation more effectively. So, Thursday, see you. There's.