So last time we started with supervised learning in earnest, and we were looking at this decision tree model. So, in a decision tree model, you have essentially a table in which the columns are the. So each item, a row in the table and one special column denotes the class that you are trying to classify. And we assumed it's a binary classification. So the class is zero, one, or yes, no, or whatever you want to interpret. The two categories are. So the algorithm was very simple. At a high level, what you want to do is choose an attribute based on the answer to the question that you're asking about that attribute. The data splits, and then you take each partition, ask another question. So that's the thing. And you want to come down and keep asking questions until either you have reached, like these situations here, a partition in which all the values of the class variable are the same. So you have a uniform class level or you have run out of questions, because we can only ask in any path one question per attribute. And if in case we have the second case where we run out of questions, but we have not reached a so called uniform node, then we just use the majority as a prediction. So we will see an example of this later as we go. Yeah, so what we also said was that, as we can see here, we have two different trees, and the trees are not the same shape. And we argued that a smaller tree is probably a more desirable one. It's more explainable. And we also claim, without at the moment justifying, that it also has a better generalization property. That is, it will move from the specific training data to the unknown data in a better way. So how do we build small trees? Well, computationally, we said, I claimed at least that it was infeasible to build the smallest tree. So you would have to do a search through all possible trees. So instead we wanted to use some heuristic. So the main question that we have to ask is, which attribute do we query next at any stage. So our goal is to achieve this partition, which is pure, where all the values are yes or all the values are no. So we try to accelerate towards that. So that is this greedy strategy. So we use a heuristic, which will reduce the impurity as much as possible. And when we move from one node to its children by asking a question, what we do is we compute a weighted average. So for each node we can compute the impurity. But then when we combine the children of a node to get the impurity at the next level, we use a weighted average. So going simple weighted average just assigns a weight one to every item. So you just take the fraction. So for node one, you take the fraction of instances in that partition, multiply that fraction by the impurity and so on, and add it up, and we choose the one which reduces. So what we said was that this impurity function, if we just take misclassification as our impurity function, then we get this linear behavior, and it turns out empirically that we can do better. And so we suggested that there are two natural functions which are borrowed from other fields. So there's entropy, which is used in information theory, which talks about, in some sense, randomness. So the maximum randomness happens, as you would expect, when both possibilities are equally likely. And the maximum order or minimum randomness happens at the extremes when only one possibility is there. But the in between thing follows a much sharper curve. Actually, entropy goes like this between zero and one, but because misclassification only goes to zero to half, this scaled down by a factor of two. So the red line is a scaled entropy, where you just divide the entropy by two. So that is one thing which comes from information theory. And the other way of comparing these two fractions, that is the fraction of zeros and the fraction of ones, is using this thing called genie index, which is used in economics to measure inequality of, say, distribution of wealth, how much a fraction of wealth is distributed among the population. And there, again, the maximum kind of genie index happens when you have half. So here it is, this green line. So this green dashed line is the genie index. So the genie index fits actually under the entropy. So, entropy, our argument is that the steeper the curve, that is, the faster the thing goes away from zero when we get impurity, the better it is empirically. So, in a sense, entropy is technically better than gini index, but because of the fact that entropy requires us to calculate logs, whereas Gini index is just simply in terms of multiplication, it turns out gini index is easier to work with. And actually, as we will see, the packages which are deployed in languages like the genie index, in order to. So I just want to emphasize. Yeah, sir. Like we are seeing in Grady, rusty case also. And that even if these are linear, then at every node they are reducing the impurity level, right? Yes. So the same thing is doing either gene index or the entropy. Both are also doing the same thing. Correct. 

So why we are considering that and not the greedy heuristic? Yeah. So you're saying that if I'm going to compare, then the comparison will be the same in all three cases, right? That's what you are saying right, sir, and also Thansen are also behaving in the same way, first increasing, then decreasing. So it will not. I think there are two answers. So you are right in one sense, which is that if I actually have to only compare one choice versus another choice, then because they are monotonic in the same direction, the choice will look better whichever index I choose. So that is a valid question. So really it's more to do with actually stopping. So basically, if you put a threshold for stopping based on the impurity, then the threshold will be different at this end, right? So if I say that this is my threshold, then what is the corresponding impurity level at which that threshold is achieved? So I think that will be probably the reason why the sharper entropy or genie index does better than the sir. But even we see this as a computationally, then I think that greedy heroistic will be much better. No, these are all greedy. Greedy is only referring to the fact that we are. But linear will be better. It's not a part of the choice. The chasing is which value are we using to make the greedy choice. So that's the point. Yeah. So I guess linear will be better, even computationally. Right? Well, this kind of computation doesn't take much effort, right? Instead of comparing two values, you're squaring it and adding it. And so arithmetic operations hardly make any. See, log is a more expensive calculation. So that is the point. Take logs of small values. So logs of small values are also painful to calculate. So that's one reason you don't want to take logs. But otherwise, if I'm just doing multiplication and addition, it doesn't really cost much. Okay, so in the case of genie index, we are achieving a stopping criteria much earlier, right? That's the case, yeah. What we are saying here is that we are actually going to, we say that we are going to go until we either use up all the attributes or we reach a uniform class. Now, this is a reasonable thing to do if the number of attributes is small. But if you have a very large number of attributes, then it may not be really practical, and you may not want to ask 100 questions. So you may want to stop when the incremental gain of asking one more question is not much. So now that not much will now depend on the actual gain. So that's where the value will act. It's now an absolute question. Right? I have so much impurity now. And I have so much impurity after I ask this question. How much is the gain? So then it will matter what your impurity. Whether what you're saying is correct, that if I'm asking the difference between asking attribute one and attribute five, then the difference will be better in all notions. But if I'm asking you whether asking any question asking at this point, then it will depend on which measure I'm doing. So the actual improvement is an absolute value, and in that case, the choice of the value matters. Whereas if it's a relative value, is this value better than that value? Then all of them, I agree, will be the same. Okay, so one thing to emphasize is that I think in either the previous class or before, there was this question about whether interpreting the answers will help build a model. So if we know what these attributes mean, then can we build a better model? And I said that, of course, in practice that would be true. That if you know what attributes mean, you might focus on some attributes. You might, for example, if it is, say, some medical diagnosis, then you may not be very concerned about the date of birth. I mean the year of birth, yes, but you may not need to know whether it's January 27 or May 30. It may not make much difference. So there are some things which may be less or more relevant, or even place of birth may not be so relevant, and so on. So you might automatically throw out some. But in general, we don't know. So in general, we are running all these things. So the algorithm works just looking at the values without asking what does a one mean and what does a five mean. So this is something to keep in mind. So our strategy is to maximize the reduction in the impurity. And this terminology comes from that entropy interpretation. So in entropy information theory, entropy is loss of information. If you have more entropy, you have less information. If you have more information, your entropy reduces. Right. I discussed that if you know the message for certain, you have full information about what the message contains in advance. You don't even need to send the contents, you just need to say there is a message, like a missed call, and you would know what the message means. So, information is entropy going down is the same as information going up. So this is sometimes called maximizing. Reduction in entropy is sometimes also referred to as maximizing the information gain. But because this is an uninterpreted kind of algorithm, that is, we are just looking at the attributes and comparing the information gain without asking what the attributes really signify, we are just taking the one that works best. So the problem is that there could be some attributes which give information gain but have no significance. And a typical example of this is something which is like a serial number, right? So imagine that you have something like Aadhaar number or passport number, or in a class, you have distinct roll numbers. Every student has a different role number. 

So now, obviously, if I ask a question on that column, so the column has behavior like this, then what will happen is that for each, each answer, there will only be one person. There will only be one person with a given passport number and so on. So if I only have a partition of size one, then that one item is either yes or it's no. So there's no ratio to worry about. So the minority is always zero because there is only one item. So I have actually split my. So this is an extreme case where I've split my data into singleton partitions. So what I've done is I've taken some initial entropy or some initial information, theoretic notion of misclassification, and I've reduced it to zero weighted sum. But it's a weighted sum of zero. Right? Every branch has no impurity. So I get these partitions of size one. Each partition is guaranteed to be pure because it has only one item and the new impurity is zero. So I cannot do better than this. So such a question would then be picked up. And remember, this reason that the question is being picked up is because we have not interpreted. So you might ask, okay, why passport number? But it could be something indirect, like, it could be some kind of an encoded timestamp. It could be some actions which are recorded in some system, and then one of the columns records somehow the timestamp, and no two events happen with the same timestamp. So there could be any number of hidden reasons why something is actually unique. And either you have to clean up the data and check it, or you have to build a safeguard against this happening. It may or may not happen, but I just wanted to bring this up because this is a byproduct of the algorithm that we have now. It's a shortcoming. It's a shortcoming in the sense that if we apply the algorithm that we have now, right now, in such a situation, blindly, then we will end up with this kind of curious choice right up front. It will tell us that the only question you need to ask is, say, the passport number of the patient, and then you will be able to predict whether they have a disease or not, which is totally nonsensical. Right? So the greedy algorithm will say, take it, but from an interpretation point of view, it's a totally useless choice. I'm not saying that this will happen. I'm just saying that if you provide an algorithm in any machine learning situation, you have to be aware of the fact that this algorithm is going to be applied in general blindly to the data. So it has to be driven by the data. So if the data can produce some peculiar behavior, you have to be able to tailor your algorithm to take care of this peculiar behavior. The question is, how would we adapt our algorithm to fix this? So right now, our tree building algorithm blindly picks the attribute that maximizes the information gain. So we need to somehow penalize those attributes which have a wide number of possibilities. Now this is also intuitively clear. So if I ask a question to which there are 25 different answers possible, and I try to now make a decision based on the 25 different answers, and I have a different question which has only two or three answers, it's more sensible to make a choice based on the two or three answers. So the splitting into a larger fraction of number of pieces of smaller size is intuitively not the best way to design a good decision tree rule. Ultimately, these are rules, right? So I can think of a decision tree as saying that if the value is this and the value if I walked on a path, right? Saying if the person is young and the person does not have a job, then the person should not get a loan. So I can interpret every branch in this tree as a rule in that sense. So the rules become less interpretable if the choices split like this. If it says if the passport number of this person is r 347852, then do something. If it is t nine, four, eight, it doesn't make any sense, right? So this natural way to do this is to actually extend this notion of information or randomness or entropy to the attributes themselves. So let me take a typical attribute. It might take some k different values. So the question is, how random is this attribute? So this k values, now some frequency, n one, n two. So each value appears in my data. This is all with respect to the training data that I have. So each value vi or vj appears ni or nj times. So I can again interpret these ratios as probabilities and say that the probability of this attribute taking bi is ni divided by n. So n is the number of rows. So each attribute, we are assuming that these tables don't have the separate issue about what happens if you have missing values in the table and object. So we are assuming for the moment that we don't have anything. So all these tables are, the data is sort of cleaned up to the extent that there are no missing values. And so on. So if there are n rows, this attribute will appear n times and what fraction of them take the value vi. So that is PI. So then I can apply my usual things. So this entropy that we calculated was a binary entropy, right? Because it's a two category situation, we said take the fraction of zero answer and take the fraction of one answer and say p zero log p zero plus p one log p one, and add the minus sign. So in general, if I have k answers, I just take p I log p I and sum it up over the k and take a minus. So this is the generalized version of entropy where I have more than two choices. Of course, remember this, PiS add up to one, right? So because this is a ratio of these things, so I am really somehow thinking of them as being like probabilities. So this would be the entropy, and there is a corresponding version for gini. So remember the earlier on was one minus p zero squared plus p one squared. So now I'm just saying add up all the PI squareds, again, assuming that the summation of the PI squared. So in this way, I can apply the same criterion that I'm using to discuss the class randomness in this group of rows that I have. What is the split between yes and no? So that is, at the category level, I'm looking at the randomness of the entropy, and I can look at an attribute and ask, in this group of rows, what is the randomness of this attribute? And what we are saying is that we want to penalize those attributes which are scattered, which have lots of values. In the worst case, they have this one value per one item per value, like a passport number or something. 

So if I take this extreme case that each of these values occurs exactly once, then each of the correspondence, remember that p I divided by so vi is ni divided by Ni is number of times. So each vi happens exactly once. So I have one by n. So if I have one by n, then the entropy, for instance, each of the PiS is one by n, so I'll have k is equal to n. So this is the summation one to n, one by n, log one by n. And if you work this out, because this n comes here, so it's n times one by n, and then log of one by n is the same as one minus log n. So this is minus log n. So if you work it all out, it turns out to be log of n. So what is log of n? Log of n is a slow growing function, but it's a growing log of n will keep increasing as n increases. So if you have something like this, if you have more passport numbers, you will have higher entry. If you have more values, you will have higher entry. And you can do the same calculation for genie index also. So if it is one by n and you add it up, then it's n into n minus one by two. So you're saying that, for example, if there are five possibilities, it's going to be four, there are six possibilities, it's going to be five by six, and this is six by seven, and so on. And these fractions are actually increasing in value, right? If you go n minus one by n, as you get n larger, the ratio becomes closer to one. So in both these cases, what we are seeing is that if you have a larger. So this is the extreme case. But you can generally argue that if I have a larger number of different values, each with small probability, then the overall entropy or genie index or whatever associated with that attribute increases. That attribute is actually very random, and we want to avoid choosing such random attributes for our question. That will be a way to fix this apparent bug in our choice of attributes, where we blindly take the best one. So these increases and increases. So what we do really is we in some sense normalize. So what we want, we want to take. So the numerator is the one we were calculating before. That is, if I choose attribute a, how does the data split in terms of the categories? So that's the entropy or genie index of the old one that is in each group, how many s's, how many nodes, that's what display. But the denominator is telling me how good or bad a itself is. What is the scattering of a? So the argument here that I was trying to make is that as a gets more scattered, there are more values and the ratio of each value is smaller, then that denominator will become larger. So whatever gain you get on the top. So remember the problem was that these give us high gain on the top, but we are dividing that gain by the scattering of that attribute. So if we have high gain divided by high scattering, then the net normalized effect is lower. So you might choose an attribute which actually gives you a lower information gain overall, because the attribute itself is less static. And that's the thing that we want to achieve. Just to emphasize that when you have a machine learning algorithm, see generally, what is a machine learning algorithm doing? It's trying to describe the shape of the model based on the training data. So in this case it's literally the shape, because you have many possible trees you could construct, and the actual tree you construct depends on the data. And because you don't have any preconceptions about the data, you have to make sure that if there are these kind of patterns which disturb your algorithm, you will be able to counteract. So that's the purpose of this information gain ratio. It's not just the information gain alone, which is what we had initially. It's information gain divided by the entropy of the attribute. So is this clear what role this is playing? We are just saying that if you have a scattered attribute, then you discount the gain that you get from it by the scattering value. So more scattered attributes will, the net effect will be less because of the denominator. So in terms of our heuristics, what we said is that nonlinear functions work better in practice. And the genie index is in fact the one that is used in most libraries. But we should not blindly use this because of this problem of these scattered attributes. So we will actually use this ratio of business to correct. So we have this information. So now let's get back to this question that we initially observed. That is that something like this age, it would be much more natural if this age were described in terms of numbers. So somehow this data has been arranged so that we get this data in terms of categories. But who does the arranging? So how do we realize that in this context? Sir, I have a question, sir. In this loan example, if we add attribute like is an indian citizen or not, then the attribute has only one class, that is Indians. Suppose a bank has, in this case, the impurity is low because we have only one attribute, but it does not give us. What will happen is if you use such a category, then it will split. As what? Because if everybody is indian, for example. Yes, sir. Then after I text, if I ask the question is the person indian? Then I will get two groups. One group will have zero rows and one group will have the same set. Yes, sir. So therefore the same set will have the same entropy, right. And so the entropy will actually not change. That question will be totally redundant. Right. So I will get the one times the entropy of the table plus zero times the entropy of zero, which is just the entropy of the table that I started with. So the difference will be zero. So that situation actually does not create a problem. So if I have a trivial, it will actually get filtered out by the algorithm as something which is not useful to ask. Okay, thank you. And the same would hold, actually, if you think about it, if you have an attribute which is not trivial, but there are only very small minor, as you said, supposing one out of 15 is an Indian. I mean, not an Indian, maybe. Right. Then still 14 out of 15 will go into one category. So it'll have almost the same entropy as the original table. Yes, sir. It will not change much. So generally, if you have these kind of attributes which are not really meaningful because they actually cross across a lot of the classification boundaries, usually they will be autocorrect in that sense. Okay, sir. Okay, thank you. So now, returning to the question. The question is, how did somebody come up with this classification? Right? And now, as you can imagine, the notion of young, middle and old is actually a subjective thing, right? So if you're thinking in terms of, say, professional sports, then young may be somebody who's under 20 and middle aged, maybe somebody who is between, say, maybe under 25 maybe, and five to 30, 32. And then somebody who's in their mid 30s. You already consider NPO, but if you're looking at an organization in terms of the employees, then young people will be typically shifted up everywhere. 35 or so will still be young and middle aged will be, say, in their forty s and so on. And of course, in the extreme case, if you look at politicians and anybody who's under 65 is young and so on. So it really is a question of context. So how do you get this context? How do you break this? So, in general, we have to do one of two things. Either we have to be able to take these numerical values and assign, in some meaningful way, these groups, but these groups are also sensitive to the context, or we have to be able to work with the numbers. But then the question is, how do we ask questions about numbers? So let's look at an example. It's a very well studied example. So this is a type of flower called an iris, and it comes in three varieties. So these are the three varieties shown in pictures here. And there's a very famous statistician, one of the fathers of modern statistics, Fisher, who actually analyzed a data set consisting of these things. So he looked at 150 flowers, 50 of each type, and he measured two things in this flower. It's a little misleading. Normally, when we see a flower and we see the colored parts, we think of them all as petals. But botanically, in this, there are these leaf like structures, these things. And then there is something in the middle, which is something else, which is a separate bulb like structure sticking out. So these leaf like structures are called sepal, and the petal is this thing. So now you can see that in this one, for example, this is smaller compared to this as well. 

So there is a distinction visually between the size of the relative size of the petal and the sepal in these three types of flowers. So his question was, can I use this difference to categorize by just measuring these parameters without looking at it? I mean, there's also obviously a visual botanical way in terms of color and all that, of seeing it. But is there a data scientific way of looking at it? So he looked at the steeple length and width, so that is this and this, and he looked at the petal length and width, and he did it for 150 flowers, 50 of each type. So if you then collect these 150 samples and you just focus on, remember there are four parameters, right? There is a petal, there is a sepal, and for each you have length and width. So if I try to plot four parameters, it's a mess. So let me just pick one of the two. So let me just pick petal. So for petal, I plot it in this way. So this is the scatter plot. So the x axis is the length and the y axis is the width. And each dot represents a flower with that width and that length. And these three colors, this yellow circle and the blue square and the green triangle indicate the categories. So you can see that there is a kind of clear statistical correlation between the sizes of these petals and the three categories. At this, there is a very clear separation here. There is a kind of gray area because there are these mixed up things which have, like here for instance, we have similar values and you have flowers from both categories with similar values. So this is this iris data set, and you can actually construct a decision tree for it. So this tree, which we will see later, this python library for decision trees, actually constructs from the iris data set, this decision tree. So what you do is you start with these 150 samples, and initially there are three categories, so there are 50 in each category. And remember that if there are at any point when you have an impure sample, you have to decide on something. So this particular algorithm, they're equal. It happens to just choose one of them more or less at random as the category to predict nation. So this last line is the prediction, this is the distribution of values. This is how many samples there are. And this is the actual genie index calculation. If you think about it, it is the three samples are each with one third. So you have one third squared plus one third, one third squared, one nine. So one minus one 9th plus one nine plus one nine, which is one minus one thirds. So that's why you get this in the index of zero, 67. So this is the initial. Now, what this decision tree is doing at this point is it is choosing one of the attributes, which is in this case the petal length. And it is asking whether the petal number, so this number, if you look at it, is somewhere here, it's asking whether the petal length is less than 2.45, which is a number between two and three, which splits the yellow from the blue and the green. So if it is, yes, less than 2.45, then all the samples are in the smallest category and it is 500 zero. So all the yellow flowers fall in this category. On the other hand, the remaining hundred are equally distributed and it basically picks the first category among those. And now I have a different genie index of 0.5 because it's half an hour. And now what it does is it looks at a different attribute, it looks at petal width at 1.75. So if you go back to this picture, it's basically asking something like this at this point. And because we have this area here which is kind of not clearly separated by these horizontal and vertical lines, you end up with an impure slip, right? So you end up with this situation where you have majority of the second type and majority of the third type, but not all. So you still have some nontrivial genie index. It's not zero or it's not zero, but here, in this case, we have stopped to stop, because if I am here, for instance, I've drawn this line and I've drawn this line now, I can still take this and decide to split here and then split there and split there so I can ask more and more questions. But we don't. So that's one reason, going back to the earlier thing, that you might want to stop, because your genie index is small enough, and it may be different to stop for the genie index versus to stop for the entropy versus to stop for the linear misclassification. But our question is not that. Our question is where did you get these numbers from? Why did you choose 2.45? Why did you choose 1.75? Where did these numbers come from? Because obviously the algorithm is not seeing the picture. It's not like us interpreting that picture and saying these are natural places to draw the line. So how do you get that? This number will minimize the getney index? Is that. Yeah, but then how do you know it is Y 2.45 and not say 2.75 and not 2.15? So that's one question, and in this direction, it's a little more tricky, because the tricky is that if I move it slightly, if I move it slightly up or down, I might get slightly more or less. So how do I actually decide on that? So that's the question, is, how do I specifically pick on these numbers? So, obviously, you want to pick on it in such a way that you will achieve some improvement, but how do you find those numbers? And the real problem is that you have a range, right? So, for example, the petal range here is roughly from one to seven. The width is roughly from zero to 2.5. So you can pick any number in that range. You can pick any value v in the range and say, is the petal length less than equal to v or the petal width less than equal to v? 

The question is, how do you pick that? So, in principle, there are infinitely many choices. But even if I have a range in which I think I should choose, even in that range, I have infinitely many choices. How do we pick one? So the first observation is that we are always working with a finite data set. So even if the potential values. So we just used in this particular data set that there is a minimum and a maximum, there are actually very specific values for the length which appear in our data. So, in general, if I have some n data items for a particular attribute, they will take n values. For simplicity, I've assumed they take n distinct values. And these are numbers. So I can order, so I can order the numbers that I see in the smallest to the largest. So this is for one fixed attribute, then we will see what happens across. So, say we are thinking of petal length. So petal length across these 150 flowers takes potentially 150 values, and we can order them from the smallest to the biggest. So now let's look at an interval, right, v I to v I plus one. So if I pick any point in that range, and my question is of the form a less than equal to u, and I pick any u which is from vi up to but strictly less than vi plus one, then all of these points will say yes, and these points will say no. If I shift the u a little bit, it's the same. These points will say yes, these points will say so as long as the u lies strictly less than vi plus one, and it is greater than or equal to vi, everything from b one to vi will say, all the points which have values from vi to b one to vi, will say yes, and all the points. So the data will split in exactly the same way. Remember, what we're finally getting is a split. We're getting some of the flowers go into one category, some of the flowers go into other category. So every question that I ask in this interval has the same net effect as far as the algorithm. So therefore it really doesn't matter. So to go back to our earlier question, if I have that empty space, as we saw in that particular example, it doesn't really matter what question I ask about the length, if it is anywhere between this and this, it's going to give me actually the same value. So it doesn't matter because all the yellow flowers are going to be smaller than that and all the non yellow flowers are going to be bigger than. So that is the fundamental observation that actually because of our data being like this, there are only a fixed number of intervals, that there are only a fixed number of different questions. The form of the question can be many. I can pick many different u's that give me the same answer, but the number of different answers I can get is only equal to the number of intervals. Of course I could ask a stupid question which is to ask something here or something here. So if I ask something which is smaller than b one, everything will fall into the bigger. This will be like that earlier thing about indian citizen citizen I get no information. So if I ask for a petal width which is smaller than one, when I know that the range is one to seven, then I get no useful information. If I ask for something bigger than seven, I get no useful information. So only the intervals between these n values give me useful information. So there are only n minus one questions I can ask and these n minus one questions I can ask in many different ways. So one canonical way is to just use the midpoint. I can choose any u, I can choose any u in this range to ask for that interval. That interval which says everything smaller than vi is in the set one and everything bigger than vi plus one is in the set. So one way is to just pick ui vi plus v I plus one. So I just pick the midpoint of each interval and I ask a question saying is this attribute a smaller than that midpoint? So for each midpoint that I choose, I get a different split. So if I split it by u one, I get unsplayed, I have v one on the left and v two onwards, if I split it u two, I get v two on the left and v one two on the left and n minus two on the right and so on. So each choice. So I have this many questions I can ask for this one attribute, each attribute, each question that I asked splits in a different way. So remember that this is ordered value of a. So I could have an arbitrary pattern of zero one on top of this. So just because I split at a particular point, it doesn't tell me anything about what is the distribution of zeros and ones on the left hand side. That's what I'm really looking for, right? I'm looking for the split in which that entropy of what is on the left plus what is on the right is minimized. And that is something which is not directly connected to which value is smaller, because I'm just looking at the length. So the entropy is not directly connected in general. So I look at each of these, and I calculate for u one, I will get some more or less. Phrase it this way. If I choose the question as a less than equal to u one, I get some new entropy. If I say a less than equal to u two, I get a different one. Ui, I get it different un minus one, I get it. So for each of these, I will calculate the resulting entropy, or gb index, whichever one you want. So I have those n values, and I will choose the best one. So the best one amongst these is the most useful question to ask about this attribute. So this is how I will convert a numeric range into a question, which is the best question to ask about that numeric range. Is that clear? Yes, sir. So now what I can do is I can assign this. So let me, I think I'll make it more explicit. So I'll come back to this point in a minute. But remember that we said that any point, we chose the midpoint, any point can be used. In particular, the endpoint can be used, right? So we wanted u to be bigger than, equal to vi and smaller than vi plus one. So I could also choose this as a value. We have chosen the midpoint. I could choose that. So why would you use vi as a value? Well, it might be better to choose vi as a value if these things are not actually meaningful. So, classic example of this is supposing the answer to this question is how many, whatever, how many children are there in the house? Or how many rooms does the house have? So if I take the midpoint, I'm asking a question about a family which has less than or equal to three and a half children, which is not a very sensible question to ask. Whereas if ask less than equal to three children, it makes more sense. So depending on the nature of the attribute, it sometimes advantages to actually pick the endpoints, the lower endpoint of each interval and not the midpoint, because the question may not have an interpretation. If you ask the midpoint if it is salary or something, it's okay if it's an arbitrary number, but if it's some kind of a discretized number which only takes certain values, then you are guaranteed that the values that you see are possible values of that, but the values in between, the values you see may or may not occur in real life. So it may not make sense to ask. But the main point to notice that there are n minus one intervals and you can capture each interval by one question. 

The only thing you have to agree on is what is the choice that you use for this question, which ui you pick. So now just to revisit the way we'll do this decision tree where now I have mixture of categorical attributes and numerical attributes to choose from. So for each numerical attribute, what I do is I run through this procedure I just described to pick for that numerical attribute the best question to ask. So I choose that. So I've written it as v, but that Ui in some sense, which gives me the most information gained for that attribute given the current range of values. So for now, for each numerical attribute, therefore, I have a best question, and for the categorical attributes I already have only one question. What is the answer? Because it's one of. So I can compute for every categorical attribute, I can compute the entropy directly for every numerical attribute I compute the best split and take the entropy of that best split, and then I compare now across the categorical and these best numerical attribute questions, which is the best one to use. So in some sense I've converted without actually assigning this artificial label of young, middle aged old. I've directly from the numerical values extracted somehow the best possible question. So that's how we now incorporate numerical values. And that's more or less what is happening in that decision tree that we saw for that iris data. So one thing to remember is that earlier we said that when we have a categorical value and we ask the question, then occur uniformly in each partition. So there's no point asking the question again because once I've partitioned by the categorical value, then everybody in the first set is, for example, young, everybody in the middle set is middle aged, and everybody in the last set is old. So if I ask it again, I will just get this trivial split right, because there's only one value. But if I have a numerical attribute, I could ask the same thing again. So we saw that that earlier decision tree stopped here. But if I go back to that question picture. So that decision tree, the original decision tree was like at 2.45, right? And. .7. Now I am maybe interested in fixing this problem. So what I see is that maybe if I add a new line at somewhere here, then I actually divide this greens and the blues more correctly. So I'm saying this is, this. So I could ask a third involving the petal length. So I've already asked petal length here, but I'm asking petal length again. But this petal length is with respect to the constraint. So I'm not asking at this point. This is the non trivial split of everything which was bigger than 2.45. So I could ask the same thing again, but with a different value, because I need to shrink an interval which I have constructed after some steps. So in that sense, the number of questions that you could ask is not equal to the number of attributes technically, because these attributes actually have numerical values. So that's the difference between categorical attribute questions and numerical value questions, because numerical value questions you can ask again and again with the different right hand side and the different right hand side will come into play when you have narrowed it down to a smaller set. And now you want to again ask within that set. Okay, so this is the way that you handle numerical attributes and decision. So this is what I wanted to say right now. We will come back to decision trees later. And decision trees are a fairly interesting model in many different ways. But for now this is what I have to say about decision trees. I'm going to move on to another question now. So if you have anything to ask about decision, we can stop for a minute and just look at. Okay, fine. So let's get back to this other fundamental question that we had, which is, how do we query the effectiveness of our model? So how do we validate whether the model we have built is right or not? So what I had mentioned earlier is that there is a fundamental difference between normal, say test software validation and this machine learning model validation, because when we have some software, we write some code in a traditional setting, we have an expectation, we know something about how the inputs should be mapped to the outputs. So we can create this kind of a test suite which maybe checks the boundary conditions, as they are called. Some extreme cases where you want to make sure that the software is doing the right thing, and then we compare the output with what we know should be the output. Right? So we take the program output and compare it with the expected answer. So we have a notion of a correct answer to compare. So the problem with the classification model is that this correct answer is not known. I mean, if we knew the answer, if we knew a better way to come up with the correct answer, we would not build this model. So the reason we are building this model is we don't have a reliable way to compute the correct answer. So in this situation, when we do not have a reference value to compare our answers with, how do we measure whether or not our model is a good one? So on what basis can we evaluate? So what we need is a comparison where we know the answers. But the only situation where we know the answers is our training data. 

Our training data comes to us with labels. So for each input in our training data, we know the manually or however classified correct answer. So we are assuming those are the correct answers because otherwise our model building process will not make sense. So assuming that those are correct answers, we are building our model. So that's the only source of inputs and outputs where we know the answer. So the only solution really is to use that. So we have to take that training data and keep some of it aside because we are optimizing our model, as we saw in the decision tree, we are optimizing it to give correct answers on the training data. So therefore it doesn't make sense to evaluate it back on the same inputs which are used to construct the tree. But if I withhold some data, so I basically take my training data as a whole and then I kind of split this and say, do not use this part, only use this part to build my model. Then the model that I built from here, I can ask, how does it do on that? And there is no problem with this because the model has not seen that data. So it's not been biased by the data on the right hand side. So this data has never been used in the model building process. So it is as good as unknown data as far as the model, but it is unknown to the model. But the advantages is known to us. We know the answer, so we can compare what the model says with what the data says. So one important thing about this is that we need to choose this split randomly, because even though the training set is given to us with no information, usually training data is collected in some way, right? For instance, if it is some data, what I don't, let us say today's everybody's favorite data set is COVID, right? So it might be collected statewide. So it might be that though the data is anonymous and randomized and all that it's actually the first. So many tens of thousands of things are from a particular state, and then next state, and so on and so on will collect it and collate it into one large state. So now if I say take the first 80% and build a model and leave the last 20% for training, for testing, it might be that the first 80% all corresponds to one part of the country and the last 20% corresponds to a different part of the country. And then we have this problem that if there is a regional variation in the distribution of this particular classification, it will get badly affected. So we need to do some random sampling. So this is a general problem. But here specifically, we want to randomize. It's out of the scope of this course to do random sampling, but it's a standard statistical problem. Sometimes you also want to do what's called stratified sampling. That is, you want to preserve other ratios. Like you might have some information about age or gender or something. So you might want to say that if in the overall thing, the ratio of gender is 50 50, then in the sample that I keep for testing also should be 50 50. If the age ratio is in some particular distribution, I should keep a similar ratio. So this is called stratified sampling. But these are sort of different things. But the good thing for us is that in the libraries that we will use, this is an automatic step. You just have to say, split the data into train and test, and it'll do. So, training set and test set. That's it. So this is a little bit, what should I say? Confusing, because we actually call the whole thing also training, and then we split it. And then we again call this training and test. Right? So training data is a little bit of an ambiguous term. So training data is both the entire data set that you are given to start with. And then it is the specific choice that you make about how to split this data into a large part, which you will use for building the model, and a small part that you will reserve for validating the model. So depending on how much data you have, the usual practice is about 20% to 30% of it you keep aside. So you use about 70% to 80% of the data that you are given to build a model, and you keep a random subset of 20% to 30% away from the model building process. So it is unknown as far as the model or unknown data. It's the same as getting new data from the real world in some sense, and you can use it to validate. So, of course, this is a little bit of a disadvantage, because one of the features I mentioned right at the beginning is that typically machine learning expects that the model improves in quality with the amount of training data you give it. So you are in some sense artificially, or because you have no choice, more or less. You are building a suboptimal model, you have 100% data, but you are forcing yourself to use only 70% to 80% of the data available to build the model, because you need to validate it against some unknown data. So, one other strategy where you don't want to, I mean, one of the other disadvantages of doing this is that maybe that there are some minority anomalies in this data, and the choice of test data that you have made might have hidden all these anomalies, so you never see them, and maybe it's important to see them. So there may be many situations, or maybe you just don't have enough data as a whole to build a good model by only looking at 80%. So another strategy is to systematically do this with different subsets. 

So what you're really asking at some higher level is that machine learning approach. So, remember, we have seen only decision trees, but we are going to see many models. So there are clearly many ways to build models. And the reason, whenever you see that there are many ways to do something, it's only because there is no guarantee that a given way is the best one. So more or less what you want to validate is, I have, say, seven different strategies to solve this problem using machine learning. And I want to check which one of them is good. So I need to compare. So once I compare, then I'm stuck with whatever I have. It may be that this gives me only 70% accuracy or some such metric, but the other models give me even less. So it's more really a comparative statement to start with, which is, is this method actually giving me something reasonable? And is it giving me better something than something else? So, for this one strategy is so called cross validation. So what you do is you take some fraction, say, in this case, it is one fifth. So 20%. So you take your data and you leave out. So you randomize it into five groups, which preserve, in some sense, the random behavior, so they partition it five ways. So if I leave out any one of these groups, it's as though I have taken the remaining 80% for training and this 20% for test. But because I don't want to actually think of it this way, I will actually keep doing this for each one. So I will leave out the first 20%, then I will leave out the second 20%. So I will do this. If I have like k chunks, I will do this k times in each chunk. Each model I will build using all but one of these chunks. So this then will solve this problem of many problems. It will solve the problem where you don't have enough data to begin with to afford to not look at some of it, because the interesting things may be in that sum of it. Now, what will you do with this whole thing? Well, you have five. Typically, each of these models are going to be different because we saw with the decision tree, it's kind of the questions that you ask depend on the distribution of the different roles. And if the distribution changes slightly, one attribute might get better entropy than another attribute. And you will choose that question first. So the shape of the tree might vary a lot between these, so you may not get the same. Okay, so the question about cross validation, cross validation. What I'm saying is that you do not make a hard and fast choice once and for all to split your data this way. So you're taking your data and you're saying, hey, first let me leave out this 20% and build one model. So I am building multiple models. I'm building m one, m two, m three, m four, and m five. Each of these models is built using 80% of the test data, but a different 80%. So each of them leaves out some 20%. So I'm building five different models and comparing what I see in all these five. So the first thing is that if this approach is good, then in general, these five models should behave in a similar way with respect to the outcome. So if they all behave similarly, then I can decide what to do. So there are two strategies. I can, I can somehow combine these by saying, build a composite model, or I can say, okay, this strategy is working for me. So this approach that I have chosen seems to be a good way to build a model for this data. So now let me go back and take the 100% data and build from this whole thing, build a new model M, which will be hopefully better than any of these. M one, m two, m three, m four, m five. But I built those five models to allow myself to know whether this strategy I'm using is good or not. So once I've decided I have a good strategy, then I can do something to make the best possible use of the data to build a new model. Other strategies you can use is you can use these five models and vote. So any input that comes, you can pass them to each of these five models. Each of these five models will give you an answer, and you take the majority answer. So that averages out the errors across the model. So there are many different ways in which you can think of combining these models. In general, as I said, these models may not be the same. So I will not get a uniform answer to my question, but it will help me to decide whether what I'm doing is right or not. And then I can decide to do one of two things at least. There may be other strategy. One is to combine these models into a kind of a hybrid model which votes among them. Or I can just construct a new model which takes 100%. But now the second question, which this leaves us with, is, okay, I do all this testing, but what am I measuring? So what is the measure that I use to compare or evaluate a model? So, the most obvious measure is I'm given a set of correct answers and I'm given a set of model predictions. What fraction of these predictions are correct? So, this is what's called accuracy. So, on how much of that test set that I kept aside, on what fraction of the test set did my model make the right prediction? So, the problem is that very often the choice, let us stick to our assumption that we are doing a binary prediction. The choice between the two parts is not symmetric. So the thing we are looking for is very often a minority. So this is a graphic. Blow it up a little bit. So this is saying that if I look at credit card fraud, which is one of the typical examples of where supervised learning is used, right? So we look at different parameters of the transaction, where it was made, what time of day it was made, and how does it compare to other transactions made by the customer and whatever, and the bank wants to know if it's a fraud. So if you actually look at the attempted fraud. So this is going into the future, it's going into 2027. 

But let us look at even the past data, which is presumably accurate. So I don't know from when this dates, but one can assume that at least, say, up to, say, the last ten years maybe is correct, say, up to here. So what we see is that if I look at the volume of credit card transactions in terms of value, then out of $100 worth of credit card transactions, the total volume of the fraud transactions is less than $0.10. If it was one dollars, it will be 1% or less. So if you assume that it is proportional to the number of transactions, this is the volume in terms of the value but if you can also reasonably extrapolate and say that the total number of transactions which are fraudulent, attempted fraudulent, are still very small, so it is safely less than 1% because the value is less than 0.1%. And why would people be doing low value frauds? They will prefer you doing medium value frauds. So the cheapest transaction, the one that you go and buy toothpaste, is not going to be likely to be a fraud. It's more likely to be something where you buy some clothes or something. Somebody asked about shuffling before partitioning, right? Yes, the partitioning is done randomly, and I'm not going to talk about how it is done, but yeah, one way is to shuffle it, or you can hypothetically shuffle it by running through the index in some random way. Right. So sampling is a standard statistical problem, and it is not something I'm going to discuss most. But the libraries that we will use, which I will show you later on, have a standard way of saying taking this test data, split it uses, tell it how much you want, 70%, 40%, 30%. Users have to tell it, it will do. So it's a black box as far as they're concerned. But yeah, in practice, you will have to do some shuffling and. Okay, so now coming back to the question, now I have only 1%. So now I want an accurate classifier. So what would I do? So I have a classifier where I know that in this case the frauds are less than 1% of the total cases, which the classifier will see. So I can build a trivially accurate classifier by just doing nothing. I can just say no. If I just say no, then I will have 99% accuracy, because I know that the distribution of yeses is less than 1%. So this is highly accurate, but highly useless. So really we want to force the classifier to find the thing. And the usual assumption is that, as I said, this minority is quite common. I mean, email, junk mail is a much higher proportion, but in many cases the minority, the classification that people use it for in real life, is a really small fraction. So the real question is to flag the yeses because the no's are in a vast majority, and to say no and get away with it doesn't help me much. So I want to catch the yeses. So now, in any situation, we will see that you could have two types. You could have aggressive classifiers. So I have a borderline case, right? It could be yes, it could be no. So if I try to flag as many yeses as possible, I might be pushing some no's into yes. And these are called false positives. A false positive is when the test says yes. That is, a model says yes, but actually it's not yes. So the falseness is with respect to the output and the output. So this could be a bit confusing. But false positive means that the answer is false and the answer is positive. And similarly, the other way around. If I'm very cautious, I might push these borderline cases to no. Then I get false negative. So we have all heard about this in the context of COVID tests. So if a test comes out positive and you are not COVID affected, then it's a false positive. If you have COVID and it comes out negative, it's a false negative. The answer should say yes, but it says so. That's the false negative and false positive. So the thing is, accuracy is only answering two questions. I mean, it's only looking at a two way classification. Is the answer equal to or not equal to the expected answer? But what we are seeing is that the expected answers come in two asymmetric buckets. There's a correct yes and there's a correct no. So the actual answer could be yes or no, the prediction could be yes or no, and the mistakes come in two types, and the correct answers come in two types. Right? So we saw this false positive and false negative. So these are the two types of wrong answers where I expect to see a no and I say yes, that's a false positive where I expect to see. So if I expect to see a no and I classify it positive, it's a false positive. If I expect to see a yes and I classify it negative, it's a false negative. So these two are the wrong answers. Answers could either be that it is yes and I say yes or it is no, and I say no. And our assumption is that this is actually the biggest category. Right. Most of the cases are no because of the way we have assumed that the minority answer is yes. And if our classifier is any good at all, it should get most of the no's correct also. So true negative will typically be the largest fraction. So what we are really looking at is these four things. So this thing is called a confusion matrix, this particular classification. So now we take this accuracy Question, and we split it into two separate questions. Right. The first question is that it's really something related to accuracy. It's a question of Reliability, saying that if the classifier says yes, then what is the probability that it's really yes? The classifier says yes. In this column, right? 

These are all the entries which are classified as positive. That's a denominator. How many of these are actually correct? So that is called Precision. Right? So how many times, in some sense, does your classifier make a mistake on the yes? It's different from making a mistake on the no. That would be referring to this column. We are talking about this column. So the Precision says how many yeses that the classifier produces are actually correct YeSEs. And the issue you are dealing with with that trivial classifier is that I'm interested in finding out things in this row. These are the real frauds in the Clinical. How many of them does my classifier actually give me? So how many of these are reported in this box? So what is the fraction of true positives among all those that should be positive? So this is called recall. So this gives us, typically a finer demarcation between right and wrong. Now, the problem with this is that typically, as we said, we have this aggressive and kind of lenient classifier. So an aggressive, cautious classifier will not say, we'll come to examples in a minute. Will not say yes unless they show. So here it is very sure about one case, so it never gets it wrong. So here the precision is one. Every case that the classifier said is one. So there are people who can count it is 900 plus 99%. There are 1000. Right. And the split is there are hundred here and 900. So in the actual data, 90% are negative and 10% are positive. And the first classifier we build has this property that it is overwhelmingly cautious. It will usually say no unless it is very, very sure it will say yes. Now, I might want to make it more permissive. So I might want to move something from here. I want to give it some borderline cases I make. It stands to reason that some of these will also. Whatever criterion I use on the top is also likely to affect the bottom. So if I make it more linear or more permissive, I say positive more times, I will improve the recall. Right? So the recall earlier was one by 100. Now I've improved the recall to 40 by 100. That is this ratio. But as a result of moving those 39 cases on the top row, I've also ended up moving 100 case on the bottom. So I've dropped the precision down to 40 by 140. That's this 40 by 142 by seven. And if I move it still more, if I really want to get to very high recall, I want to catch almost all the positives, then the precision will go down further. Now, it's gone down to like one by six, almost. So it now really depends on what you want. So do you want to catch all the s's or do you want to be careful? So there are situations where you want both. So think of hiring. Right. You're trying to recruit somebody. When you do an initial screening, you don't want to miss out any good candidates. You will pass on some dud candidates. So you don't mind getting some poor people passing your screening test, but you don't want to rule out anybody. So you want high recall. Everybody who is good should pass your screening test. Then you have an interview. Now you're actually calling in your organization. You don't want to hire somebody who will be a misfit whom you're stuck with for years. So now you will be very strict. So you might have an excellent candidate, but there is something about this candidate you don't like, maybe the personality, or you think this person will not get along with. There's something wrong. So even though this may be highly qualified and may have been a great success, you will say so here. You want high precision. Everybody you select must be definitely selectable. You don't mind losing out on some good candidates for whatever reason. So depending on the nature of your screening, you might want to do again, similarly, in medical diagnosis, if you want to make everybody immunized against something or if you want to give them. So if you have a disease for which there is a simple medication, then you may not be very specific about the diagnosis. Anybody who comes with certain symptoms, this is more or less what's happening now with this omicron, right? Anybody who has a cough, cold and fever is assumed to have omicron, and you just tell them to isolate. So this is something which is high recall. You want to catch everybody who's sick and make them less of a risk to people around them. On the other hand, if it's some really critical diagnosis, it's like pancreatic cancer or something where you're going to die in six months or three months. You don't want to tell somebody that unless you're very sure, because that whole person's life will be and their family's life will be uprooted by this. So then you will have a second opinion, third opinion before you actually make the decision. So you will not just go on an initial suspicion that, oh, this looks like pancreatic cancer, and blurt it out to the patient. You'll make them both. So in many situations, it's not clear, but what is generally clear is that these two things are usually not aligned. You cannot normally do both together. You cannot have normally. If you have to trade off either you want recall or you want precision. And one will go up. 

If you want the other one go up, the first one will come down. So now I just mentioned that these are not the only things so people also talk about. So there is, for instance, the corresponding thing to precision for the negative part is specificity. Right? So what is the ratio of correct negative results compared to all negative results as a true negative? So that's with respect to this column, then accuracy, of course, is just saying how many of the correct things are there with respect to the whole thing. So that also can be measured easily from this. And then there's something called thread score and so on. So I'm not going to talk about it. And just for completeness, I will mention that sometimes people want to say that we want a single number. It's not good to give me a precision separately and a recall separately and let me choose, I want a single number. There is this thing called harmonic mean. So the harmonic mean is one upon p plus one upon R. So remember, p is precision one and it's one upon that. So it's the reciprocal of the mean of the reciprocals. So if you work this out, you get two PR on two p plus. So this is sometimes called the f score. Right? So this is a way of combining the precision score and the recall score into a single number. But as I said, in most cases, it really depends on the application. So either you want something based on the context which has higher precision or higher recall, and you choose. So these are some of the ways in which we can actually. So, remember, our starting point was that we wanted to evaluate classifiers. So we said first we have a strategy for evaluating classifiers, so we can test them by using this test set and so on. But what is it that you want to measure on the test set? So that we have this wide variety of things, but most commonly this precision and recall are the ones that people are interested in, and they will choose which one to emphasize. Depending on the context. Depending on the context, you may want precision predominant. You may want recall to be predominant. So there, it really becomes a domain specific. So I'll stop there. Any questions, sir? Like, given a precision or recall value, what we need to change in the algorithm so that we could come to the certain value of precision and recall. So that is a harder question. Right. So what is it that you need to change? So that's a very difficult question. All you can say is that given what we are doing now, this is what, then it becomes really an experimental issue. So this is the hardest question in machine learning, which is, how do I improve the quality of the classifier? So there is no answer to that question, which is obvious. So you have to start experimenting. You have to do something. So there is something called feature engineering. You might want to come out with some combination of features. So some of you may have heard of this thing, for example, which is used in medicine called body mass index. So, body mass index is some combination of some formula involving height and weight. So you might have a data set in which you have something like height and weight usually classify using height and weight. It doesn't work. But if you combine it in this strange way using body mass index, then you actually get a better class. So this is an example of what is called feature engineering. You make new features out of the old one, or you might need to change your model, maybe decision tree is not good, you need to use another model. So these are all very open questions, and I don't think there is any. If there were a clear answer to that, then life would be a lot simpler. But unfortunately, that is the real challenge of machine learning. What is it? If I am given a certain measure of performance, what is it that I should do to improve that measure? It's not clear. Okay, got it. Okay. Sorry to keep you over time. So next time, what I'll do is I will start by showing you some decision tree code from that python library, which is there in thing, and then we will move on to another module. So that will be on Monday. Bye.