So the problem we are looking at is this problem of market basket analysis. Like people who buy X also buy Y. So we are looking at these kind of association rules, saying that if there is a set of items X in the basket, then there is also likely to be a set of items Y in the basket. So formally, we have this set I of items, which is assumed to be a large set. We will talk about typical numbers and then also have 1. Second, we have a set of transactions. So each transaction is just a set of items. So we said last time that this is a simplification because a basket of items could have multiplicities. So you could have five of one and three of another. But we will look at it in this very simple case, and maybe I might mention later on about how to generalize it. But the question for us right now is just look at the simple case. So we have M items, I mean, m transactions over n items. And we want to find these two sets of items, x and Y, such that if X is a transaction, X belongs to a transaction, Y is also, like it, getting an echo from somewhere. Hello? Okay, so not every such set implication is relevant to us. So what we said is that we want to, first of all, make sure that this happens often enough. So as a fraction of times when we see X, how many times do we see Y? So that we said was one threshold and the other one was how significant? This pattern is over. So these are these two thresholds. So we have this confidence level, which is the ratio of times when we see the desired conclusion compared to the premise. So the premise is that X is there. The conclusion is X and Y are both there. So notice that this is X union Y, right? X and Y. Normally we think of as both X and Y are there. But X and Y are two separate sets. So we want both sets to be present. All of X should be there and all of Y should be there. So that's why it's X union Y. So we want everything in X and Y to be there as a fraction of only X being there. Of course, if everything in X and Y is there, then Y, X is definitely going to be there. So the top is going to be a subset of the bottom, the count. So remember, the count is just a number of transactions in which a subset appears. So the number of transactions where you see both x and Y will always be less than those where you see x alone. Because if you see x and Y, of course, x is present. So this will be some ratio which is between zero and one, and we want it to be above this confidence level, chi. And finally, we want to say the number of times we actually see the conclusion. Both x and Y should be a significant fraction of the total transactions, and this is a support level signal. So the problem now we have is actually in some sense an algorithmic problem. It's not technically a learning problem, because this is a fixed problem once we fix these quantities. So given the set of items and the transactions, and given these two thresholds, find every x y which satisfies these two constraints. This is what we are trying to do. So we will see what it means for learning at the end. Sir, I had a question. So therefore, what we said is that a rule is interesting only if this ratio is bigger than sigma. Right? The fraction of transactions where we see the target of the rule must be at least sigma. If I take m to the other side, this is same as saying there's a number of transactions with x and Y. So this is just the number of transactions. X and y exceeds sigma times the total. It's some whatever, 1% of the total, 10% of the total, and so on. So what we said last time is that this would be our first criteria. So we want to find x union Y, which is plausible. Once we have x union Y, which is plausible, we will try to break it up as x and y. So we are in some sense looking for every subset which satisfies this constraint. And what we were saying was that the simple way to do that, a naive way to do that, is to just maintain a counter for every subset and run this loop. So for every transaction, you enumerate every subset that appears in the transaction and increment the counter for that subset. So in this way, if you're maintaining it as, say, a python dictionary or something, then you will be only maintaining a counter for every subset which actually appears in your transactions. But even then, the problem is that there are, compared to the number of items, the number of possible subsets is very large. So you cannot really be sure that you are going to be able to keep it under control. So the question is, how can we do it better? So before we see how can we do it better, perhaps the first question is. So the question is, how can we do it better? But before we say how can we do it better, we should first have some realistic expectation that we can do it better, or it should be possible to do it better. So we can see that just by looking at a numerical example. 

So suppose, first of all, we discussed this last time also, somebody asked this question, which is how big is a transaction? So remember that in this market basket thing, a transaction is like a set of items that somebody proceeds towards the checkout counter, to the billing counter. So it's not going to be very large, certainly much smaller than the total number of items, and certainly also not very large in general. Also, from the point of view of using the fact that x implies y, right? So if I say that, if I say that x implies y, supposing this has some 20 items and this has some seven items, right now, this is unlikely to be something that we are able to take any action with. I mean, how can you say that somebody who bought 20 items together also bought these remaining seven items? What will you do with this information? So usually we are looking at it when x and y are small from a practical point of view. So even from a market basket, retail example, if people want to use it to sell items, they will say that, okay, toothbrush and toothpaste means they're also going to buy some soap or something, but you're not going to put together large set. So it makes sense to think of small sets. So let us just put this, this is just an example. Anyway, let's assume that there is an upper bound of ten items. The other bounds, I'm assuming, is that we have ten to the nine, that is 1 billion, that is 1000 million transactions, and we have 1 million items, ten to the power six, one with 60. So this is 1 million, this is 1 billion. So we have a large number of transactions. And now we need to remember we are only dealing with the frequent item set problem. So the confidence level is not yet an important factor for us. We are just trying to find this thing. All the z such that z dot count is bigger than sigma times m. So the only factor that is important to us right now is the sigma. So sigma, let's assume, is very small. Sigma is just 1%. We just want to know which subsets appear in 1% of the transactions. So as we said before, if you actually naively count the possible subsets that you have to keep track of, then it will be for every size up to ten, right? Because we have at most ten items. For every size up to ten, we have to choose that many items, I items from the 1 million. Each of those is a different subset. So we will have that many possible subsets. If you are counting every possible subset, on the other hand, just look at a particular subset. So fix an item. So fix x belonging y and ask this question. So because our threshold is 0.1. It must appear in 100 of the transactions. So since there are ten to the nine transactions, this particular x must appear in ten to the seven transaction. This is just equal to 0.1 times ten to the power. So this is a straightforward statement. But now notice that we have ten to the nine transactions and ten items per transaction at most. So this tells us that the total amount of total volume of items which are mentioned in our entire transaction history is at most ten to the nine times ten. Each transaction possibly has ten different items, in which case across the ten to nine transactions I am mentioning ten to the ten items. Of course, in general some items are going to be duplicated, but this is a complete upper bound. So ten to the ten is an upper bound on all the items that can appear in my transaction database. It's not saying distinct items, it's saying the number of items, which number of slots. So if you just think about your thing as a matrix, right? So this is a transaction and these are the items. So there are only ten to the ten slots grid. And now what we have done is we have filled up somewhere in this x appears here. X appears here. X appears here because x is frequent. How many slots does it appear in? It appears in ten to the seven slots, right? So now if there's another y which is frequent, that will also appear in ten to the seven slots. So every item that is frequent individually, forget about sets. Any single item which is individually frequent must appear in ten to the power seven slots. So if I keep dividing this into ten to the power seven slots per item, then you can only have 1000 items which appear at all. So if every item that appears in this transaction history is actually frequent, then there are at most thousand items. In general it will be even smaller because some of the items will not quite make it to pretend to the seven, in which case I can ignore. So this is just an illustration as to what happens in reality, which is that because of the numbers there is a practical bound on what will actually be frequent, even though theoretically the number of frequent items is very large. So we had 1 million possible frequent items. Actually 1000 of them can only be frequent. In this situation, of course you can tweak the numbers and get different answers. But for this particular choice of numbers, this is what comes. So the important thing is that you bring down this even at an individual level, you bring down 2000, which is a huge improvement. So is the calculation in this problem clear? So this is saying that with large number of transactions, large number of items and a relatively small threshold, I still cannot have too many frequent items. 

So this is a motivation for thinking that there should be a better way to calculate these frequent subsets, rather than just maintaining a counter for every possible subset that could be frequent, because the number of subsets that can be frequent are actually relatively few in numbers. So the question is, how can you identify these thousand things or these small number of things easily? So we already said that if x and y are both present, then x must be present, right? So we talked about this x union, y dot count divided by x dot count. And the reason that this is less than equal to one is because if every time I see the top, I must see the bottom. So this is a general property that if I see a set, then I see everything that is inside that set. So any subset of that set which is frequent must also be frequent. This is a very simple observation, because y must appear every time z appears, and z appears often in. So the interesting thing that we can do is turn this question around so we can take what is called a contrapositive. So we can say that if this is not there, then this is not there. Sir, can you just explain the previous slide again? Previous slide. This one. The calculation. No, the a priority. Yeah, so all I said is that if z appears, say if z appears some k times, right? Then I'm just saying that any y which is subset of z appears more than equal to k times. Because every time z appears, y is big part of z. So y also appears. Y will also appear sometimes when not all of z appears. So the frequency of a set y which is a subset of z must be more than the frequency of z. Okay, I got it. And what we are interested in is something of this point, right? Z dot count is bigger than equal to something. So if this is already bigger than that, then y dot count will also be already bigger than that because it's bigger than z. So we'll have. So y will also be a frequency. Yeah. So what we are saying is that if some subset of z is not frequent, then z cannot be frequent. So this is another way of saying it is that. So I've just soaked the fire axis. Poor terminology. But you can fix it if you want, by changing this. So you can say that if y is not frequent, then no superset z of y can be. So if. If something is not frequent, then it cannot form part of a frequent set because it will be a subset of that set. And if that set appears often enough, this thing must also appear often enough. So you can only build up frequent sets from frequent parts. That's what it's saying. You cannot build up a frequent set from components which are not themselves frequent. And this principle is called a priority. A priority means latin phrase. You may have come across it. So a priority. You assume something. So in the beginning. So basically it's talking about something that if in the beginning something is not true, then something cannot be true. So this is called a priority. So the principle is very simple. It just says you cannot build a frequent set from infrequent components. So that gives you this. From the previous example, it says that we only have 1000 frequent items. Every item is in some sense a trivially, it's an element of the set. It's also a one item subset of any set it belongs to. And you cannot build a set from infrequent components. So every item that belongs to a frequent set must itself be individually frequent as a one item set. Correct. But we saw that there are only 1000 frequent items. So all the frequent sets we could ever build can only use those 1000 items. It cannot use any items outside that. So we had 1 million items in that example. But every frequent set, by doing this first approximation of one item sets, cannot have more than 1000 different things. So for instance, if I look at a pair, then both x and y must be frequent. X must be from that thousand, and y must be a different element from that thousand. So when I'm counting pairs, I only need to look at pairs of the form x comma y, where x and y have both been known to be frequent. So first, I count the sets which are the elements which are frequent items individually. Then I can say, okay, now remember, our whole problem was maintain. I mean, counting is an obvious thing, right? When I go through the transactions I go through for each transaction, for each was our basic thing. The problem was, at this point, how many z's are we counting? So what we are saying is, we don't need to count any z, which is not a candidate. And a candidate will mean that it was already made up of frequent parts. So I'm only counting. For instance, if I go from here, I only have 1000 times 1000 possible. Actually 1000 times 999, because that would be different, right? So this is like, even if you don't, even if you count it for everything of this possibility, that's still thousand times thousand is 1 million. So 1 million is like 1 size array. So even if you had an array and stored an integer, it'll be like four megabytes of data, which is easily achievable in any standard program. So this is the UpRI principle. So let's formalize this. So what you do is you start with, you start building up these frequent sets level by level. So fi is the set of frequent item sets of size I. So these are all the sets which you know to be frequent, but whose cardinality is exactly I, not at most exactly I. 

So what we said initially was, you do f one. That is, you find all the frequent items by maintaining a counter for every item. So you have to assume that this much counter space you have. So you maintain a counter for every item. So in our previous example, the items were ten to the power six. But even if you have something bigger, like even if you have 100 megabytes of data, ten to the power eight items, that also will work. But there's no problem storing that much information. I mean, nowadays we have four GB RAm and all that. So it's not a big problem. So we can maintain a single, we have to count pairs. So when we count pairs, what we do is we first make a list of all the pairs which are worth counting, and we call these the candidates. So the candidates at level two are pairs of which both components belong to the first one. Otherwise I'm not interested, right? If I have x comma y, and y is either x or y is not in f one, then I know that x comma y is not going to be frequent. So there's no need to maintain. Accountant. So, sir, f one is just singleton sets, right? F one is just Singleton. Just think of singleton sets. Yeah. Items singleton sets. So it's sets of size one which are frequent. So every subset of a set that is frequent must already have been frequent. That's what we are saying. So therefore, if I have pairs, the nontrivial subsets of a pair are the two individual sets. So I want x to be frequent and y to be frequent. So maybe if you are asking that it should be maybe x and y if that is the question, right? Yes, sir, I got it. So we will maintain a counter for every x comma y of this form. And then we will scan the transactions one more time. So we scan the transaction the first time to count f one. Then we'll use c two as our set of counters. So c two depends on f one. And then we will scan this and again, maintain account. And those which cross this count will become f two. So for every set which belongs to c two, if the count crosses the threshold, we will include it in f two. So now we go to three. Right. So what does level three tell us? Level three tells us that I am looking for X-Y-Z such that every X-Y-Z all the three two item subsets are in f. So this basically restricts so naively, we have all triples, roughly all three elements, tuples, three tuples of I. Yeah. And then we filter it and form f two. Yeah. So f two is now going to be a subset of c two. So f two is going to be some subset of c two, and it's usually going to be a small subset, just like f one was a small subset. That's what we got in the previous thing, right? So 1 million came down to 1000. So now I maybe have 1001 thousand, 1 million choices again for f two. But again, by the same logic, in fact it will fall by even more, right? Because every pair that is frequent will again occupy that many slots. So how many different pairs can occupy the slots? Again, you'll find that only, I mean, even if they overlap, it will be less than 1000. So it could be that x y is frequent and y z is frequent. But still between x y and y z, they have occupied three times ten to the power seven slots. Remember that table I drew of all the transactions times the number of things are done? So whichever way you think about it, the number is going to drastically reduce compared to the candidates when you actually count. So in fact, in reality, this is going to be the largest signal. The items you can usually count individually, but then when you restrict it, the frequent items of f one times f one is actually going to be your largest normally or largest candidate set that you have to actually keep track of. Remember, for each candidate set, the bottleneck is you have to keep a counter for that candidate set. That's the reason that this is something that we need to keep track of. So now c three says do this, take all xyz such that every two item set has been certified to be in f two. So this is a nontrivial thing. I'm going to come to this in a moment. Right. So you have to construct c three before you can proceed. So it's not straightforward. I mean, you have to. The naive thing says, look at every xyz and check whether X-Y-Z and x z and y z are all in f two. If so, throw it into c three. Otherwise go to the next. So you have to enumerate. So even though I'm saying you restrict this to this, the naive way of doing it is by first enumerating every possible xyz and checking whether it should be in c three or not. That's what we have right now. But modulo that, which will, we will address that in a minute. Okay, but modular that, this is how it goes. So if we generalize this, if I come to ck, so ck is, I'm going to try to now check for all frequent sets of size k. So what does this mean? I must have every k minus one subset. So notice that this is downward. So here it's not so obvious, but here I don't have to check. So what? That up priority thing says that every nontrivial subset must be in frequent set. That means every two item subset of the c three item set must be in f two, and every one item subset of that must be in f one. But we already know that if a two item set is there in f two, then its components must have been in f one because they're all. 

So this uprithing is closed transitively that way. So if a bigger set is there, you can assume that everything smaller is there. So you only need to check the biggest ones. So when I'm looking at k, I only need to check k minus one. I don't need to check k minus two, k minus three, k minus four, and so on. Because if a k minus one set is there, I know that all its smaller sets must be there. But the point is I have to look at k element sets and check every k minus one subset. So just like here. So this is like saying, so in general, if I say, say x one, I enumerate a k element set, then I'll have to say, okay, is this set x one two? Is this set there? Then if I drop this, is that set there? And so on, right? I have to drop each element one by one and see if the corresponding k minus one element set is there in fk minus one, which I've already calculated in the previous step. And if all these, how many of them are there? There are exactly k of them. Because if I take k elements and I generate k minus one element subsets, each subset consists of dropping one element. So if I drop the first element, I get a set. If I drop the second element, I get a set. So there are k ways of dropping one element from this set to get a k minus one subset, and all those k ways of dropping one element must still give me a subset in fk max. But I have to enumerate everything. This is the problem. I have to enumerate everything of size k in order to do this and explicitly check each of the k subsets of that, propping one item at a time. But once I've done that, then I can run through my skin again and do right. So that's the situation right now. So what we want to do is figure out how to make this little better. The question really is, how do we generate this ck at size k? At two, it's easy, two, it's just we have no choice. It's f one, cross f one. But at three already we have a problem in some sense, because we have to go through all triples and then choose those which every pair is there. And at higher k it's even more because if I enumerate every set of size k, I'm doing something like. So if I have, remember that this is n, I'm doing some n, choose k. These are the number of sets of size k, and this is something which grows exponentially in k. So I don't want to do this. So do you see the problem? So this is k factorial into n minus k factorial. So this is for the other way. So this is going to become very large. So we don't want to do this. So what is our goal? Our goal is to count everything that is potentially frequent. So the idea is that Fk should be a subset of CK. This is our goal. So if we count here, we don't miss anything. So it should not be that I did not count a subset because I thought it was not useful, and as a result I did not enumerate it. So that should not happen. So, so long as I choose my ck in such a way that anything which is potentially frequent after counting is present in Ck before counting, because I'm counting only those sets that are in CK. So in our case, because of a priority, we know that anything that is frequent must have all its subsets frequent. And that's why Ck is a valid upper bound for Fk. So what I'm saying is that if I'm not very insistent on that, I could pick anything which is bigger than Ck and count it. In particular, I could count every k tuple. But that is the problem, right? We don't want to count every k tuple because that is too many. So can we generate something which is easier to calculate, but which includes everything in Ck and is not too big? So this observation is clear that any Ck prime which is bigger than CK can also be used. Our goal is to identify a suitable version of CK prime. So let's make an assumption which is not unreasonable. Let's assume that these items is not just a set, but it's like a set with some order on it. So you can think of, for instance, if it's a supermarket type of retail kind of example, you can think of these numbers as being, say, some serial number. At every checkout counter of every one of these supermarkets, they scan the barcode. So the barcode is a number. So you can associate. So this numbering has no meaning otherwise. I mean, it doesn't matter if toothpaste is smaller than soap or bigger than soap. It's just that every item has a distinct barcode. And these barcodes are in some natural numeric order. So once we have this numeric order, the problem with writing a set is you can write it anywhere, right? I can write x, comma Y. I can write y comma X. But if I say you must write it in order, smallest to biggest, there's only one way. So I will make sure that I list out every set that I ever talk about in increasing order with respect to this arbitrary order on items. 

So I'm just assuming that all the items have some kind of a serial number associated with them, which allows me to write out any set in a fixed order in some canonical way. So now I will look at two sets of size k minus one. So remember, what we are trying to do is that we want k size set such that every k minus one subset is an f k minus. So I'm looking at k minus one subsets which come from some k size set. So they belong to some k size set. So if they belong to some k size set and they are different from each other, they must differ in one element. If x is a subset of z and y is a subset of z, and this is of size k, and these are both of size k minus one, then I claim that these two must, if x and y are different, then between them, when you add them up, they have only k different elements. So they can differ at most one element, right? You drop one element from X and add one element, you'll get one, right? So if I look at the k minus one set, that's how I said you take x, you take a set of size k and you drop one element at a time and you get a k minus one set. So they all differ in only which element was dropped and which element was added back to get the other. So I'm going to say that I'm not interested in them differing in too many ways. They differ in only one element, but the element in which they differ is the last element. So if I write out X and x prime, these are two different k minus one sets. If I write out X and x prime, they differ in only one element, and that element is the last element in this specific order in which I have assumed all elements are ordered. So the serial number order of the elements or the barcode number, whatever. So the first k minus two elements in both sets are the same, and then the k minus one element, the last element, is different. So now if I take the union of these, then in order it will be up to k minus two will be the same, and then in some order. I don't know. I mean, I've written it this way, it could be the other way around also. So assuming that I k minus one is smaller than I prime k minus one, then the canonical order of the union will be this. So I will call this the merge of x and x minus. So I'm only merging sets of size k minus one which differ in the last position. Last position is a well defined thing because I'm writing out every set in this ascending order in which I've assumed I can enumerate the elements. So in some barcode order. Is this clear? Just stop me anytime if something is not. In this case, the remaining k minus two element should be same always in x and x dash, only the last element should be different and different. Yes, exactly. That's what I was saying here. So that's what the point is. This is what I was saying. If they combine to form a k element set and they are different from each other, if the union of two sets of k minus one is k, then so therefore these two sets must be the same. Only part is different. So therefore here I get the same, these two combine to remain. So that's what I'm calling the merge operation. The merge operation is the point really is that the merge is defined only if x and x prime are k minus one sets which differ exactly in the last position. They cannot differ anywhere else. So I don't want a situation where I have I one prime, I one, and then up to the same I k minus. I don't want this to differ in any position. I don't want them to differ in any other position. So this is my merge operation. So this is a merge operation where I just have one question. Yeah. So if we define that merge can only happen on sets which only differ in the last element. So if we think about the k minus one subsets that we think of Ck from the previous Fk set, then that will be the case. That there are only two k minus one subsets that will satisfy this case, right? Exactly. So that's what I'm just going to claim, right? So the claim here is that if I now look at this as my candidate, remember what I said was that I want Ck, but I'm allowing myself to take any bigger ck prime, because I know that if Fk is sitting inside Ck, then it's also sitting inside ck prime. So I'm going to say that. So I start with what I know. What I know is the frequent sets of k minus one. And then in that I do the merge of everything which is compatible, and I'll define that to be this dk. So this ck is this one. So now the question that you asked is a valid one, which is that, why do I know this? The question is, why is this the case? I must guarantee that the ck prime that I'm considering by. So what I'm doing is simple. Right now, I'm taking all the frequent sets of k minus one. So instead of going backwards, instead of first enumerating everything of size k and checking are its subsets in k minus one, I'm saying take everything in k minus one and go forwards and try to generate candidates of size k. We do this at the first level. When we say that go from, we say go from f one, cross f one to give me c two. So I want to say go from fk minus one, cross f k minus one in some sense, and give me ck prime, right? This is how I'm doing it. So rather than going backwards from k to k minus one, in which case I have to enumerate everything of size k and then filter it out, I'm saying directly only construct those things which I think are useful. So I'm starting with only, I mean, the left hand side is much smaller than the right hand side in some sense. So starting with a huge set and filtering it down and saying take the smaller set and build up combinations which are plausible. Okay, so this is what we have to claim now. We have to claim that CK is actually sitting inside Ck print. And now what you said is exactly right, which is that supposing that Ck has a candidate, I know that every k minus one subset of that Ck belongs to Fk minus one, right? So that's my original Ck, right? Ck is all k subsets whose every k minus one subset was frequent. But in particular, these are two concrete k minus one subsets. The one where I dropped the last element and one, I dropped the second last element. So even this k subset. 

When I write in canonical order, it has one fixed order, I one to ik. So if I drop ik, I get one set of size, k minus one. If I drop I k minus one, the second last element, I get a different subset, and if I merge those two, I get back this set. So if I have this set which y which is supposed to be in Ck, then it will actually be got by merging back these two subsets of y. So for every y that should be in Ck, it's going to be in Ck prime. Now I'm also going to get things in Ck prime which are not in Ck because they merge correctly. But there may be some other k minus one subset which is not frequent and which I'm not checking. So there will be some spurious entries in Ck prime, some things which should not be frequent because there is actually a known k minus one set which is not frequent. But I don't bother. I just am interested in saying that Ck prime is good enough for me. So the main advantage of this is that it's relatively speaking, given that our assumption is at every level, this fks are small. This is much faster than going to the bigger set and then filtering down, building up the bigger set from the smaller set. Even if we over approximate is going to be much faster and it's not going to be too much, because we know that the smaller sets are actually quite small. So, so the, if you want to think about how to do this algorithmically. So what we have is we have our set f k minus one. So supposing we just sort it in dictionary order, right? We think of these as k minus one tuples, and we sort it in dictionary order. Meaning that if the first position where two entries differ, if one is smaller than the other one, then that tuple is smaller. The first letter in the word which differs is smaller. So that's a dictionary. So what I will have is I will have a bunch of things where I have the same I one, two, I k minus one, and then I have a different j one. Jk, jk. So I will have a block of things at the beginning of my sorted order, where the first k minus one entries are the same and the last entry is different. Then I will get to. And so this is one block. So you understand what I'm saying. These are, think of a dictionary. These are all the words that start with a. The next will be all the words that start with b. So I'm looking at this whole thing as one fixed chunk, right? So everything which starts with I one to ik minus one. The smallest such chunk which is there in fk minus one will form the first block. Now I will have a second block, so I'll have some I one ik. And in general, maybe this position is bigger. And I will have a bunch of these which have that start. So these are all, all the green ones have the same first k minus one elements. Then the first yellow one is bigger than that in dictionary order, so it differs somewhere. Let's assume it for simplicity. It differs in the last position. So the first k minus two are correct. But the last entry is, this should be off, right? So this should be k minus two because this was already, these are entries of only size, k minus. So the first k minus three entries are the same and k minus two at the entries. Now, the merge only happens inside a block, right? I never have to take something here and something here and merge it because they differ at two positions, not one position. They differ in the second last position. I only want things to differ in the last position. So the ones that differ in the last position are actually only in the first block. So I've actually kind of divided up my large set into these blocks or bins or whatever you want to call them. And I then only have to construct pairs within each block. And within each block I construct every possible pair. I don't have to filter every possible combination within this block. So within this block, if I take any two items and merge them, any two sets and merge them, any two items and merge them, they will be this in the ck prime. So I do that block, then I do this block and so on. So this is how you would typically calculate it. And this will be significantly better than enumerating every possible thing of length k and filtering it down for every k management. So that's how this priority algorithm is actually implemented. Any questions about this? Sir, I had a question. Yeah. What about the elements? Or what about once in which like a second last element differs, but the last element is still the same? So where does that fall in this system? So the sets in which the second last element differs, but last element is the same. Okay, if we have a shopping list, all the elements are the same. Yeah, but you are thinking about it backwards. You think about it from here. You want something in this set, right? You should not think about it as starting from. So think about, the question is, are you counting everything that you need to count or not? Right? So if you start with something that you need to count of size k, then every k minus one subset of that must be an f k minus one. So it is true that there will be something possibly where the second last thing is dropped. But I don't care because there will be one where the last two items are the ones which are dropped one by one. And if those two things are merged, then I will get it back. So I will be counting it, right? So I will not miss anything. Because if you say that there is something here which necessarily requires something from this and something from that, if this is what you are saying, something which differs, say something here and something here, this is the question that you have. But then look at that set, right? So that set differs in two positions. Now if I look at the corresponding set which it generates. So maybe I should do it in a concrete. 

Sir, I think I understood like. Yeah, but let me just do an example with three elements of. So supposing you're saying that you have X-Y-Z and x y prime z, right? And I am telling you that I am not going to count this. So let me put it in alphabetical order. Does this get counted in the second block? No. So supposing this is the thing, right? According to my calculation, these will be in two different blocks because they do not differ only in the last position. But of course, if you were to merge them, you would get wxy z, which is a valid four item. But now I'm saying that if you take this and go backwards, you should have got wxy and wx. So therefore I claim that this set should have been here. Because I'm taking every. If this is frequent, if this is frequent, then these are both frequent. And therefore I should have merged this and this. So that's why I'm saying you should not go forward. You should think of the proof backwards. That is, if I'm going to generate something of size k, then is there a witness for it in my ck prime or not? And I claim it must be because if I take anything which is valid in Ck, then if I drop the last two elements, then those two witnesses will be mergeable. It cannot be that the only witnesses that can be generated are ones which differ elsewhere. So it is true that you might have something in this box and this box which merges, but they should also be therefore a partner for this box, this guy in the same box if it is really frequent. So that's what I mean. So just go back and digest this. So this is just a proof that you're not missing out anything as long as you generate everything that should be in Ck. Because ck is what you're counting what you want to count. As long as you generate everything that should be in Ck, you're fine. And that's what we are. So this gives us now a kind of refinement of that prayer algorithm. Just to make it explicit, I can think of c one, the candidates of size one as everything, all the singletons, right? And f one is just saying count everything in c, one whose count is above the threshold, sigma times n. And for every other k, you merge the subsets that you have already found. So, in the case of the first case, if I look at x and y, so this is of size k minus one. So I'll basically merge these. So one to k minus two is nothing. So it's a kind of a trivial case of that. So the merge of this is just x comma one. So if the k minus one is actually the only position, then if I merge them. So when I do this merge, what happens is that these two parts are the same, and then I add these two. So in the one element case, there is nothing from one to k minus two. The only element that is there is the k minus one. That is the last element, right? So you get x comma Y. So even that pairs is a special case of this merge, where I just take everything in f one and pair it up with everything else in f one. But in general, as I go up, then it will start making sense. If I have three elements, then we are saying you take every set of the form, say for instance, you look at x comma Y, next comma Z, and you merge this to xyz. But I will not look at x y and y z. So this I will not look at, though they form an xyz, they don't differ in the last. So if xyz is indeed frequent, all three of these sets will be there in my f two. But I will only look at the ones which share the first element. So I will look at Xyxz and merge them to get xyz. I will not look at X-Y-Z-Y-Z will be compared to y something else, but not to x y. So I keep doing so. That's basically my priority. So this is fine. So we do this for one, two, three. But what about this question is, how long do we, when do we stop? For our example, it is ten. So, one obvious thing is you cannot have a frequent set which is bigger than the largest transaction, because if not more than so many items. So we said ten uniformly. But if you give me any concrete shopping data set, every transaction is going to be a finite transaction. So maybe one customer bought 100, but still, therefore no set is going to be bigger than 100. That's the largest transaction. So there's always going to be a largest transaction. So that's one possibility. So once you exceed the size of the largest transaction, you don't have any possibility of generating, because there are no, that size set is not there. The other possibility is that at some point, and this is intuitively going to happen, right, that as the number of items becomes larger, the possibility that they were all bought together is going to shrink. So in general, the number of items, number of sets, number of frequent sets is going to drop as the size increases. Because why would somebody be buying eleven items together and twelve items together very frequently? Not that they don't buy it in isolation, but frequently it's very unlikely. And the moment you hit a situation where there are no frequent sets of a particular size, then obviously there cannot be any frequent sets of a bigger size. So this is more likely to be the stopping point than the other. And another thing, as I said, is that you can also stop. 

So typically, if you are looking at this from a perspective of offering a sale on items or something like that, you're not going to be, I mean, you look at anything that you see in the newspaper, like big Bazaar or something, it'll say buy five kilos of Ata and you'll get one liter of oil free. Or it says buy two kilos of sugar and you'll get some, one kilo of salt free. So typically these type of cross selling or whatever you do will not be on some humongous thing. So there might be a basket, buy Ata and salt, and then you'll get something else. So you might be only interested in sets, usually of size small, two, three, four. After that things may be frequent, but you really don't care because you can't take any action. So there might be a practical consideration in addition to these theoretical. Although in principle this algorithm does not have a natural termination criterion, there are exact criteria based on the size of the sets. But also there might be a situation where you will just stop so you won't run this. So this is going to take time in addition to the time to compute at each set, each time, that pairwise thing that I said, that merge operation, I have to do one scan of the transactions, but I'm doing a kind of number of scans I do. The transaction is smaller. It's not like size of the transaction squared or something. It is some fixed number like five times the number of transactions. So these things are acceptable, right? So you can do some, because number of transactions is large. So you don't want to do something which requires you to do size of t times size of t. This is going to kill you. But if it is some c times size of t for small c, this is okay. So that's what we are promising now, that you will make some number of scans of your because every time you go from k to k plus one, you have to scan the transactions one more time. But you'll do that only for small numbers of k, so you won't do it too often. Okay, so what we have achieved right now is part of our problem, right? We have found all the frequent item sets, but our goal was to find these actual association. So we have one more step to finish this market basket analysis, which is to. So we know that any association rule that we construct must add up to a frequent side. The left hand side plus the right hand side must be frequent. Otherwise it was not worth considering. But now how do we find the left hand side and the right hand side? So what we have done is this part. So we have ensured that we have found all the z which satisfy this. And now we have to actually do this part, decompose into x and y. So what we have done in our priorities, to find z such that z dot count is bigger than sigma times n. But now z has to be decomposed into a left hand side and a right hand side. So what we can do, of course, now if brute force thing would be okay, pick every item set that you have found, break it up. Remember, x and y must be disjoint. So you break it up into x and y such that x and y are disjoint. And check whether that x union y dot count divided by x dot count exceeds this threshold. This support level, as it's called confidence level. So it has enough reason to be a valid group. So this again is not very appealing, because anytime you have to enumerate every possible subset of a set, that's an exponential calculation within that set. So even though these z's are going to be typically small, still it's a lot of work, right? If we take a ten element set, then I have to enumerate all possible ways of breaking it up into smaller pieces. So maybe I want to do something better. So first of all, this is not very clear, but because the x intersection y is empty, first of all, they cannot have any overlap. And the claim is it's enough to take z and just divide it into two parts. So supposing the rule is such that this is not all of z. So supposing there is a part which is left out. So I have a rule of the form x implies y and x and y together sit inside z and z is frequent. So in this case, x is not x and y do not partition z because there's this red part which is. So partition means that they are disjoint subsets which add up to z. But I claim that if this is indeed the case, then that picture which I had. Right? So now in this picture which I had, if I look at this, this part, this is some z prime. So x plus y, x union y is a smaller set, z prime, but z prime is sitting inside z. And because of the priority rule, if z was frequent, every subset of z will be frequent. So z prime will also be frequent. So in other words, if I'm doing this for every frequent z, I will also do it for Z prime. So wherever I actually have the rule x implies Y for the set which is x union y, I would have been doing. What we are saying is that if I pick up every candidate that is possibly a rule, it adds up to a set z. It's enough to partition that z as left hand side and right hand side. I don't need to look at sets which are smaller than a partition. That's the first observation. 

Okay, so now supposing. So this is the notation. So this is called disjoint union. So X and Y are disjoint and their union is z. Right? And suppose that there is a rule of the form X implies Y that is valid. That is, it meets that threshold. So now what happens if I take something from the right hand side and move it to that? So I'm taking a Y from there and moving it here. So now I've got a different rule. In some sense, I've taken a rule and I've modified it. So remember that. So if you want to take me to the pictures. So this is my situation. So I have X and Y. And now what I'm doing is I'm taking a Y here and I'm adding it to z. So I'm making this x union Y. I make this Y minus. So I'm partitioning it differently by taking something from the right hand side and moving it to the left hand side. And my question is, is this also a rule or this is not a rule by rule, meaning, of course it's a rule. But is it a valid rule? Does it meet the required count? So we know that the previous rule is valid. That means that this x union Y dot count divided by X dot count has become. So this is what it means to say that this is a valid. For the second one, the left hand side is x union y, but the numerator is x union y. It's the left hand side plus the right hand side. Right? But whichever way you do this, this whole thing adds up to the full set, right? So this is just x. I've just transferred the small y from the right hand side to the left hand side, but it adds up to the same. So the numerator is the same. So I know that x union Y dot count divided by x dot count meets the threshold. My question is, does xunion Y count divided by x plus the element y? The larger set dot count meet the threshold? Remember, I'm not telling you anything more about that small y, right? It's some arbitrary small y that I moved from one side of the thing to the other side. So, will this meet the threshold? Yeah, it is very true. So y. Yeah. So x union y, the count of X union Y should be greater than the count of X, right? Should be greater than the count of X, should be lesser than, should be lesser than the count of X. And why is that? Very prime. The set increases. So every time I see x union y, I will see x, but sometimes I will see x without seeing x union Y. Right? So therefore, this is the same thing as earlier we said numerator, denominator. That ratio is going to be less than y. So the ratio of the bigger set to the smaller set is going to be less than one. Every time I see x union, small x union, the element y, I will certainly see x. So that count is going to be at least as big and it could be more. So x dot count is going to be bigger than or equal to x union y dot count always. So therefore, this numerator, I mean this denominator is bigger than this denominator. So the denominator is decreasing. So as a fraction, the value is increasing. If I take one by four and make it one by three, the fraction value increases. So it's going to exceed the first one exceeds the threshold. Second one is also going to exceed the threshold. So this is that if I have a valid rule, if this is valid, and if I take something from here and move it there, then the resulting rule is also valid. Okay? So now this tells us that if you look at it the other way, that if something is not valid, then I cannot take something from the left hand side to the right hand side and make it valid. So if I have this is not valid, then if I remove this is also not valid. Because if the second half after moving x from left to right, if it were valid, then moving x from right to left will also keep valid. Moving from right to left does not disturb the validity of a rule. So therefore, if something cannot be is not valid, then I cannot take anything from the left hand side to the right hand side and make it valid. Okay, so the way you use this is by saying that, remember, the problem is that we need to enumerate, even though we now know it's partitions, we still need to enumerate a lot of different partitions of a set, different ways of splitting a set into two and checking them. So what we want is a systematic way of doing that. So just to look at this appropriate again, so we start with singletons. So basically what this is saying is that the appropriate thing earlier was saying that bigger sets in general must be made up of smaller sets which are frequent. Here what we are saying is right hand sides of rules which are bigger must have been justified by right hand sides of rules which are smaller. If I have a right hand side of a rule which is bigger, then something was added to it. And where was it added from? From the left hand side, because the whole thing together adds up the side. It's always a partition. So if I add something to the right, the only way I can add it to the right is to remove it from the left. So I'm using that left to right shift. But if the original rule was not good, this rule cannot be good. So if I want to build up a bigger rule, bigger in the sense that the right hand side has more elements, I must necessarily have had the rule with fewer elements already in my set. So if I want this rule, for instance, if I want x implies x comma y, then I either got this by moving the x small x to the right or the small y to the right. If I use the small x to the right, then before this I had this rule. And if I move the small y to the right, then before this I had this rule. So both these rules I would have already checked because they have a smaller right hand side. So both of them must be there in order for this rule. So this is very similar. You can see, right? I'm saying that two element subset on the right has to have corresponding one element subsets on the right. Once I fix what's on the right, the left is fixed, because the left is just z minus what's on the right. So you can think about this. 

I'm not going to formalize this anymore, but it's very similar in idea to the prior year. So you can formalize this algorithm for yourself, but it's essentially saying the same thing, except here the thing is constraint, because we know that z is fixed. So once z is fixed, then we just have to start with each individual element in z as a possible right hand side, and then work forwards to that. We don't have to look at all. So for each z, we do this calculation within that set. So that's how you generate association rules once you have a frequent set. So within that set, you play this a priority game with respect to the size of the right hand side, and then you generate all the rules. Is this clear? Yes. So it's exactly the same as the previous one, just a slightly different presentation of the idea. Okay, so now, as I said, this whole thing so far has been largely a kind of algorithmic problem, right? In some sense, if I fix the set of items and the set of transactions and the set of the two thresholds, the support and the confidence threshold, then for any x, comma y, either it is true or it is not true. So there is a fixed answer. So the answer is known. So this is something where I can actually validate that you have either enumerated all the association rules or you have missed out something. So technically there is no learning involved. It's more an algorithmic question. So how does one connect this to learning? Well, in specific context, you can interpret these rules as classification rules. So, remember we said supervised learning says, if this is there, then it is a fraud. If this thing is not there, then it is not a fraud, and so on. So, let's look at a typical example. So, supposing we have some question which is about labeling the topic of a document. So our items now are words, and our baskets are collections of words, sets of words. So for each document, we have one word which describes its topic. So here is an example. On the right. So the topic words are education and sports. And the other words that appear on the document are like students, school, city spectator, and so on. So some words have been discarded. Maybe there are common words discarded. But with respect to some set of words which we are interested in tracking, what we see is that a word document which has the word student, teach and school in the first line says that this document. There is a document which has these words and it has education. Now, another thing which has teach and school, but it has two different words. City and game is also education. But here, for instance, we have city and game, but it doesn't have teach and school. It has football and team, and it's about sports. So this is now our transaction database, except that there is a kind of separate category within each transaction. So, each transaction has words of the generic words, the words that make up the document, and a special word which is the topic of the document. So the documents consists of regular words and topic words. And each transaction consists of a set of regular words and one topic word. And now we are looking for these kind of association rules where the right hand side is one item and the left hand side is a set of items. But the right hand side is always an item, a singleton item which consists of a topic word. So what this rule is saying is, if the document contains student and school, then it also contains the word education, namely its topic is education. And if it has game and team, then it is. So this is a special kind of association rule, where the right hand side of the rule is always a singleton, and it corresponds to a special type of item which is a category, in this case a topic. So when you have a table like this, the rows may have different number of entries. Or if you want to think about it as a table, then some columns may be blank. So not all columns are filled, but there's one last column which is filled. And now you want to now construct these rules corresponding to the first k minus one columns, implying the last column. And this now becomes something where these rules that you get out of this give you a kind of classifier. So this is a kind of a simplistic way of doing supervised learning. So that kind of brings me to the end of this basic treatment of classes, this market basket analysis. So what you're really looking for is correlations between sets of items across transactions, which we formalize in terms of these association groups. X implies y. And what we saw is that as such, combinatorially, this is a very complicated thing to calculate, given the sizes of the sets involved. But we can use this appropriate principle both to identify the sets which are interesting, those which are frequent enough to be considered, and also to decompose them into rules. So you use the approoriate twice, once to find the frequent z, and second to decompose each frequent z into possible x implies y. And using this, we can build our first simple supervised learning model. So these rules now become a very simple way of prediction. Now, if you give me a document, I will look at what words are there in the document in this previous example. And if it contains the word student and school, I will predict education. If it contains game and team, I'll predict sports. 

Now, of course, if it has all four of them, all four of these words, then I have to decide. So either I have to say that it is one of two or I have to give some priority or something. So it's not a very refined model in that sense, but it is a model. So you can imagine how you can predict. Now you get a new document, you check the words that are there and you see which rules apply. So I am not going to get into these refinements. But you can think of, you can. So last time somebody asked about multiplicities, another related refinement, which is not quite, but if it is not a set, but a sequence. So supposing I say that somebody who buys a computer and buys a router is likely to later subscribe to an Internet plan, it makes sense. Why would you buy a computer and a router if you're not going to do that? But there is sometimes a sequence involved, right? So you're not going to buy it all in the same transaction. So you're going to do something and then something and then something else. So that's a sequential thing. So that's a different type of prediction, which is also interesting, which also sometimes can be addressed in this market basket thing. And then the other last thing is that not all items must, may be of equal importance in terms or equal value if you think about it in terms of supermarket. So if you set your thresholds too low, if you are trying to catch low frequency things, say for example, things involving expensive items like say pressure cookers or something like that, then you will trigger a lot of spurious rules involving day to day items. People who buy milk also buy sugar. People buy milk also by butter. People buy bread, also by eggs, which are not epidemically useful. On the other hand, if you set your threshold to catch only these very high frequency items, then all the low frequency things which may have high value to you, you may want to really make offers on expensive items. You will not see them. So sometimes you might want to work with this market basket analysis where you assign different thresholds to different items. So if it is pressure cooker, you want to say something. If it's something else, you want to say something else. So you have not one sigma, but you have a different sigma for each item. But then the problem is what happens with mixed baskets. If I have a basket in which I have a high value item and a low value item, and they have two different thresholds, if I buy butter along with the pressure cooker, then how should I measure the frequency of this particular basket? Should I measure it with the butter frequency or with the pressure cooker frequency? These are discussed in that book where I have given you the reference for the first lecture and for this lecture, so you can read about it. It's not particularly relevant to the work that we want to focus on in this course, so I won't get into it in any detail, but there is a very accessible introduction to that, in that chapter later on. So you can look at it and see a lot of it is some kind of heuristics, but you can look at it. So I'll stop with this. Any questions? Is there any way to predict how better our association rule is? So that is a general problem that we will come across. There is no real good way to predict how good it is. You can, of course, associate with a concrete data set how reliable it is for that data set. But this is going to be one of the challenges, as we will see going along, which is that you are always building rules or any model with respect to a fixed data set, and you are going to apply it to some other data set which you have not seen. So will that generalize well or not is the biggest question in machine learning. How do you know that the training data that you have used to build your model is actually representative of the data that you see in the real. And there could be many reasons why it is not valid. It could be because of sampling. So, for instance, you might be building a model for some consumer behavior by looking at one segment of. Maybe you are looking at the behavior of people who are in their, are building a model of how people behave, but your actual market consists of people who are maybe in their 40s, who have a very different way of dealing with things. So you need to be careful that the data that you're looking at is actually representative. So this is a huge problem and there's no correct answer. You can measure it, but you cannot. In some sense, if you could validate it, then this whole problem becomes much simpler. But the whole difficulty with machine learning is exactly this, that there is no sensible way to predict the behavior or even when it will fail. Even if you could say that this is likely to work well here and not likely to work well there that itself would be a huge improvement, not even necessarily a very precise quantification, but just some indication. But all these are very speculative. So that's a huge challenge. So there are some theoretical things you can say which we may not, may or may not get to in this course. But it's a big problem. This is the so called generalization problem. So that's what basically, if you build association rules, is it going to hold or not? I don't know. 

Okay, sir, I had a question. We have been talking about items and transactions, but we haven't anywhere spoken about whether one customer, suppose if you're talking about somebody buying groceries, whether the behavior of one customer is what we also trap. So for example, what I mean is like you gave the example of a person buying a computer and a modem, and then they are most likely to buy Internet. So here we need to trap the behavior of the customer. So when I said sequences, it implies that there are connected transactions. Right. As you said, involving one customer. So we have not done it. I agree. So what we have here looked at is aggregate behavior. And we have ignored whether all the customers are different or whether some customer is coming repeatedly. So we are really looking at it anonymously in that sense. So we are not connecting any one transaction to any other transaction. Every transaction is independent. Yeah. So every transaction essentially could be a different person. So we are really looking at some kind of behavior of a population as a whole without tracking subgroups or sub, we don't have any identification of transactions belonging to other categories in terms of who it is. So that's right. So if you wanted to do that, then you have to do something else. So that's what I meant. So when I meant sequences in this particular thing, one of the things it includes is a fact that there is some kind of a linkage between the two. Right. There are transactions which are kind of connected and we want to track in some sense the sequence of those transactions, or there are a bunch of transactions which have come together, and then you want to look at each of these groups as a whole and do some kind of. So not just one consumer, but across consumers, you want to say, as a rule, consumers who do this also do that and so on. So these are all refinements. And there are many situations where these are important and interesting, but this model as such, this kind of market basket analysis approach is very naive. And so that's why these things will come up in different contexts. But we are not going to explore it anymore in this context, because in this context, the kind of model that you build, this association rule model is rather simplistic. So we are going to look at more sophisticated models as we go along. And there you can ask the same question, but it'll take a slightly different format. Does this also include tendencies like, if there is one person who tends to buy all his groceries once a month, then they will have a bigger basket size. But for people who buy their groceries, say like per week or every other day, they will have smaller basket size, but their transactions would be spread out. So that will be treated as a different transaction every time. But for somebody who's buying everything together, doesn't that give rise to a lot of. Yeah, so there are all these situations, I agree, which are not directly addressed in this. There are many different variations of this that you could ask. So there's no doubt about that. So some of them people have looked at because they have kind of natural solutions in this. Some of them maybe you cannot do. So that's another thing about this whole model building thing is that the same model may or may not be capable of tackling every different question that you ask. So this model, some of these things maybe you can segregate and answer. But if, as you're saying, you want to compare behaviors across customers of different types, you cannot, as you said, sensibly aggregate them into a single market basket model because they all have different profiles. So you will have to, in the example that you gave, you would have to first separate out the data for these, say, the people who buy one month groceries at a time from those who buy at more frequent intervals and then analyze them separately, because together the data doesn't actually make sense. So last time, if you remember, I talked about this unsupervised. So maybe one thing that you need to do when you have something large is to basically look at the transactions and categorize them according to some criteria, maybe by the size of the basket. And then you will find that there are maybe a lot of people who buy five items at a time. There are people who buy ten to 20 items at a time. There are people who buy 40 items at a time. And maybe you need to then apply the model separately to each of these clusters. So all of these things are not a one size fits all solution. And also, it's not that one approach on its own is going to solve the problem at any given time. So you need to have a kind of think of it as a toolbox, and then you need to apply some combination of tools for a given problem based on what you can do about analyzing the problem. So what we are looking at is the question of what these tools are. But they will not work out of the box in some sense, in every situation, like you said. So that's a valid point. Okay, so, thank you. Okay. I've kept you long enough, so see you on Monday.