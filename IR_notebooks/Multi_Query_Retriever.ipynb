{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e0f26342dc4949bc85de0e60c4032951": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_beec602181434bf59551e97b9d6f14cd",
              "IPY_MODEL_e92a406043fb42ceba54e4836bd8e88c",
              "IPY_MODEL_be511e5218fb4f1aa700a562b41722a8"
            ],
            "layout": "IPY_MODEL_8cf8f71b785940f59fdbac906c07d715"
          }
        },
        "beec602181434bf59551e97b9d6f14cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5261c9a247ab4c57b235a4f060ae559e",
            "placeholder": "​",
            "style": "IPY_MODEL_7354a16ead33404f9cbc8f5a09de5fcd",
            "value": "modules.json: 100%"
          }
        },
        "e92a406043fb42ceba54e4836bd8e88c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40930b0ef72a446689866666e9bf2ee9",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2a85da94208f45509255612cc2f8918e",
            "value": 349
          }
        },
        "be511e5218fb4f1aa700a562b41722a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6aa87e2cfe96497ba2fd6bf94eab26c2",
            "placeholder": "​",
            "style": "IPY_MODEL_5251c618552f46ebac2d6699fe54160d",
            "value": " 349/349 [00:00&lt;00:00, 14.0kB/s]"
          }
        },
        "8cf8f71b785940f59fdbac906c07d715": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5261c9a247ab4c57b235a4f060ae559e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7354a16ead33404f9cbc8f5a09de5fcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40930b0ef72a446689866666e9bf2ee9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a85da94208f45509255612cc2f8918e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6aa87e2cfe96497ba2fd6bf94eab26c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5251c618552f46ebac2d6699fe54160d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a9b152a5951412eae312595a99e617b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9c8607d5d2d64516b55006e7fe33d33c",
              "IPY_MODEL_7d6c63f18e7e4d729674f2d29b8ce082",
              "IPY_MODEL_345a874bff784e0695c4a140a8651a1f"
            ],
            "layout": "IPY_MODEL_eb341f07e05942c0a5a56976c99a34a9"
          }
        },
        "9c8607d5d2d64516b55006e7fe33d33c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1736b0dbdf6471bbd7b1a141a1612c1",
            "placeholder": "​",
            "style": "IPY_MODEL_b5e183395c7e4ca4882c1920f3bfafbe",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "7d6c63f18e7e4d729674f2d29b8ce082": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf8ded8ed8c045378946c0fe7b7f5132",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_112598c4b65d4187b4ad27aacd84c08b",
            "value": 124
          }
        },
        "345a874bff784e0695c4a140a8651a1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7da80dae677c4a01b5279921345a60a9",
            "placeholder": "​",
            "style": "IPY_MODEL_05a485c1f8124feda0c3216f62c16ac6",
            "value": " 124/124 [00:00&lt;00:00, 6.20kB/s]"
          }
        },
        "eb341f07e05942c0a5a56976c99a34a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1736b0dbdf6471bbd7b1a141a1612c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5e183395c7e4ca4882c1920f3bfafbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf8ded8ed8c045378946c0fe7b7f5132": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "112598c4b65d4187b4ad27aacd84c08b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7da80dae677c4a01b5279921345a60a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05a485c1f8124feda0c3216f62c16ac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "085f5c2f98134aecb261c32928038d0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_614e0ca7f8104969a98abdada2dba028",
              "IPY_MODEL_f34f0899424b4815ba18d6bb9d7bfb6c",
              "IPY_MODEL_0bc7abd85c364288a0f777e481cccb3f"
            ],
            "layout": "IPY_MODEL_38847a9b1b154e7194152daf81d27703"
          }
        },
        "614e0ca7f8104969a98abdada2dba028": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_feb02fa9a7e84bc297b231efe398e8c3",
            "placeholder": "​",
            "style": "IPY_MODEL_879a9b61955f4417a9e2473eec3fae9c",
            "value": "README.md: 100%"
          }
        },
        "f34f0899424b4815ba18d6bb9d7bfb6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac092fd887e144d9b2af01baf6b99f40",
            "max": 90272,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4e9847b838354001b595a06a9246e828",
            "value": 90272
          }
        },
        "0bc7abd85c364288a0f777e481cccb3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3fe93fa00904db2b97c521073f33b9b",
            "placeholder": "​",
            "style": "IPY_MODEL_edd6470122234ecd9b6fd405256d976d",
            "value": " 90.3k/90.3k [00:00&lt;00:00, 3.81MB/s]"
          }
        },
        "38847a9b1b154e7194152daf81d27703": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "feb02fa9a7e84bc297b231efe398e8c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "879a9b61955f4417a9e2473eec3fae9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac092fd887e144d9b2af01baf6b99f40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e9847b838354001b595a06a9246e828": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a3fe93fa00904db2b97c521073f33b9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edd6470122234ecd9b6fd405256d976d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "803d1531e8ae47e4a00ae1a82924d8b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bbc7bb27f6b74403878783f333fc9d87",
              "IPY_MODEL_c055d494b97641d28d7138ae9a8fe7e9",
              "IPY_MODEL_920c06911c7f4d8cae0794845304ab4f"
            ],
            "layout": "IPY_MODEL_2baf9203c537475283da6c5b6059ae20"
          }
        },
        "bbc7bb27f6b74403878783f333fc9d87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05f59ace32ea4ec09530c3071897334a",
            "placeholder": "​",
            "style": "IPY_MODEL_83cf6374b53047109bb0e4f1a3a69dbb",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "c055d494b97641d28d7138ae9a8fe7e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cbb193f1cb849eb969350728ad77837",
            "max": 52,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_963c04e552fd4ac18dc3ccae361c3de3",
            "value": 52
          }
        },
        "920c06911c7f4d8cae0794845304ab4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4880ea5ebf0f4aeda748f2d9f2939f5b",
            "placeholder": "​",
            "style": "IPY_MODEL_48866d4187dd4e20bbd3434f5d805215",
            "value": " 52.0/52.0 [00:00&lt;00:00, 4.32kB/s]"
          }
        },
        "2baf9203c537475283da6c5b6059ae20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05f59ace32ea4ec09530c3071897334a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83cf6374b53047109bb0e4f1a3a69dbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1cbb193f1cb849eb969350728ad77837": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "963c04e552fd4ac18dc3ccae361c3de3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4880ea5ebf0f4aeda748f2d9f2939f5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48866d4187dd4e20bbd3434f5d805215": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c6eb13de0cc4e92b19e890d914d6467": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4800ee35c4f74857920d9feee548c535",
              "IPY_MODEL_7d6f139dc28d4e00a5f0a7c1efa78e1e",
              "IPY_MODEL_259bed448f284baca5e9ed8a84c792bc"
            ],
            "layout": "IPY_MODEL_007ea007e47e4ae6a4e9c71247902ff3"
          }
        },
        "4800ee35c4f74857920d9feee548c535": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2038064a8bd746a98311a02fd4abc7e2",
            "placeholder": "​",
            "style": "IPY_MODEL_84191086382c442fbf0d7f95d5c0cb70",
            "value": "config.json: 100%"
          }
        },
        "7d6f139dc28d4e00a5f0a7c1efa78e1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b1506a601994f0a8597f8fd35e51458",
            "max": 720,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bbc646ed6fb446fd8f10086b1d31ac2d",
            "value": 720
          }
        },
        "259bed448f284baca5e9ed8a84c792bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d7e9a2ee6c94c3592dc5507ec8c1875",
            "placeholder": "​",
            "style": "IPY_MODEL_41af9d5033dd4e93811bec9bd4f3473d",
            "value": " 720/720 [00:00&lt;00:00, 52.0kB/s]"
          }
        },
        "007ea007e47e4ae6a4e9c71247902ff3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2038064a8bd746a98311a02fd4abc7e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84191086382c442fbf0d7f95d5c0cb70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b1506a601994f0a8597f8fd35e51458": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbc646ed6fb446fd8f10086b1d31ac2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7d7e9a2ee6c94c3592dc5507ec8c1875": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41af9d5033dd4e93811bec9bd4f3473d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "983b40f0095147aa9e1b474d03d86059": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2ddf1cfb14dd4ab0bc61f302e84c8010",
              "IPY_MODEL_2c1acf290bde47a68656453e24888508",
              "IPY_MODEL_d25b53d997ba445eb8a2b96b05601cbc"
            ],
            "layout": "IPY_MODEL_095b2f7b64114512b7b1f0000b9cfd0f"
          }
        },
        "2ddf1cfb14dd4ab0bc61f302e84c8010": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcd2e533fd4b4375b0b17624ec59c2fb",
            "placeholder": "​",
            "style": "IPY_MODEL_ab1401ed222d42ea888713983df7a154",
            "value": "model.safetensors: 100%"
          }
        },
        "2c1acf290bde47a68656453e24888508": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6897ae7f06ed4a6f9724399bbbacfad0",
            "max": 1340616616,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_48d7d416f6d8432db43b7588d386bd7f",
            "value": 1340616616
          }
        },
        "d25b53d997ba445eb8a2b96b05601cbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_161f41619d10412585b4031a9f59ea42",
            "placeholder": "​",
            "style": "IPY_MODEL_a149b3303a3e4eb6b420ebaea9e8aa05",
            "value": " 1.34G/1.34G [00:12&lt;00:00, 32.4MB/s]"
          }
        },
        "095b2f7b64114512b7b1f0000b9cfd0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcd2e533fd4b4375b0b17624ec59c2fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab1401ed222d42ea888713983df7a154": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6897ae7f06ed4a6f9724399bbbacfad0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48d7d416f6d8432db43b7588d386bd7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "161f41619d10412585b4031a9f59ea42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a149b3303a3e4eb6b420ebaea9e8aa05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8a32b75054a4fbeb3ba8829a5ba7d52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e6249cb4ada4596b2843f27d54db15c",
              "IPY_MODEL_06fae87262944c1097aec28999ea6e94",
              "IPY_MODEL_8616ed225b654921a055adcf83151ce6"
            ],
            "layout": "IPY_MODEL_efd638f331264751932a394b5e2e50e5"
          }
        },
        "8e6249cb4ada4596b2843f27d54db15c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2b3a8ad8f174476b9a795bdca7a787a",
            "placeholder": "​",
            "style": "IPY_MODEL_9e877347c7774c68b89ec99398d0ccba",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "06fae87262944c1097aec28999ea6e94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d15dcdc5c0b0432b961ef3e8e7ad711e",
            "max": 366,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2ff24ec10d2d474daf36e674a7fbadc9",
            "value": 366
          }
        },
        "8616ed225b654921a055adcf83151ce6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_922208d7ca094198a0e53a06043e8577",
            "placeholder": "​",
            "style": "IPY_MODEL_9bfc3adc0db24922a9c4f7607251448e",
            "value": " 366/366 [00:00&lt;00:00, 23.4kB/s]"
          }
        },
        "efd638f331264751932a394b5e2e50e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2b3a8ad8f174476b9a795bdca7a787a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e877347c7774c68b89ec99398d0ccba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d15dcdc5c0b0432b961ef3e8e7ad711e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ff24ec10d2d474daf36e674a7fbadc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "922208d7ca094198a0e53a06043e8577": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bfc3adc0db24922a9c4f7607251448e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2f9cf89a05145f09fd1b693ac5f21c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_07a9b623a1bc403b8a24028c4af581af",
              "IPY_MODEL_60f0df423d8c4a85bc9c2a7bd95db9f8",
              "IPY_MODEL_28806177c96f452faaa590d0875d2b9c"
            ],
            "layout": "IPY_MODEL_65b171ac406743e08216ebf4015c2dde"
          }
        },
        "07a9b623a1bc403b8a24028c4af581af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed088afb6ccc4bdcb67a994608548117",
            "placeholder": "​",
            "style": "IPY_MODEL_321aec2d23bd44f79d888b165983ba62",
            "value": "vocab.txt: 100%"
          }
        },
        "60f0df423d8c4a85bc9c2a7bd95db9f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e8ef238bd4f48028c01d2bd25a66006",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d2026b7548e34221b2cf285c0cd52dd1",
            "value": 231508
          }
        },
        "28806177c96f452faaa590d0875d2b9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a4bdb0790814b3bb523e9b8368aa46a",
            "placeholder": "​",
            "style": "IPY_MODEL_abbaed65369348bab45cb6588a50087f",
            "value": " 232k/232k [00:00&lt;00:00, 13.4MB/s]"
          }
        },
        "65b171ac406743e08216ebf4015c2dde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed088afb6ccc4bdcb67a994608548117": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "321aec2d23bd44f79d888b165983ba62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e8ef238bd4f48028c01d2bd25a66006": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2026b7548e34221b2cf285c0cd52dd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0a4bdb0790814b3bb523e9b8368aa46a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abbaed65369348bab45cb6588a50087f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2fbea8c70f74ed8b23895fc5c7132a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c3abdec81924b45a9d9c2e198e4dc26",
              "IPY_MODEL_147fd572852e45ae9ec4b4f07297e029",
              "IPY_MODEL_6ffab45e03484f37b4f610034fed41f9"
            ],
            "layout": "IPY_MODEL_1bfc92b7c3d342d590e40eab19a3982b"
          }
        },
        "0c3abdec81924b45a9d9c2e198e4dc26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d06170b8888049678b620d3c6b760c1b",
            "placeholder": "​",
            "style": "IPY_MODEL_67b9fb1ae135451aa8154ac1f575bb25",
            "value": "tokenizer.json: 100%"
          }
        },
        "147fd572852e45ae9ec4b4f07297e029": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6522cea2c6f946e5af6a89e6ed757a73",
            "max": 711396,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_482d6c33af8b49dbba0c1a5ad9af9ff0",
            "value": 711396
          }
        },
        "6ffab45e03484f37b4f610034fed41f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f697c14c60e4408bb4063192535025b",
            "placeholder": "​",
            "style": "IPY_MODEL_851bae01b1184f02b00b012a56f4258f",
            "value": " 711k/711k [00:00&lt;00:00, 8.25MB/s]"
          }
        },
        "1bfc92b7c3d342d590e40eab19a3982b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d06170b8888049678b620d3c6b760c1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67b9fb1ae135451aa8154ac1f575bb25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6522cea2c6f946e5af6a89e6ed757a73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "482d6c33af8b49dbba0c1a5ad9af9ff0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f697c14c60e4408bb4063192535025b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "851bae01b1184f02b00b012a56f4258f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b63c3adf15e431b9ff76d18d5ca69a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_05a4dc46f3404fa5ae6c15ecd496083a",
              "IPY_MODEL_66ef2ce86676439185bcb8b30d2a8a05",
              "IPY_MODEL_4593d67f2bd84d8ba34e8fab5880f6c9"
            ],
            "layout": "IPY_MODEL_6a97fd6b7e2a49488fc1b0177fdd7f58"
          }
        },
        "05a4dc46f3404fa5ae6c15ecd496083a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e71cd4414c04a5a90e6a7de082144d9",
            "placeholder": "​",
            "style": "IPY_MODEL_c26d4122a6f9431c95ddd0e837b2767b",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "66ef2ce86676439185bcb8b30d2a8a05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1079dd6068964665b12ebfe3d87861aa",
            "max": 125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8ba038629d3e45388da0e207714967e7",
            "value": 125
          }
        },
        "4593d67f2bd84d8ba34e8fab5880f6c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bce05cf3d594fbbae229081c90588dd",
            "placeholder": "​",
            "style": "IPY_MODEL_1c158b1f051846b29fb2e5c30287c8cb",
            "value": " 125/125 [00:00&lt;00:00, 9.42kB/s]"
          }
        },
        "6a97fd6b7e2a49488fc1b0177fdd7f58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e71cd4414c04a5a90e6a7de082144d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c26d4122a6f9431c95ddd0e837b2767b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1079dd6068964665b12ebfe3d87861aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ba038629d3e45388da0e207714967e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6bce05cf3d594fbbae229081c90588dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c158b1f051846b29fb2e5c30287c8cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3daef41460284d38a13296e72fd3bd9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6f3bd55dd2ed44df820966182b6d12e5",
              "IPY_MODEL_6b220f0a41224853bf13d34ab8d20f3f",
              "IPY_MODEL_29932a1b63ca4114b7fe41002911115e"
            ],
            "layout": "IPY_MODEL_4a83b7dba1aa43c29062687483dad720"
          }
        },
        "6f3bd55dd2ed44df820966182b6d12e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97fb4fd947124ad89b7f23f0af9edd19",
            "placeholder": "​",
            "style": "IPY_MODEL_d1e00228c76140459d0857290e7e6b31",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "6b220f0a41224853bf13d34ab8d20f3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b785aefb6224da8b59462e3351ac71f",
            "max": 191,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2b584593148f4d0f9d1ca9c970a8e885",
            "value": 191
          }
        },
        "29932a1b63ca4114b7fe41002911115e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_241e4b39cf1e44a3a3b479cb05dff672",
            "placeholder": "​",
            "style": "IPY_MODEL_6fc1ffb11bb14eb19f52b743b21a9672",
            "value": " 191/191 [00:00&lt;00:00, 9.75kB/s]"
          }
        },
        "4a83b7dba1aa43c29062687483dad720": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97fb4fd947124ad89b7f23f0af9edd19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1e00228c76140459d0857290e7e6b31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b785aefb6224da8b59462e3351ac71f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b584593148f4d0f9d1ca9c970a8e885": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "241e4b39cf1e44a3a3b479cb05dff672": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fc1ffb11bb14eb19f52b743b21a9672": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install chromadb"
      ],
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2024-04-03T14:30:00.110126Z",
          "iopub.execute_input": "2024-04-03T14:30:00.110761Z",
          "iopub.status.idle": "2024-04-03T14:30:37.862378Z",
          "shell.execute_reply.started": "2024-04-03T14:30:00.110731Z",
          "shell.execute_reply": "2024-04-03T14:30:37.861146Z"
        },
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "id": "lb13JbjzL7YP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e303814d-6b94-4d74-df67-6add38e4bc6f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb\n",
            "  Downloading chromadb-0.4.24-py3-none-any.whl (525 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.1)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.6.4)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.110.1-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.25.2)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.10.0)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.17.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.24.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.24.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.45b0-py3-none-any.whl (11 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.24.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.2)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.2)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.0)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.62.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (698 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.4)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.2.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson>=3.9.12 (from chromadb)\n",
            "  Downloading orjson-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (24.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting importlib-metadata<=7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading importlib_metadata-7.0.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.24.0-py3-none-any.whl (17 kB)\n",
            "Collecting opentelemetry-proto==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.24.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-instrumentation-asgi==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.45b0-py3-none-any.whl (14 kB)\n",
            "Collecting opentelemetry-instrumentation==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.45b0-py3-none-any.whl (28 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl (36 kB)\n",
            "Collecting opentelemetry-util-http==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.45b0-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.7.2)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.6)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.20.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.18.1)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.38.0,>=0.37.2->fastapi>=0.95.2->chromadb) (3.7.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi>=0.95.2->chromadb) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi>=0.95.2->chromadb) (1.2.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53724 sha256=4a8bacbac7c59a84a1c0d277ad34cb3e9fd20b57c755daa0ca366b61806763b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, mmh3, websockets, uvloop, python-dotenv, pulsar-client, overrides, orjson, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, importlib-metadata, humanfriendly, httptools, h11, deprecated, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, uvicorn, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, kubernetes, fastapi, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 7.1.0\n",
            "    Uninstalling importlib_metadata-7.1.0:\n",
            "      Successfully uninstalled importlib_metadata-7.1.0\n",
            "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.1.2 chroma-hnswlib-0.7.3 chromadb-0.4.24 coloredlogs-15.0.1 deprecated-1.2.14 fastapi-0.110.1 h11-0.14.0 httptools-0.6.1 humanfriendly-10.0 importlib-metadata-7.0.0 kubernetes-29.0.0 mmh3-4.1.0 monotonic-1.6 onnxruntime-1.17.1 opentelemetry-api-1.24.0 opentelemetry-exporter-otlp-proto-common-1.24.0 opentelemetry-exporter-otlp-proto-grpc-1.24.0 opentelemetry-instrumentation-0.45b0 opentelemetry-instrumentation-asgi-0.45b0 opentelemetry-instrumentation-fastapi-0.45b0 opentelemetry-proto-1.24.0 opentelemetry-sdk-1.24.0 opentelemetry-semantic-conventions-0.45b0 opentelemetry-util-http-0.45b0 orjson-3.10.0 overrides-7.7.0 posthog-3.5.0 pulsar-client-3.4.0 pypika-0.48.9 python-dotenv-1.0.1 starlette-0.37.2 uvicorn-0.29.0 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T14:30:48.224715Z",
          "iopub.execute_input": "2024-04-03T14:30:48.225526Z",
          "iopub.status.idle": "2024-04-03T14:31:06.148482Z",
          "shell.execute_reply.started": "2024-04-03T14:30:48.225489Z",
          "shell.execute_reply": "2024-04-03T14:31:06.147330Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "id": "h-GVrKBmL7YR",
        "outputId": "9ac6ebd0-8933-4599-9379-582c002a878d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.14-py3-none-any.whl (812 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.8/812.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.30 (from langchain)\n",
            "  Downloading langchain_community-0.0.31-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.37 (from langchain)\n",
            "  Downloading langchain_core-0.1.40-py3-none-any.whl (276 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.8/276.8 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.40-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.37->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: packaging, mypy-extensions, jsonpointer, typing-inspect, marshmallow, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed dataclasses-json-0.6.4 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.14 langchain-community-0.0.31 langchain-core-0.1.40 langchain-text-splitters-0.0.1 langsmith-0.1.40 marshmallow-3.21.1 mypy-extensions-1.0.0 packaging-23.2 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence_transformers"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T14:31:17.103484Z",
          "iopub.execute_input": "2024-04-03T14:31:17.104302Z",
          "iopub.status.idle": "2024-04-03T14:31:30.716705Z",
          "shell.execute_reply.started": "2024-04-03T14:31:17.104271Z",
          "shell.execute_reply": "2024-04-03T14:31:30.715619Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "id": "3LGq_NdaL7YS",
        "outputId": "a22de64c-070b-44fa-ff6d-a69d715c86cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence_transformers\n",
            "  Downloading sentence_transformers-2.6.1-py3-none-any.whl (163 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.3/163.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.38.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.4.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence_transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 sentence_transformers-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T19:08:28.021313Z",
          "iopub.execute_input": "2024-04-01T19:08:28.021677Z",
          "iopub.status.idle": "2024-04-01T19:08:40.934403Z",
          "shell.execute_reply.started": "2024-04-01T19:08:28.021647Z",
          "shell.execute_reply": "2024-04-01T19:08:40.933237Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "id": "hgLZ8z4ZL7YT",
        "outputId": "024aa010-dc13-4df9-e83c-e04b653feec3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.16.2-py3-none-any.whl (267 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.1/267.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Installing collected packages: httpcore, httpx, openai\n",
            "Successfully installed httpcore-1.0.5 httpx-0.27.0 openai-1.16.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import chromadb\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.document_transformers import (\n",
        "    EmbeddingsRedundantFilter,\n",
        "    EmbeddingsClusteringFilter,\n",
        ")\n",
        "# from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
        "# from langchain.retrievers import ContextualCompressionRetriever\n",
        "# from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T14:31:58.360066Z",
          "iopub.execute_input": "2024-04-03T14:31:58.360533Z",
          "iopub.status.idle": "2024-04-03T14:31:59.475274Z",
          "shell.execute_reply.started": "2024-04-03T14:31:58.360497Z",
          "shell.execute_reply": "2024-04-03T14:31:59.474346Z"
        },
        "trusted": true,
        "id": "jops1cdtL7YT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding Model"
      ],
      "metadata": {
        "id": "A5CZ3DfiL7YU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"BAAI/bge-large-en\"\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "hf = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n",
        "print(\"Embedding Model Loaded..........\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T14:32:56.543699Z",
          "iopub.execute_input": "2024-04-03T14:32:56.544195Z",
          "iopub.status.idle": "2024-04-03T14:33:14.955154Z",
          "shell.execute_reply.started": "2024-04-03T14:32:56.544165Z",
          "shell.execute_reply": "2024-04-03T14:33:14.954180Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "e0f26342dc4949bc85de0e60c4032951",
            "beec602181434bf59551e97b9d6f14cd",
            "e92a406043fb42ceba54e4836bd8e88c",
            "be511e5218fb4f1aa700a562b41722a8",
            "8cf8f71b785940f59fdbac906c07d715",
            "5261c9a247ab4c57b235a4f060ae559e",
            "7354a16ead33404f9cbc8f5a09de5fcd",
            "40930b0ef72a446689866666e9bf2ee9",
            "2a85da94208f45509255612cc2f8918e",
            "6aa87e2cfe96497ba2fd6bf94eab26c2",
            "5251c618552f46ebac2d6699fe54160d",
            "0a9b152a5951412eae312595a99e617b",
            "9c8607d5d2d64516b55006e7fe33d33c",
            "7d6c63f18e7e4d729674f2d29b8ce082",
            "345a874bff784e0695c4a140a8651a1f",
            "eb341f07e05942c0a5a56976c99a34a9",
            "a1736b0dbdf6471bbd7b1a141a1612c1",
            "b5e183395c7e4ca4882c1920f3bfafbe",
            "bf8ded8ed8c045378946c0fe7b7f5132",
            "112598c4b65d4187b4ad27aacd84c08b",
            "7da80dae677c4a01b5279921345a60a9",
            "05a485c1f8124feda0c3216f62c16ac6",
            "085f5c2f98134aecb261c32928038d0d",
            "614e0ca7f8104969a98abdada2dba028",
            "f34f0899424b4815ba18d6bb9d7bfb6c",
            "0bc7abd85c364288a0f777e481cccb3f",
            "38847a9b1b154e7194152daf81d27703",
            "feb02fa9a7e84bc297b231efe398e8c3",
            "879a9b61955f4417a9e2473eec3fae9c",
            "ac092fd887e144d9b2af01baf6b99f40",
            "4e9847b838354001b595a06a9246e828",
            "a3fe93fa00904db2b97c521073f33b9b",
            "edd6470122234ecd9b6fd405256d976d",
            "803d1531e8ae47e4a00ae1a82924d8b2",
            "bbc7bb27f6b74403878783f333fc9d87",
            "c055d494b97641d28d7138ae9a8fe7e9",
            "920c06911c7f4d8cae0794845304ab4f",
            "2baf9203c537475283da6c5b6059ae20",
            "05f59ace32ea4ec09530c3071897334a",
            "83cf6374b53047109bb0e4f1a3a69dbb",
            "1cbb193f1cb849eb969350728ad77837",
            "963c04e552fd4ac18dc3ccae361c3de3",
            "4880ea5ebf0f4aeda748f2d9f2939f5b",
            "48866d4187dd4e20bbd3434f5d805215",
            "4c6eb13de0cc4e92b19e890d914d6467",
            "4800ee35c4f74857920d9feee548c535",
            "7d6f139dc28d4e00a5f0a7c1efa78e1e",
            "259bed448f284baca5e9ed8a84c792bc",
            "007ea007e47e4ae6a4e9c71247902ff3",
            "2038064a8bd746a98311a02fd4abc7e2",
            "84191086382c442fbf0d7f95d5c0cb70",
            "7b1506a601994f0a8597f8fd35e51458",
            "bbc646ed6fb446fd8f10086b1d31ac2d",
            "7d7e9a2ee6c94c3592dc5507ec8c1875",
            "41af9d5033dd4e93811bec9bd4f3473d",
            "983b40f0095147aa9e1b474d03d86059",
            "2ddf1cfb14dd4ab0bc61f302e84c8010",
            "2c1acf290bde47a68656453e24888508",
            "d25b53d997ba445eb8a2b96b05601cbc",
            "095b2f7b64114512b7b1f0000b9cfd0f",
            "bcd2e533fd4b4375b0b17624ec59c2fb",
            "ab1401ed222d42ea888713983df7a154",
            "6897ae7f06ed4a6f9724399bbbacfad0",
            "48d7d416f6d8432db43b7588d386bd7f",
            "161f41619d10412585b4031a9f59ea42",
            "a149b3303a3e4eb6b420ebaea9e8aa05",
            "a8a32b75054a4fbeb3ba8829a5ba7d52",
            "8e6249cb4ada4596b2843f27d54db15c",
            "06fae87262944c1097aec28999ea6e94",
            "8616ed225b654921a055adcf83151ce6",
            "efd638f331264751932a394b5e2e50e5",
            "b2b3a8ad8f174476b9a795bdca7a787a",
            "9e877347c7774c68b89ec99398d0ccba",
            "d15dcdc5c0b0432b961ef3e8e7ad711e",
            "2ff24ec10d2d474daf36e674a7fbadc9",
            "922208d7ca094198a0e53a06043e8577",
            "9bfc3adc0db24922a9c4f7607251448e",
            "e2f9cf89a05145f09fd1b693ac5f21c5",
            "07a9b623a1bc403b8a24028c4af581af",
            "60f0df423d8c4a85bc9c2a7bd95db9f8",
            "28806177c96f452faaa590d0875d2b9c",
            "65b171ac406743e08216ebf4015c2dde",
            "ed088afb6ccc4bdcb67a994608548117",
            "321aec2d23bd44f79d888b165983ba62",
            "0e8ef238bd4f48028c01d2bd25a66006",
            "d2026b7548e34221b2cf285c0cd52dd1",
            "0a4bdb0790814b3bb523e9b8368aa46a",
            "abbaed65369348bab45cb6588a50087f",
            "d2fbea8c70f74ed8b23895fc5c7132a7",
            "0c3abdec81924b45a9d9c2e198e4dc26",
            "147fd572852e45ae9ec4b4f07297e029",
            "6ffab45e03484f37b4f610034fed41f9",
            "1bfc92b7c3d342d590e40eab19a3982b",
            "d06170b8888049678b620d3c6b760c1b",
            "67b9fb1ae135451aa8154ac1f575bb25",
            "6522cea2c6f946e5af6a89e6ed757a73",
            "482d6c33af8b49dbba0c1a5ad9af9ff0",
            "7f697c14c60e4408bb4063192535025b",
            "851bae01b1184f02b00b012a56f4258f",
            "3b63c3adf15e431b9ff76d18d5ca69a7",
            "05a4dc46f3404fa5ae6c15ecd496083a",
            "66ef2ce86676439185bcb8b30d2a8a05",
            "4593d67f2bd84d8ba34e8fab5880f6c9",
            "6a97fd6b7e2a49488fc1b0177fdd7f58",
            "4e71cd4414c04a5a90e6a7de082144d9",
            "c26d4122a6f9431c95ddd0e837b2767b",
            "1079dd6068964665b12ebfe3d87861aa",
            "8ba038629d3e45388da0e207714967e7",
            "6bce05cf3d594fbbae229081c90588dd",
            "1c158b1f051846b29fb2e5c30287c8cb",
            "3daef41460284d38a13296e72fd3bd9f",
            "6f3bd55dd2ed44df820966182b6d12e5",
            "6b220f0a41224853bf13d34ab8d20f3f",
            "29932a1b63ca4114b7fe41002911115e",
            "4a83b7dba1aa43c29062687483dad720",
            "97fb4fd947124ad89b7f23f0af9edd19",
            "d1e00228c76140459d0857290e7e6b31",
            "2b785aefb6224da8b59462e3351ac71f",
            "2b584593148f4d0f9d1ca9c970a8e885",
            "241e4b39cf1e44a3a3b479cb05dff672",
            "6fc1ffb11bb14eb19f52b743b21a9672"
          ],
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "Nq-kyjxtL7YV",
        "outputId": "97cae8ce-0b12-47f5-a527-2da50681c213"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e0f26342dc4949bc85de0e60c4032951"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a9b152a5951412eae312595a99e617b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/90.3k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "085f5c2f98134aecb261c32928038d0d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "803d1531e8ae47e4a00ae1a82924d8b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/720 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c6eb13de0cc4e92b19e890d914d6467"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "983b40f0095147aa9e1b474d03d86059"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a8a32b75054a4fbeb3ba8829a5ba7d52"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2f9cf89a05145f09fd1b693ac5f21c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d2fbea8c70f74ed8b23895fc5c7132a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3b63c3adf15e431b9ff76d18d5ca69a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3daef41460284d38a13296e72fd3bd9f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding Model Loaded..........\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pre-processing"
      ],
      "metadata": {
        "id": "5DnjTWyUL7YW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.document_loaders import DirectoryLoader"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T14:33:29.091387Z",
          "iopub.execute_input": "2024-04-03T14:33:29.092275Z",
          "iopub.status.idle": "2024-04-03T14:33:29.105448Z",
          "shell.execute_reply.started": "2024-04-03T14:33:29.092242Z",
          "shell.execute_reply": "2024-04-03T14:33:29.104452Z"
        },
        "trusted": true,
        "id": "EuHqhjLRL7YX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and process the text files\n",
        "loader = DirectoryLoader('/content/', glob=\"./*.txt\", loader_cls=TextLoader)\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T14:33:32.022964Z",
          "iopub.execute_input": "2024-04-03T14:33:32.023966Z",
          "iopub.status.idle": "2024-04-03T14:33:32.099505Z",
          "shell.execute_reply.started": "2024-04-03T14:33:32.023932Z",
          "shell.execute_reply": "2024-04-03T14:33:32.098639Z"
        },
        "trusted": true,
        "id": "b6Gaqr5YL7YX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(documents)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T14:33:35.472943Z",
          "iopub.execute_input": "2024-04-03T14:33:35.473678Z",
          "iopub.status.idle": "2024-04-03T14:33:35.480483Z",
          "shell.execute_reply.started": "2024-04-03T14:33:35.473647Z",
          "shell.execute_reply": "2024-04-03T14:33:35.479435Z"
        },
        "trusted": true,
        "id": "7Rxxb-KpL7YY",
        "outputId": "d218e900-3828-4666-f3ce-3b1f336689c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting the text into\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "texts = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T18:24:14.285742Z",
          "iopub.execute_input": "2024-04-01T18:24:14.286099Z",
          "iopub.status.idle": "2024-04-01T18:24:14.466882Z",
          "shell.execute_reply.started": "2024-04-01T18:24:14.286072Z",
          "shell.execute_reply": "2024-04-01T18:24:14.466157Z"
        },
        "trusted": true,
        "id": "LSY0pRooL7YY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T18:24:44.252795Z",
          "iopub.execute_input": "2024-04-01T18:24:44.253145Z",
          "iopub.status.idle": "2024-04-01T18:24:44.292335Z",
          "shell.execute_reply.started": "2024-04-01T18:24:44.253116Z",
          "shell.execute_reply": "2024-04-01T18:24:44.291506Z"
        },
        "scrolled": true,
        "trusted": true,
        "id": "2oWdjEFbL7YZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95aafdb5-78db-43ef-90c2-ef942d1d13fc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"Okay, so I want to start by showing you some python code about how decision trees are used in the Scikitlearn library. So it should tell you at what level comfort you need to know about decision. So let me just try. And. So is this readable or should I make it a little bigger? It's visible, sir. It's readable, sir. Okay, so this is, if you remember when we were looking at the second lecture of decision trees about dealing with continuous attributes, we saw this decision tree which used this iris data set. So it turns out the iris data set is actually predecided pre available in this thing. So the first thing to note is that we are using library called scikitlearn, which is abbreviated sklearn. Lost my. So Sklearn is what we're using, and this code wants sklearn of a particular. So I'll share this code with you. And this is from this book, which is there in the list of references. The last book is Aurelian Jerome hands on machine learning with scikitlearn. So you can go and look it up.\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"this code with you. And this is from this book, which is there in the list of references. The last book is Aurelian Jerome hands on machine learning with scikitlearn. So you can go and look it up. So this code, the full code, is really, I've extracted some parts of it. So the other thing it uses is numpy because we will frequently use arrays and all that. And then there are some random numbers which are needed. So it seeds it with a predictable thing. So it uses 42. So if, you know, if you have read hitchhiker's guide to the galaxy, you know why it uses 42, but otherwise it's just some random seed. And now this is some stuff which we are going to ignore. But essentially they're going to be a lots of things which will Matplotlib will be used to plot various figures at various points. And this is related to that also where to save. So this is some setup which is not relevant to decision. This is more to do with organizing how and visualize things and save those figures for later. Okay,\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"is related to that also where to save. So this is some setup which is not relevant to decision. This is more to do with organizing how and visualize things and save those figures for later. Okay, so now this is the real part. So, so Scikitlearn has some built in data sets and one of them is this iris library. So the way to get the iris library into your data set is to import this function called load iris. And Scikitlearn has a bunch of models predefined. So what we have been looking at is decision trees as classifiers. So we are going to import from the subset of models of form tree. We are going to import this model called decision tree classifier. So the first thing that we do is we actually load this data set. So we call this load iris function, and then we are going to remember, the iris data set had two measures. It had the petal and the sepal, length and width. So it had four quantities. But we decided we will only use two quantities. So this is what this is doing. It's taking\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"iris data set had two measures. It had the petal and the sepal, length and width. So it had four quantities. But we decided we will only use two quantities. So this is what this is doing. It's taking this iris data set and it is looking at essentially columns, indices. Two and three are the siple and two, three are the petal. So it's taking all the rows and it's taking the third and fourth columns. So we're throwing away two columns. So we are making it, instead of a four column array, we are making it into a two column array. And in this data. So when you load the data set, it automatically produces these two subparts, iris data and iris target. So target is the classification variable. Remember, in this case, the classification variable is a three way thing, right? There are three types of irises that we saw, so it will be essentially a number, zero, one. And now to get this decision tree classifier, we have to take this function that we have imported. So we say take a decision tree\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content='of irises that we saw, so it will be essentially a number, zero, one. And now to get this decision tree classifier, we have to take this function that we have imported. So we say take a decision tree classifier. We have to provide it with, again some random state because it uses some randomization inside it. And now this is something which we have not talked about. So we discussed that we will build a decision tree normally until we either achieve some pure leaf nodes where the classification is uniform, or we run out of attributes. Now this is a numerical case, so we are not going to run out of attributes because we are going to run out eventually when we run out of data points. But we will end up with these very long slicing, the width between 1.5 and 1.71.5 and 1.6 and so on. So what you can do is you can tell the decision C classifier not to expand beyond a certain depth. And here it has fixed it to be depth too. So we will come back to this at a later lecture, which is, this is', metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"you can do is you can tell the decision C classifier not to expand beyond a certain depth. And here it has fixed it to be depth too. So we will come back to this at a later lecture, which is, this is related to this generalization question. That is, if you build a very detailed tree that asks a lot of questions, then you start asking questions which are very specific to the training data and you end up with what is called an overfitted model, a model which is following your training data too closely and is therefore picking up some peculiarities which don't necessarily exist within. So one of the things we mentioned in passing was that we like short trees for two reasons. One is because they are easier to explain. The second thing, which I claim without any justification, is that they generalize better. So here, priority, they are saying, let us not construct deep trees. So whatever tree we construct, we are going to stop it when it reaches. So it's a two step process, right? So what\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"better. So here, priority, they are saying, let us not construct deep trees. So whatever tree we construct, we are going to stop it when it reaches. So it's a two step process, right? So what you do is you first say what are the parameters for the decision tree classifier? So he says, I want a decision tree classifier to be set up with this random state 42 and which will not grow to more than depth two. And then I have to actually construct the classifier for a particular data set. So that's the next thing. So I use this fit function, right? In some sense this creates a decision tree object with certain operating parameters, and then you pass it the training data in the form of the input and the output x and y, and say fit. So the important thing to see is that this is all you need to do to build a decision tree in python. If you have the data, what you need to do manually is to probably decide. So the question is, can we specify a certain threshold? A certain threshold of what? Yes\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"to build a decision tree in python. If you have the data, what you need to do manually is to probably decide. So the question is, can we specify a certain threshold? A certain threshold of what? Yes sir. So there is also stopping criterion where we can stop. Correct. So you can also specify criterion saying that, saying that stop when your improvement is less than so much and so on. All these things have a lot of options. So I don't know offhand what the syntax for that is, but certainly you should be able to do that. So you need to look up the scikitlearn documentation for this classifier to see what are all the different ways in which. So what are all the different options that you can pass at the time of constructing the tree.\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"So this is something that certainly should be what is the, but the main thing I wanted to emphasize is that modulo understanding the parameters and what you can do with it, you actually don't have to do any of the encoding that we discussed in terms of finding the impurity and then maximizing it all that is actually something that is taken care of by the library. So all that discussion that we had about how a decision tree is constructed, choosing the best split for the numerical values, then comparing the impurity gain for all the things, picking the best one and so on, all that is done automatically. So you really never have to implement that. So the reason that it's good to know how all these things work is because indirectly that impacts how these parameters that you are allowed to control will work. So you have to have a little bit of an idea about what's going on behind the algorithm in order to make best use of it when it is not producing an answer which is acceptable to you.\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"will work. So you have to have a little bit of an idea about what's going on behind the algorithm in order to make best use of it when it is not producing an answer which is acceptable to you. But as far as using it per se, all you need to do is set up the appropriate. So here it's a decision tree, but pretty much the same. So these two steps that you see here, these two steps. So this is a pretty canonical style of programming. In this scikitler, you import the right type of classifier, you build a classifier of that type with the parameters that you think are reasonable for your application, and then you pass it the training data and say fit. Now here, one thing we have not done, for instance, is to segregate out, we talked about training and test data. We have not done that. So that's also something that you can do. Before you set it up, you might get all the iris data and then you might want to keep part of it aside. So we will see as we go along how to do all these things. But\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"also something that you can do. Before you set it up, you might get all the iris data and then you might want to keep part of it aside. So we will see as we go along how to do all these things. But for the moment, it's taking all the iris data which has come from the iris data set and using it to train the decision. That's also. So now this is some visualization stuff, which I'm not going to explain, but as a result of this whole thing. So the thing that has been fit before is now available. So I can pass that reclassifier to this graph thing. There is some issue which I have not debugged yet as to why it is not drawing things correctly. It's something to do with magnifying the screen. But anyway, we saw this picture in the slides, so this picture is actually produced by the python. So this is how the classifier looks in a human readable form. So as we said there, what you can see is that it's using internally this Genie index. So it's using that as the format there. And it's telling\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"is how the classifier looks in a human readable form. So as we said there, what you can see is that it's using internally this Genie index. So it's using that as the format there. And it's telling you at each point some information about how the data. Remember, we had three classes, 50 50, and then once we did the first split, we got this 500 zero and then all that. So if you remember those pictures, which I. So this was that histogram, remember? So this bottom most thing was the first type, the Cetosa, and then we had these other two things, this verseicolor and Virginica. So the first split was this petal length at some 2.45. And the second split effectively did not affect this part because this part was already, the left hand side was already pure. And then the right hand side we further split on the y axis at 1.75, I think. So that's what's happening. So 2.45, if you're on the left of 2.45, nothing more, you already have pure split on the right of 2.45. Then you further split that\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"split on the y axis at 1.75, I think. So that's what's happening. So 2.45, if you're on the left of 2.45, nothing more, you already have pure split on the right of 2.45. Then you further split that region into two by drawing a horizontal line and splitting it into two vertical parts. And at every point, there's a corresponding split. So 00:50 50 now goes as 00:49 five and zero, 145. So it has a nontrivial GD index. But because we said so, it's not because it decided to stop, but because we told it to stop. We said, do not expand history beyond depth two. Here we have reached depth two. Depth two means I've asked two questions. It won't ask any. If I said depth three, it would probably gone almost. So this is just a kind of. So notice that that classifier tells us now, if I give you a new pair of petal length and petal width, right? If I give you some combination of numbers, which is a hypothetical flower, it will follow this tree and it will put into one of these three classes. So\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"pair of petal length and petal width, right? If I give you some combination of numbers, which is a hypothetical flower, it will follow this tree and it will put into one of these three classes. So these classes, these boundaries are clearly horizontal and vertical lines. They really divide the space of all points in this region. That is petal length between, say, one and seven, and petal width between zero and three, it divides this space into these regions. So the first line is this vertical thing here. And then the second one is this horizontal line here. So every point known or unknown which falls into this yellow region will be classified as setosa. Everything known or unknown. If I produce a new point, even if it's not a real flower, but something which I just fictitiously manufacture, if the combination fits in this green zone, it'll be called the last one Virginia. And if it is in the purple zone or the blue zone, it will be called. This complicated code here is doing is it's\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"if the combination fits in this green zone, it'll be called the last one Virginia. And if it is in the purple zone or the blue zone, it will be called. This complicated code here is doing is it's just reverse engineering these geometric boundaries that the decision trees decisions correspond. Right? So these decisions define some geometric lines in the space of all possible inputs. And this is just helping you to visualize them, right. It's nothing more than that. So these are all byproducts. The actual classification is only those two lines of code. After that, you might want to see the data in different formats to interpret it. That is really not part of the classification, it's more the visualization part. And there are various visualization packages which you can use. So the way that it actually works internally is that if I give it some new input, give it some five. So remember that these are not exact, right? So there is some doubt because of the misclassification of the two\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"it actually works internally is that if I give it some new input, give it some five. So remember that these are not exact, right? So there is some doubt because of the misclassification of the two categories on the right hand side. So if I give it something which is of the form here, like five, this one, if I give it an input of the form five, comma 1.5, then because the petal width is five, it is clearly not yellow, but because it is 1.5, it is in this zone here. There is some ambiguity. So just purely based on the numerical values associated with these pure categories, this 54 and this 46. So there is a probability that it is verse color, which is 49 by 54. So 49 by 54 is about 90%, right? Because 54, if I divide by ten, is 5.4, so it will be a little bit more than 90%. And similarly here, so remember, the boundary is 1.75, so if it is less than 1.75, it should be green. But there is a probability that it is wrong, which I get. So that is what's basically coming out here. So you can\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"so remember, the boundary is 1.75, so if it is less than 1.75, it should be green. But there is a probability that it is wrong, which I get. So that is what's basically coming out here. So you can actually get the fact that with probability 90%, it is coming out to be the second category. But there is a small balance, residual probability, which is coming there. And these are pure, these are not real probabilities. These are just the misclassification ratios applied to those categories.\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"Right. If I am here, then I cannot be 100%. So if it were the case that at that second level they would pure, then I will get zero, 10, or one. But because those last two boundaries are not 100%, then internally, what the question is, how is the model calculating probability? I'm just saying it's exactly calculating it the way I said. So if it falls into. So if I say five, 1.5, and I follow the tree, it is bigger than 2.45, so it goes down the right, and then it is smaller than 1.75, so it comes down the left. So if it comes here with 1.75, then with probability. And this is where the thing is. So, with probability, 49 by 54, it is versicolor. So 54 -5.4 is 48.6, so 48.6 would be 90%, so 49 is slightly more than 90%, so 49 by 54 is slightly more than 90%. So it's saying, if it comes here, then with 90% probability, just as a ratio of the majority, the probability is that, and therefore the possible probability of it being wrong is five by 54. So that is the way it works. Is that\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"here, then with 90% probability, just as a ratio of the majority, the probability is that, and therefore the possible probability of it being wrong is five by 54. So that is the way it works. Is that clear? So that's exactly that number. So this 0907, if you work it out, should be. I don't want to mess around with this thing, but if you calculate it, 49 by 54 should be 0907 and five by 54 should be zero, nine. So these two add up to one, because it is clearly not zero. It's not clearly not the first case. The probabilities are just arithmetic expressions of the ratio. And if some data fall into one category, then it's certain that probability will only that it will not change, right? Yeah. So if that thing had said 54 out of 54 of that type, it will be one. So basically this is just the ratio of the majority. So we talked about misclassification as being the ratio of the minority. This is just a ratio of the majority interpreted. So with 90% probability it's going to be the majority\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"ratio of the majority. So we talked about misclassification as being the ratio of the minority. This is just a ratio of the majority interpreted. So with 90% probability it's going to be the majority class, and with 9% or whatever probability is going to be the minority class. And that's just that. 49 by 54 and five by 54. That's okay, good. So it's not really a probability, it's just a frequency count. If I come there, then 49 out of 54 times I have observed this and five out of 54 times I've observed that. And then of course it has to give an answer and that's what basically the prediction is. So this is really the other side of it. So fit was a function that we saw earlier. So fit created the model. We passed it an x and a Y. So X was a table of inputs and Y was a vector of output predictions which were learned. So that is the training data, and it produced a model which we visualized there. Now, when I want to actually run this model, what I have to do is I have to give it an\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"predictions which were learned. So that is the training data, and it produced a model which we visualized there. Now, when I want to actually run this model, what I have to do is I have to give it an input and ask it what do you think is the output? So things, effectively, the output will be ambiguous because there will always be some degree of mixing between the classes. So what it will do is it'll first compute according to what. In this decision tree, it will basically follow the input to the leaf. And in that leaf it will look at the distribution of nodes and it will assign a number to each one so that they are normalized to add up to one. And after I normalize and add up to one, I take the highest probability one as the answer. So if you now insist that this tree should tell you one of the three classes is obviously going to predict the class with the highest probability. So it gives me one. The reason it gives me one is that the three classes are encoded as zero, one, two. So\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"of the three classes is obviously going to predict the class with the highest probability. So it gives me one. The reason it gives me one is that the three classes are encoded as zero, one, two. So one is really the second class. The reason it's an array is because normally I predict not one value, but I predict a collection of values. So I pass it an array of inputs and I get back an array of outputs. So here I pass it a trivial array. So I pass it an array. There's an outer square bracket and the inner thing is an array of inputs, right? It's a two dimensional object. So normally I would two column matrix to predict, saying, here is a set of rows for which I want the output and this will produce a column of outputs. So that's why we have this double bracket. So you have the fit and then you have the predict. And the predict is the outcome of choosing the highest probability. So if you really want to see how close, I mean, for instance, if you get a one, then how likely was it that\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"you have the predict. And the predict is the outcome of choosing the highest probability. So if you really want to see how close, I mean, for instance, if you get a one, then how likely was it that it could have been a two instead? That is not visible here, because you just blindly get the highest probability. You don't know what is the difference between the highest probability and the second highest probability. So if you look at this predict, underscore the probability of the prediction, then you will get the actual differences. So you know in this case that it is reasonably correct, right? Whereas if the ratios had been some zero point 51 and 00:49 then you would have not taken this prediction with that much confidence. You would have said, okay, maybe it is not really that any question about this. Okay, so now one of the problems with decision trees, which you will see later on, is also that a lot of the construction of the tree depends on these statistics. Because we said that\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"about this. Okay, so now one of the problems with decision trees, which you will see later on, is also that a lot of the construction of the tree depends on these statistics. Because we said that we look at which attribute has the maximum discrepancy, whether it's entropy or genie index or whatever, which one of them will improve it the most. And that improvement, or lack of improvement really depends on the distribution of the different inputs at that point in my table. So if there is a borderline case, it could be that by shifting one value here or there, the attribute which has the highest improvement might shift from one to the other. And then I ask a different question. The moment I ask a different question, the tree becomes radically different because the question pattern changes and the split will change and so on. So here is to illustrate that what they have done is they have first, so y equal to one. Remember that the categories are zero, one and two. So they are picked out\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content='the split will change and so on. So here is to illustrate that what they have done is they have first, so y equal to one. Remember that the categories are zero, one and two. So they are picked out from the input those. So remember the petal has width and length, right? So the length and width. So after, initially we had sepal length and width, petal length and width, we reduced those four columns to two columns. So now we have petal length and width. So the second column, or rather the column index, one is the width. So this is picking out the second column of x where y is one, and this column value is maximum. So this is basically finding out that this particular flower, so remember that the width was varying between zero and three. But we are saying in this purple region, not in the purple region rather, but among all the flowers, which are the training data labeled blue, because this y is the training data value, what is the widest flower and that is this.', metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"So this blue flower here where you can see the cursor just at this corner here where I'm rotating the cursor. So this has width 1.8. So that's what we're identifying. We're identifying among the training data. So the training data where y is equal to one, where the label class label was one among those points, which is the one where the second column of the input, the width is maximum. Right? So now what you do is you basically remove it. So you say that I want a training set in which I don't have 1.8, so I have either class is equal to two, y is equal to two, or I don't have 1.8. So basically this has all the inputs except that one. If there were more than one, all the ones which are of class one and 1.8 get eliminated. So I'm basically effectively removing this one data point from my training set. Understand what I'm doing? So I'm taking the old training set of 150 flowers, and I'm removing this one tree by identifying which is the widest. So remember that last boundary was\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"my training set. Understand what I'm doing? So I'm taking the old training set of 150 flowers, and I'm removing this one tree by identifying which is the widest. So remember that last boundary was horizontal, so it was separating less wide from more wide. So I'm taking the one which is at the bottom category but closest to the boundary or in fact above the boundary and eliminating it from my data set. So I build now a new classifier. So I create a new decision, the same type, but I call it classifier tweak. So this is just a new object, and I fit it to this new data set, which is X minus at one input and Y minus this one. So you understood what I've done. I've basically taken the old data set and removed one extreme point from one of the categories, the widest blue flower. And now what happens is that I end up with a very different decision tree. So this is, remember, this is the second level visualization. So I'm not showing you the tree, I'm showing you the outcome of the tree. So\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"is that I end up with a very different decision tree. So this is, remember, this is the second level visualization. So I'm not showing you the tree, I'm showing you the outcome of the tree. So effectively what this tree is doing is that, remember the earlier thing, I would separate out the yellow from the blue by either drawing a vertical line here or by drawing a horizontal line as this one has drawn. So the earlier thing, we found it natural to draw a vertical line, but somehow by changing, this is the mysterious thing, by changing some point which is quite far away. It has nothing to do with this yellow set at all, by changing something very far away. Again, the idea is that the solid line is the first question and the dashed line is the second question. So mysteriously, the discrepancy which happens here at this point, which is the second question, which affects the second question, is actually changing the first question. So this is why these decision trees are rather unstable\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"which happens here at this point, which is the second question, which affects the second question, is actually changing the first question. So this is why these decision trees are rather unstable classifier. Because with a little bit of perturbation of the input, the tree can change predictable and dramatic way. So what this is now doing is it's first asking whether the width is more than whatever, zero eight or something here, right? And if it is less than zero eight, it's saying it's yellow. And if it's more than zero eight, then it's asking again about 1.75. And then, so the two data sets which differ only in this one input, produce very different decision. So that's what this is clear. Is it necessary to only remove one point if there were more? It's not designed to remove only one point. It could remove more points. It's designed to remove every point, right? Which. So first of all, it says either the width is not 1.8 or the category is two. So if you look at this line, so it\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"point. It could remove more points. It's designed to remove every point, right? Which. So first of all, it says either the width is not 1.8 or the category is two. So if you look at this line, so it says either the width is not 1.8 or the category is two. Now, in terms of all the yellow flowers, you know that their width is well below 1.8. So they are all going to satisfy the width not equal to 1.8. Among the blue flowers, we are eliminating the. We know there is one which is 1.8 and that will get eliminated. Among the green flowers. It's possible that there is something which is 1.8, but because y is equal to two, it will get retaked all the blue flowers which have 1.8, but I believe there's only one in the data set. So suppose instead of the point that we removed in this time, if we removed a different point, would we get a different decision tree? For that, it's not necessary. You will get a decision tree. It's different. But in general, the removing and modifying the data set in a\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"a different point, would we get a different decision tree? For that, it's not necessary. You will get a decision tree. It's different. But in general, the removing and modifying the data set in a very small way can produce it. So it's not guaranteed. It will produce. So you can experiment with it. So you could, for instance, in the same data set, probably that's a good experiment for you to use, which is that. Notice that we have two types of discrepancies here, right? So we have this horizontal line and we have this one blue thing which is above the horizontal. So what if you remove the one or two of the bottom green things which come below the line, these are also wrong. Right? So the blue thing above the line was wrong in some sense. The green triangles below the line are also wrong. So instead of removing the widest blue, you can try to remove the narrowest green or the two narrowest greens or something and see what happens. So you just have to copy paste this code and just change\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"instead of removing the widest blue, you can try to remove the narrowest green or the two narrowest greens or something and see what happens. So you just have to copy paste this code and just change the condition and all the visualization. You can just run it as it is and see, I have not done it, but I think you might get something different. So the predict outcome is not predictable. I cannot guarantee you that one or the other will produce a different output, but it can happen. And this is generally something which one has to be mindful of when you're dealing with decision. One reason that this is important is remember that we talked about this cross validation. So what do we do in cross validation or even in normal validation? In normal validation or in cross validation. In either case, you pull out some random data, set data points from your input and build a tree on the rest. Now the question is, the model that you built from the rest, is it a stable structure in the sense that\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content='you pull out some random data, set data points from your input and build a tree on the rest. Now the question is, the model that you built from the rest, is it a stable structure in the sense that is it sensitive to how you chose your training and test data? And this example suggests that you are likely to get different models from different subsets of training data. So you then have to worry about what is your final model going to be. So if you test it on a particular test set, then maybe if you test it on a different test set, you might get an equivalent model, but it might have a very different structure, and maybe for other reasons, that structure might be more natural. So this is a generic problem with this kind of tree model, that it is very sensitive to perturbations in the data. We will look at a more familiar problem, which you are generally aware of called regression. Right? So in regression, what do we do? We take a bunch of points, we try to fit a line to it. Now, if I', metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"We will look at a more familiar problem, which you are generally aware of called regression. Right? So in regression, what do we do? We take a bunch of points, we try to fit a line to it. Now, if I take one point in that set and remove it, or if I shift it a little bit, the line will perhaps change, but it will change in a very minor way, right? The slope will change slightly. So small changes in input produce small changes in output. So that is a kind of stable kind of situation. So this is something called variance. So how much variance is there in your model? So, variance is basically a property which says, is it the case? So if you have low variance, then small changes in input produce small changes in output. But decision tree, by definition, is a kind of discrete kind of thing. There's no way to slightly change a tree. You change a question and the tree changes drastically. So it has high variance, whereas a line fitting kind of algorithm will have a low variance, because if you\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content='no way to slightly change a tree. You change a question and the tree changes drastically. So it has high variance, whereas a line fitting kind of algorithm will have a low variance, because if you change a few points, you will get a slightly different line but the line will be recognizably similar to the other line. Does that answer your question? Yes, sir. Thank you, sir. Okay, so one of the things that you can do. So one of the classic data sets, which is difficult to classify by many models, is what are called these either half moons or bananas.', metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"So just imagine if you will just ignore the light, right? So you have this one yellow crescent of bananas and you have a blue crescent of bananas. They kind of interlocking. So you see the picture, right? So this is a very typical situation where it is hard to build good classifiers, or it's a good test of how your classifier works. There is this built in function in scikitlearn, which is called make moons. So you can make moons and you can provide that with some cellular blue. So I must warn you that I don't know exactly what all these things do, but essentially it's clear it's going to produce 100 data points, and implicitly 100 data points are going to be split 50 50. But this noise is going to somehow tell you how much they have this kind of intersection property, right? So in the middle of the blues, do you have a yellow. In the middle of the yellow, do you have blues. So what is the probability that these things are not separable by a nice boundary? So now what they are doing\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"of the blues, do you have a yellow. In the middle of the yellow, do you have blues. So what is the probability that these things are not separable by a nice boundary? So now what they are doing here is that they are basically taking this set of moons, except ym. So ym, it comes out on the moon's, the moon's data set basically produces these two sets of points of different categories intersecting. These two crescents are intersecting. So now you build these two classifiers. And here somebody asked earlier about stopping criteria. So here they're using a different stopping criterion. So here they are saying that if the number of samples that you reach in a node is less than four, then stop. So there's no limit on the size of the tree as such. But if you get a very small partition, then you just don't try to refine it further so you don't come down to zero, one and all that. So you just say the number of samples per leaf node should be at least four. So there are these two classifiers.\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"just don't try to refine it further so you don't come down to zero, one and all that. So you just say the number of samples per leaf node should be at least four. So there are these two classifiers. So one which is without such restriction and one which is with such restriction. So CLF one is the unrestricted tree classifier. CLF two is the restricted one. This is just to show you this overfitting thing. So see, the right hand side is with samples of at least four in every leaf. And this produces this reasonable boundary which you, I mean, you can see that it sort of looks like the squareish shapes look like two intersecting pieces of some kind of a jigsaw, right? So you have a yellow kind of piece here and you have a green piece here. And of course, there is this hole here, which could be yellow or could be green, but it turns out to be green in this case. Now, if you allow to go arbitrarily, then it starts finding these very, very. So if you have not noticed, there is this small\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"be yellow or could be green, but it turns out to be green in this case. Now, if you allow to go arbitrarily, then it starts finding these very, very. So if you have not noticed, there is this small yellow thing on the bottom right, which is misclassified in this cruder model. So the model which is forced to stop, whereas this model is not allowed, not constrained by that, it's allowed to construct a leaf, which consists of just that one yellow piece. So you can build this bottom right rectangle whose only job is to isolate that yellow piece. So from all the blue part there or the green part there, it asks a couple of questions. It asks one vertical question and one horizontal question and isolates that. Similarly, there is a blue point in the middle here, right there on the right hand side, you can see where my cursor is, that blue part. So here it basically create just to pull that blue point out. So this is, again, kind of an experimental or empirical justification of the argument\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"you can see where my cursor is, that blue part. So here it basically create just to pull that blue point out. So this is, again, kind of an experimental or empirical justification of the argument that smaller trees will give you more manageable and more understandable marks. The tree on the right sort of makes mistakes, but you can kind of figure out why it is drawing the lines where they are. And there are going to be a few anomalies with it. The left hand side is arguably got fewer anomalies, but it has created these strange islands of yellow at the bottom and the spike of green in the middle and all that, which may not make much sense. So this is as far as constraining the thing. Now, if I take this. So now we go back to our, this iris data set. So iris data set had capital X as the input. That was our original input. So now if I take that capital X, that capital X and I rotate it, so if I rotate it, I get a data set which looks like this. So I've just taken that same data set and\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"That was our original input. So now if I take that capital X, that capital X and I rotate it, so if I rotate it, I get a data set which looks like this. So I've just taken that same data set and just rotate it right, so that they lie in one direction rather than diagonal. And again, by this rotation, you end up with a very different classifier compared to the classifier that we had when it was unrotated. So there's no fundamental change in the geometry of the points. I just rotated them, but I've rotated them, and I'm using the same axes, right? So I'm using the same x one, X two. But I've kind of interpreted these points differently because I've rotated them and I get it differently. So all of these are just showing the susceptibility of the tree building process to the actual distribution of the data points. So if we remember, we deleted one extreme item, we got a very different tree, then we looked at that moon thing and we said, okay, if you don't constrain it, you get these very\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"of the data points. So if we remember, we deleted one extreme item, we got a very different tree, then we looked at that moon thing and we said, okay, if you don't constrain it, you get these very high dimensional trees, which don't make much sense. And here we are saying that one other thing, instead of deleting a point, if we transform it geometrically into an equivalent, geometrically equivalent shape, but the axes are now oriented differently with respect to the geometry of the points, I get very different. So these are the kinds of experiments you can do. So building the tree is trivial, but the interesting thing is to then see what you can do, the data, and seeing how stable the tree is. So these are the kinds of things that typically one does with these libraries. And I think this is, again, some other rotation. So let's not worry about this one. This is, again, some rotation example. So I just wanted to show you some code before we come back to theory. So, is there any further\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"again, some other rotation. So let's not worry about this one. This is, again, some rotation example. So I just wanted to show you some code before we come back to theory. So, is there any further questions? So, at the moment, I'm not expecting you to do anything with this code other than to understand that a, it is relatively painless to invoke a model in scikitlearn. And if you have this kind of visualization code, either you write it yourself or somebody gives it to you. You can then interpret your models visually and then experiment with it by changing things and seeing what happens. And that gives you a fair amount of insight both into your data and into how the model is working. So it gives you some indications about the limitations of that particular. Okay, so now I'm going to do something hopefully not unfamiliar to you. Right? At the beginning, when we talked about supervised learning, we said that we are trying to predict something, and those predictions can be in two types.\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content='hopefully not unfamiliar to you. Right? At the beginning, when we talked about supervised learning, we said that we are trying to predict something, and those predictions can be in two types. So one type is what we have seen in the decision tree, which is a classification. So the predicted value is one from a small range of possibilities. So we have been looking at a binary case, but even at an iris data set, there was a three way split. But we are choosing a class from a finite set. The other type of problem, which is very common, is to predict a numerical value. So we were talking in terms of predicting the marks of a student based on their board exam prelims and so on. So here is an example of that. So, supposing you want to predict the price of a house based on its square foot area. So you have, in this case, the training data will consist of the input is the house price for some given houses where you know the living area and you know the price. So these, now you would plot them.', metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"you have, in this case, the training data will consist of the input is the house price for some given houses where you know the living area and you know the price. So these, now you would plot them. In this particular case, it's a two dimensional plot. The x axis is the living area and the y axis is the price. And then you would like to, for example, say that if I give you up there, if I give you house which is of this size, then what would be the corresponding position? So I give you an x and what is the y? So you essentially want to map this data to some kind of a prediction function, which, given the living area, will produce a reasonable estimate of the price. So here is some more data. So supposing you expand that thing to say that you have two inputs, you have not just the living area, but you also have something about the number of bedrooms, and then you give price. So now I have something which is a three dimensional scatter plot. So I have x y or x one x, and my output is a\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"but you also have something about the number of bedrooms, and then you give price. So now I have something which is a three dimensional scatter plot. So I have x y or x one x, and my output is a function of x one and x two. So as we know, the simplest thing you can do is try and fit a linear function, right? So you can fit some theta one times x one plus theta two times x two plus theta zero. And try to see what are the values of theta zero, theta one and theta two, which best approximate the training date. So that's what we are going to talk about. So these are these linear predictions. So you have a bunch of input features, this case x one and x two. X one is the living area, x two is the number of bedrooms. And these are numeric values.\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"And from this you want to construct a function, a linear function of x one and x two, whose output will give you the desired value, which is the price. So because you have one coefficient with each of the terms plus this constant thing, if you have two inputs, you have three coefficients that you have to compute. So in general, you will have some k, such things. So these are usually called features, these inputs. And you always have to deal with this stupid thing. So to make it uniform, it's nicer if I can associate with theta naught, that constant term, a corresponding value x zero, which is an input. But I know that the theta zero always occurs as it is. So I can imagine that my input looks like. So I can basically pad out my input so I can take this input x one to x k and rewrite it as one comma x one, x two up to x k. So now if I take the theta as theta naught, theta one up to theta k and I do this dot product, then I'll get theta zero times one, which is theta zero plus theta one\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"one comma x one, x two up to x k. So now if I take the theta as theta naught, theta one up to theta k and I do this dot product, then I'll get theta zero times one, which is theta zero plus theta one times x one plus theta two. So I will end up being able to write my output. So this is my prediction. So theta is the thing that I need to determine. So for a given choice of theta, that is a given sequence or vector of coefficients, the prediction it makes is just this summation, right? Summation zero to k, theta I x I. So I can put it into a single summation because I have by convention inserted this x naught. But I have made sure that in my input data. So remember, these x I's are actually not unknowns. These x I's known. So these x I's are what you get. The theta I's are the unknowns that you are trying to figure out. But in terms of the theta I's you can predict for a given xi. If you have computed some theta I's, then the answer is summation theta I x. So therefore the target that\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"you are trying to figure out. But in terms of the theta I's you can predict for a given xi. If you have computed some theta I's, then the answer is summation theta I x. So therefore the target that we have is clearly to find the best line. So let's assume we have a simple case of only one variable. So we have x one, x two, and so on. So we have n points. In reality, each of these x size is a vector of k, and the y, by convention, is always the label. So in the earlier thing, when we looked at that scikitlearn decision tree thing, we used capital x to denote the input matrix and y to denote the column matrix of output. So this is a normal standard convention in supervised learning that x's denote the inputs of the training data and y denotes the output label. In this case, it's not a label, but it's an output value. So in order to compute the best fit, we need to measure in some sense how far we are away from the answers that we want. So how far is our prediction away from the true\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"but it's an output value. So in order to compute the best fit, we need to measure in some sense how far we are away from the answers that we want. So how far is our prediction away from the true answer? So of course, the usual problem is there that we don't know the true answer. But here, notice that unlike our decision tree, we are not likely to actually, because these values are all not quite lying on a line. It's not that we are actually going to get a line that passes through all the points that we started with. So in a decision tree, we are more or less tailoring our decision tree to give the correct answer for the given. So for overwhelmingly large values, large fraction of the training data, the decision tree will give the correct answer. So measuring the accuracy with respect to the training data will typically not give us very much information about how good the tree is on unseen data, whereas here, because we are kind of assuming that the data is slightly scattered and we\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"the training data will typically not give us very much information about how good the tree is on unseen data, whereas here, because we are kind of assuming that the data is slightly scattered and we will formalize this a little later on, we can measure our prediction with respect to the actual training data. So for each xi, we can first compute what our current model tells us and subtract what I know is the answer for that, which is yi. And now, this could be positive or negative, but I want to measure the error. So I could take absolute value, or in this case, I'm going to square it so I get a positive quantity. So we are taking the sum of the squares of the errors at the individual training points for this model that we have built at this current theta, which is a function h theta, which is just that linear function. Right. And so we are taking this square of the differences, we are adding this half term, which is just for convenience. We will see later on, it doesn't matter\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"which is just that linear function. Right. And so we are taking this square of the differences, we are adding this half term, which is just for convenience. We will see later on, it doesn't matter because, of course, if you scale by half, it's still going to be monotonic. So if you have more error scale by half, it'll still be more. So this is essentially the sum squared error, modulo that half. So we are taking the square of each individual error and summing it across all the training. If I divide by the number of training samples, then this will become the mean squared error. So technically, I think this should be a one. Make it correct. So we are going to use sum squared error. Why we are going to use sum squared error we'll later on. But we could have used absolute value. We could have used any reasonable function which accumulates, in some sense, the error that we have made. And we want to eventually, our goal is to make this error small. So this error is usually called, it is\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"used any reasonable function which accumulates, in some sense, the error that we have made. And we want to eventually, our goal is to make this error small. So this error is usually called, it is the cost that we incur. But traditionally it is called the loss function. So the loss function is a way of describing how far away our model is from the true values in our training data. So in a numerical setting, this makes a lot of sense. If I, then typically the loss will be zero or one. Either I get it right or I get it wrong, say a binary class. If the output should be zero and I predict zero, then I have zero loss. If the output should be zero and I predict one, then my loss is one or vice versa. So that's called a zero one loss. So zero one loss is a little bit harder to justify because you're just counting. In some sense, the fraction of it's more or less that what we called earlier, the impurity. How many are we getting wrong? Whereas here it's a more nuanced thing. It's not counting\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"just counting. In some sense, the fraction of it's more or less that what we called earlier, the impurity. How many are we getting wrong? Whereas here it's a more nuanced thing. It's not counting exactly how many, we might be getting all of them wrong, but by a small amount, and that's okay. But if you get a few of them wrong by zero, and we get a lot of them wrong. So for instance, if I were to draw a line somehow passing through, I don't know if there are three, but supposing there were three points, supposing it was possible to actually hit three points in this exact. But then in the process some of these errors became larger, then I would pay a different price, not because I've got more of them right, but because the sum square error is increased or decreased. So I'm not really counting how many I got right, whereas in a zero one case, if I did this loss, I would typically be counting how many I got right, so I would find a best fit. And by this preceding argument, best fit\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content='counting how many I got right, whereas in a zero one case, if I did this loss, I would typically be counting how many I got right, so I would find a best fit. And by this preceding argument, best fit corresponds to minimizing this discrepancy between our training data and the computed values. So we are going to minimize this sum square. So remember that we expand every input with this extra zero, right? So we had this extra zero at the beginning. So every input of k attributes has an extra zero attribute which is always set to one, so that when it multiplies by the constant term, it will just give me theta. So each x I, each input x I is a row vector. So I can now write my entire training data as this famous matrix x, where each row is one input. So I have n inputs one to n. Each row is one input. So I get n inputs n rows, and my output is a column of numbers in this case. So each yi is a corresponding output value associated with this particular equipment. And these are the values', metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"one input. So I get n inputs n rows, and my output is a column of numbers in this case. So each yi is a corresponding output value associated with this particular equipment. And these are the values that I'm trying to estimate the k plus one coefficients to go with the constant term and the k variables. So I want to estimate theta, which is theta zero to theta k. So I will write this. Now if I write, think of it as a column vector, so then it is theta transpose. So now remember our sum square error thing. So it says I compute h of x, I subtract the value y, I square it and add it up and then divide by two at. But what is h of x I h of x I is just this times. So I have one x one x k n theta naught theta one. This is theta zero. So the prediction for the ith input is just that Rho multiplied by my predicted coefficients. So I can write it as x times theta. So this is a column of predictions.\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"So I can take my column of predictions and subtract the column of expected values. And to square it, I just do this transpose, right? So this will give me a point wise squaring, right? Squaring of each term. So I have, for example, if this is so, this is my difference, right, between for y one and the predictive value for y two. So now if I take delta y one, delta y two, so this is a transpose multiplied by delta y one, delta y two, I get delta y one squared plus delta y two squared, which is what I want, right? I want the difference for each height squared. So that's why I'm saying take the difference and multiply the difference by pre multiplied by its transpose. So instead of this, I do this. So this is now an alternative representation in terms of these x, y and theta vectors of my loss function. My loss function is just the sum square error. Now I want to minimize this loss function. So I want to minimize this. So I want to say find with respect to the, so the variables, now\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"loss function. My loss function is just the sum square error. Now I want to minimize this loss function. So I want to minimize this. So I want to say find with respect to the, so the variables, now remember, the important thing to remember is these x's and y's are not variables. The x's and y's are actual concrete numbers which are given to me in the training data. The variables are the theta. That's what I'm trying to find. So how does this vary as a function of theta? And I want to find a setting for theta which minimizes this value. So I want to find in some sense a derivative of this j with respect to theta to be zero to get an extreme value. So if I differentiate this and set it to zero, then I have to do some working out to calculate the differentiation. So I take this whole, so this is just j, theta is just this. So I want to take the derivative of this rather with respect to all the thetas and set it to zero. So if I expand this whole thing out, I get this long x theta times x\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"is just j, theta is just this. So I want to take the derivative of this rather with respect to all the thetas and set it to zero. So if I expand this whole thing out, I get this long x theta times x theta. But then this is a transpose, remember, x theta transpose times x theta is theta. Just remember this. So the first x theta gets reversed and transpose. So I get theta transpose x transpose times x theta. Then I get theta transpose x transpose times y. Then I get y times x theta, y transpose times x theta and y transpose times y with the appropriate signs. I'm just multiplying that thing out by pulling the transpose into these two terms. So when I bring the transpose into x theta, I get theta transpose x transpose. Now these two are actually the same thing. So you can check this. Both of these correspond to multiplying y by the so therefore I can merge these two into this. So this is just some simple algebraic manipulation. So you can go and look it up and verify it for yourself as\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"these correspond to multiplying y by the so therefore I can merge these two into this. So this is just some simple algebraic manipulation. So you can go and look it up and verify it for yourself as you go along. And now if I differentiate this with respect to theta, then it will turn out that you want this quantity to be equal to zero. So this term will disappear because there is no theta in it. This term will produce x transpose Y. And this is where this half helps, right? And this is going to produce twice of. So this is more or less like theta squared. So it's going to produce two theta x x two theta x squared, and that's two gets cancelled. So that's how you get this. So now you set this equal to zero and you isolate the theta. So you take x transpose y to the other side, and then you pre multiply by x, transpose x, and you'll get theta. So this gives us actually an explicit solution for the best theta. And this explicit solution. So this explicit solution depends on the input,\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"pre multiply by x, transpose x, and you'll get theta. So this gives us actually an explicit solution for the best theta. And this explicit solution. So this explicit solution depends on the input, right? Because these terms. So if I change my x and y, I will get a different theta, which is what you expect, right? The parameters associated with the line should depend on the points that you provide to produce the line. But whatever x and y, you give me, if you give me any concrete set of points, this is guaranteed to produce the coefficients that best fit that set of points. And this is called a normal equation. So really for this linear prediction problem, we have a closed form solution. So as a learning problem, it's fairly straightforward in that sense, it doesn't require any fancy computing in terms of finding the solution. We don't have to. Like in the decision tree, we were trying to find out how the smallest model and all that, because we said we cannot compute the best possible\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"in terms of finding the solution. We don't have to. Like in the decision tree, we were trying to find out how the smallest model and all that, because we said we cannot compute the best possible model because it's computationally intractable, whereas here it looks like we don't have that problem, we have a closed form solution. So there is a fixed theta, which is the best answer, and we can always find. So the problem here is that this is actually, given the scale at which we are going to operate, we are going to find problems. So if we have something like 10,000 points already, we are going to struggle to do this matrix inversion. And also we need to check that x transpose x is invertible. And ten to the four is a relatively small data set. So the real challenge here is that of actual computation, not the theoretical computation, but the practical computation of this quantity as a matrix operation is not really feasible for the kind of data sets that one is expecting to see. So the\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"not the theoretical computation, but the practical computation of this quantity as a matrix operation is not really feasible for the kind of data sets that one is expecting to see. So the other approach, which is a more computational approach, is to do this iterative. So you make a guess. So, for instance, initially you plot a line which seems reasonable. Now you compute the error. So what is the error? The error consists of taking every point and measuring its vertical distance from the line. Right, the distance. This is y I minus h. Doesn't matter which way you would add it, the other way. So h theta of x I minus y I. That is just this quantity. Y I is the actual value. H theta of x I is what the green line tells you it is. So now you use this to improve your line. So you start, you have these big lines. You try to make the discrepancy smaller by adjusting your parameters. So you keep doing this until you find something which is the best. So you iteratively approximate that normal\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content='these big lines. You try to make the discrepancy smaller by adjusting your parameters. So you keep doing this until you find something which is the best. So you iteratively approximate that normal equation. So the question now becomes one of computing the best adjustment. How do we adjust? So, this is a general procedure which is called gradient descent. So the gradient is now, again, like we said before, we have a loss or cost associated with any approximation that we build. So it is sensitive to its parameters. So we want to know, in terms of each of the component parameters, theta.', metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"So what does slope tell you? It basically tells you, if I change x by a certain amount, how much will y change? That's the slope. If I just draw a simple line, if I move this by delta x, then how much is this delta? So that's what slope is telling. So now you are kind of generalizing this thing that in each parameter, if I change theta I a small amount, how does it affect this total loss function? Now, remember, the loss function is an explicit function. We know the loss function. We've already written it out when we differentiated it to get the normal equation. So the loss function can be explicitly computed. So we can actually compute this. So the gradient tells us something. And now what we do is we say, okay, if the gradient says that if I move in this direction, the loss will increase, then I move in the opposite direction. So you adjust. So the gradient is always the direction of increase, somehow, by convention. So you go in the opposite direction by a small amount. So you\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"then I move in the opposite direction. So you adjust. So the gradient is always the direction of increase, somehow, by convention. So you go in the opposite direction by a small amount. So you compute the gradient with respect to theta I, and you go a small step in that direction, and that is this step parameter called alpha. So you adjust each parameter in a direction. So here is a picture of this thing, right? So you start somewhere. So these vertical hills are supposed to indicate the loss function. And you want to come down to a thing here, and there is a kind of a strange surface whose shape you cannot really calculate because it's quite complex. And what you want to do is find a way down to the lowest point. So you look around you. So this is literally what gradient descent is doing. You look around you and you say, in which direction should I move? So that I go downwards. So it's a greedy strategy in that sense. Just like the decision tree was a greedy strategy. You say, here\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"You look around you and you say, in which direction should I move? So that I go downwards. So it's a greedy strategy in that sense. Just like the decision tree was a greedy strategy. You say, here are a bunch of attributes I can pick. Which one do I pick so that I make the most improvement. So here you're saying which direction should I move so that I go downwards? And then you make a small step in that direction. Now how much you make a small step will be of course an important question, which will come up to again and again. But you make a step in that direction, then you reassess, okay, now I have reached a new point by perturbing these parameters by moving in the right direction. Now what's happening? So you can again recompute the gradient. Again you move a small step and keep. So this is called gradient. So for a single training sample, let's see how you do it. So if you take a single training sample, then this is our formula for j theta. The loss function is theta of that. So I\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"is called gradient. So for a single training sample, let's see how you do it. So if you take a single training sample, then this is our formula for j theta. The loss function is theta of that. So I just have one sample, then I can expand it out. And then because of the square and all that, this is just saying the derivative of f of x whole squared is two times f x times f. So you work it out and you get this. So this is just coming from the fact that it's theta of x is this, we're just saying that d by d theta. D by d theta. So you're just using the simple derivative rule to come from here to here. And here you're just substituting. And then if you check this, this term has one component for every parameter theta. But I'm only interested in the theta I parameter. So all the other non theta I parameters will disappear and I will only get theta I, x minus y. And if I do, d of d, theta I of that, all the others are independent of theta I. So they'll disappear. I'll just get x. I remember\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"I parameters will disappear and I will only get theta I, x minus y. And if I do, d of d, theta I of that, all the others are independent of theta I. So they'll disappear. I'll just get x. I remember that x I's and yis are constant. Theta is the variable. So that's how I get explicitly compute. This is what I meant by saying I can explicitly compute the gradient and then I go in the opposite direction. So you can do this over the entire training set. You can work it out. So this is what is called batch gradient descent, right? So you compute this h theta for the entire training sec, right? So this gives you overall a loss for the entire training sec. Now you take this and you adjust each parameter based on this loss, and you'll get a new theta. Again. You will now compute this for the entire training set. So you'll get a new set of outputs again. You will compute the loss, and so on. So you keep repeating. And now remember, what we are ideally trying to get at is whatever answer we\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"training set. So you'll get a new set of outputs again. You will compute the loss, and so on. So you keep repeating. And now remember, what we are ideally trying to get at is whatever answer we would have got from that normal equation. But as usual, numerically this may not converge fast, so you might stop because you're not making much progress. So, convergence could either mean that you actually get a parameter for which the next iteration doesn't change anything, or you get a set of parameters for which the next improvement is small. So that's something which is normally there. You normally aim for the best theta, but for as good a theta as you can compute within a reasonable number of equations. Sorry if I missed something, but what is alpha here? What? Alpha. Alpha. Alpha is something which you have to fix. So we will not discuss it right now. We will come back to this when we look at more complex models where we apply gradient descent. But it is really something which tells us\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"which you have to fix. So we will not discuss it right now. We will come back to this when we look at more complex models where we apply gradient descent. But it is really something which tells us by how much we should adjust the parameter, I mean the value in the direction. So if it is very small, then you will make very small improvements. But if it is very large, then sometimes what will happen is that you will overshoot, you will overcompensate. So just to give a picture, so supposing you're trying to come here to this point you're trying to hit, this is your optimal parameter. Now here it might tell you that you have to go down, right? So that means you have to go to the right in some sense, not down, but you have to go right to go down, but you overshoot. And you overshoot so much that you end up actually going there. You see what I'm saying? The gradient is saying that if I go left, my error increases. If I go right, my error, how much should I go right? I should go right by\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"that you end up actually going there. You see what I'm saying? The gradient is saying that if I go left, my error increases. If I go right, my error, how much should I go right? I should go right by some multiple of alpha. But if alpha is very large, then it might take me overshoot beyond the minimum point to another point on the opposite side of the slope. Now it tells me that, oh, I should go left. But now, again, because alpha is large and the slope is increased, alpha times the slope is a large value. And I'll come here. So if you pick too large and alpha, then you will possibly diverge. If I pick too small and alpha, on the other hand, I will converge very slowly, right? So I will make incremental, many small steps towards it, and then it will take me very long time. So choosing the right alpha is a little bit of an empirical thing so that we are not going to get into. But usually you choose a small. The question about convergence. Convergence is just saying that you want to get\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content='right alpha is a little bit of an empirical thing so that we are not going to get into. But usually you choose a small. The question about convergence. Convergence is just saying that you want to get to the minimum. So the minimum is, there is an absolute minimum or theoretical minimum, you know, because if you could do that matrix transpose in matrix inversion, you can actually calculate your target and not do it iterative.', metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"So in our terms, what it means is that I reach a theta for which the gradient is zero in any direction. If I go, the loss is going to increase. There is no good way to move. So that is the next iteration, which should be the same as this iteration. But that would mean I've actually found this magical point. So in practice, what I will do is if I'm near the gradient, so let me do it this way. So supposing I'm near this. So supposing I'm here and I make a small step and I come here and I find that the difference between the two is very small. So I'll calculate the loss at the next step compared to the loss at the previous step. And if the difference in the loss function is below a certain threshold, which I will set according to my choice, then I will say I've converged. So either I converge because I'm not making progress or because I've actually hit the limit, which is very. So the other variation of this is that you don't actually. So in this batch gradient descent, what we are doing\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"because I'm not making progress or because I've actually hit the limit, which is very. So the other variation of this is that you don't actually. So in this batch gradient descent, what we are doing is we are evaluating the function that we have estimated on the entire data set before we make the next update. Right. So we have to choose an initial theta, which itself I've not told you how to do. You choose an initial estimate for the parameters. Then you have to run that thing on the entire data set, compute the loss on the entire data set, and then adjust the gradient of that loss. On the other hand, because the loss plays a role because of this. Right? So the actual values are needed here. So you need the actual values, you need the actual value in the adjustment. So the other option is that you do it as you're going along. So every time you evaluate one point, you compute the loss and then you adjust, and then you get a new theta. Then you evaluate one more point to compute the\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"is that you do it as you're going along. So every time you evaluate one point, you compute the loss and then you adjust, and then you get a new theta. Then you evaluate one more point to compute the loss and all. But you don't do it predictably, you do it randomly. So you pick up a random data point, make an adjustment based on the value at that data point and so on. So this is called stochastic gradient descent. And another way in between, which we will see later, is you take small batches. So here we batch gradient descent. You take the full. So if you have, say, 1 million inputs, you will evaluate your function on all 1 million inputs before you make one step of adjustment. In stochastic gradient descent, you will look at one input and then do it. Now, the problem with that is that that one input that you get might be one, which is kind of very extreme, and so it might give you a wild swing, and then you might oscillate a lot more. So it might take you longer to converge, but\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"that one input that you get might be one, which is kind of very extreme, and so it might give you a wild swing, and then you might oscillate a lot more. So it might take you longer to converge, but faster to compute. So there's a trade off between doing each iteration. You are making a lot of adjustments fast, but those adjustments are less controlled. The other way in between is take small batches. If you have 1 million things, you break it up into 1000 batches. So you do 1000 points. So you get some averaging out of the behavior of the function, make an adjustment, then you do another 1000, make an adjustment and so on. So all these things are actually used, we will see later in neural networks and all that. But in this linear prediction also, that's where they arise. I mean, that's the origin of this. So this is, of course, as you have all probably read. So this strategy is conventionally called linear regression, but it's basically this iterative linear prediction, which you can\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"origin of this. So this is, of course, as you have all probably read. So this strategy is conventionally called linear regression, but it's basically this iterative linear prediction, which you can do theoretically by a matrix operation, but computationally, it's more feasible to do it in this. So I'll stop here, and we'll continue next time to discuss why we are using some square error and also what to do when you have nonlinear things, how to deal with nonlinear approximations. Having done decision trees, I think at the next class, at the end of the class, the last 15 have a small moodle quiz covering up to last week's decision tree lecture. So it'll be about ten to 15 minutes at the end of the class. Okay, see you then. Bye.\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"So welcome to this first lecture on this course, DMML. So today I will try to set some context for the course and also tell you a little bit about start the first topic. So I have set up some information just a few minutes back on the moodle page to give you information about the list of topics, roughly that we plan to cover and also about the assessment and all that. So if you have any questions on that, you can look it up and maybe next time we can discuss that. So I won't spend too much time right now on the administrative aspects of the course. We'll just start looking at what we are going to do in this course. So if you look at the title of the course, it clearly says two things. It says data mining and machine learning. In a sense, this is a kind of historical thing. That's how this course was initially created some many years back. So if you want to look at the two parts of the title in some detail. So data mining is a loose term which talks about identifying some hidden\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"this course was initially created some many years back. So if you want to look at the two parts of the title in some detail. So data mining is a loose term which talks about identifying some hidden patterns in data. So we should think about mining in the sense of real mining. So when you do real mining, you're looking, say, for some precious gold or diamonds or something. So you will be digging through a lot of mud or stone or chipping away at rock in the hope of finding something precious, but you don't know for sure it is there. You might have some guidance. So in that sense, data mining involves going through vast amounts of information which has been collected for various reasons and searching for these nuggets of insight. So there are various other aspects which go into this. One is the fact that this data has to be collected to begin with. So historically, a lot of data was collected manually through forms and so on. And one part of the problem would be that these forms were\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"is the fact that this data has to be collected to begin with. So historically, a lot of data was collected manually through forms and so on. And one part of the problem would be that these forms were manually entered by somebody and then converted to electronic forms. So that would be, there would be two levels of potential sources for errors. The person writing down the information and then the person typing in the information. Now, gradually these kind of electronic forms are spilled in directly, so at least the source of the error is reduced to one step. But still, people mistype things. I mean, there are any number of situations where people type their email address wrong and so notifications don't reach them and so on. So there is this data collection. How do you collect the data and how do you clean it? And the third thing is, how do you make it uniform? So when data is being collected by different people, they may collect different things. And for instance, if you look at the\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content='how do you clean it? And the third thing is, how do you make it uniform? So when data is being collected by different people, they may collect different things. And for instance, if you look at the government, typically the government collects data in different forms. For instance, there is a public distribution system which the ration shops, so they collect some information about who is collecting ration and who is income levels and so on. Then on the other side, you have, of course, things like electoral information. Then you have property information. Who owns property, who pays electricity bills, who owns vehicles, vehicle registration, driving licenses. Not necessarily the same thing. Not everybody who has a driving license owns a vehicle. So you have all these different sources of information. Now, is it possible really to find out same person in all these? Well, a lot depends on how the information is collected. You have a common format in which you record information about', metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"information. Now, is it possible really to find out same person in all these? Well, a lot depends on how the information is collected. You have a common format in which you record information about people. Even names in India tend to be spelt in different ways, written in different ways. Sometimes we expand initials, sometimes we suppress them, sometimes we write middle names, sometimes we don't. Sometimes we invert the order, sometimes we don't. Addresses, of course, are written in a million different ways. So there are all kinds of issues with just getting the data to a format where you can work on it. So this is an entirely different ballgame. It's much more kind of. There are lots of tools and techniques to deal with this, but that's not really going to be the focus of the course. So, in a sense, if you want to think about it, this data mining aspect will be almost missing from this course, even though it's part of the title. So what we are really going to look at is the machine\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"in a sense, if you want to think about it, this data mining aspect will be almost missing from this course, even though it's part of the title. So what we are really going to look at is the machine learning aspect. So learning means you are trying to understand something that you don't know before. And machine learning suggests that it's done automatically. It's done by now machine as a computer. So there is an algorithm which learns something about, again, it's always with respect to data. So it's something about the data. So what we are trying to do is learn some kind of mathematical models from data. And this falls into two broad categories. So the first category is where you are given examples. So you are told, for instance, this is, for example, supposing you're trying to evaluate whether a painting is painted by a particular type of. So there are these pools of painting, like impressionists and cubists and so on.\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"So you might be given a lot of pictures saying, this is cubist, this is not cubist, this is impressionist, and so on. And then from these examples which are given to you with manual information, you are supposed to build a model which will take an unknown painting and tell you from its features which of these categories. So this is a typical supervised learning kind of thing. Unsupervised learning is something which is closer to the data mining situation where you have data and you are looking for patterns, but you don't really have a clear idea beforehand what patterns they are. So a typical example of this might be that a company which is selling something might want to know some information about the demographics of its customers. So what are the groupings? I mean, which age groups? What is the proportion of people who are, say, between 20 and 30 who buy their products, people above 50 who buy their products, and so on. So this kind of thing is unsupervised because you don't know\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"is the proportion of people who are, say, between 20 and 30 who buy their products, people above 50 who buy their products, and so on. So this kind of thing is unsupervised because you don't know what you're going to get. But after you get this information, maybe you can put it to good use. So these are broadly the two types of things that are there. There is a third type of machine learning called reinforcement learning, which we are not going to talk about at all in this course. So to look at supervised learning a little more detail, we will of course do it in much more detail in the lectures to come. So we are trying to extrapolate. So basically it's a prediction problem, right? We have some historical information and we want to predict something in the future based on this historical information by building some model, some mathematical model using which we can make these predictions. So, for instance, when a school conducts exams before the board exams, one part of it is for\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"information by building some model, some mathematical model using which we can make these predictions. So, for instance, when a school conducts exams before the board exams, one part of it is for practice, to give the students some practice. So these model exams are partly to get students acclimatized to the structure of a board exam, but that they could very well do on their own. But another, perhaps more relevant thing from a school perspective is to try and understand how well their students are likely to do so. The model exam performance can be seen as a predictor of board exam performance. And if there are concerns, for instance, the school feels that some students are not prepared enough and they are going to do badly, they might use the model exam performance as an indicator to kind of counsel the students about postponing their exam. So this is a situation where you're trying to predict something based on something that you have seen. And of course, how do you do this\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"to kind of counsel the students about postponing their exam. So this is a situation where you're trying to predict something based on something that you have seen. And of course, how do you do this prediction? Well, you presumably know in previous years something about how correlation is there between model exam performance and board exam performance. So that's the model that you want to build. Another thing might be a bank. So bank is handing out loans, so somebody comes to apply for a loan. Now you have a historical policy and historical data about previous customers and whatever profile information you collect about them. So you might collect information about their income and their assets and their liabilities and so on. And based on that, you'll make a decision. So now you have a new customer. You want to know whether, based on this person's profile and the historical policy that you have been following, should this loan application be granted. And of course, today, all around\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"customer. You want to know whether, based on this person's profile and the historical policy that you have been following, should this loan application be granted. And of course, today, all around us, one of the things that we are most concerned with is medical diagnosis, which is also in the same spirit. Right? So each time a new strain of COVID comes, it comes with its own symptoms. It takes a while for people to recognize that sometimes it is loss of smell, sometimes it is some stomach problem, sometimes it is just cold and a sore throat. So each time we have to kind of look quickly at people who have symptoms, who diagnose themselves, who get tested, who diagnose positive, and then figure out which symptoms are actually indicating the new brand of COVID So these are all in the same broad category of. So in all of these things, the manually labeled historical data is available. So we know from previous years students how they did in model exams and how they board exams. We know\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"broad category of. So in all of these things, the manually labeled historical data is available. So we know from previous years students how they did in model exams and how they board exams. We know previous customers, what their profile was and whether they were given a loan or not, whether they defaulted on the loan. So not just whether they were given a loan, but also their future behavior. Right? At one point you give a loan, but later on it's not necessary that you made the right decision. So if you didn't give a loan, of course you don't know anything because that customer is not with you anymore. But if you did give a loan and for some reason that loan did not work up the way you wanted, the person defaulted or something, then that's a red flag. So maybe you need to go and check those things. And for medical things, of course, over time, not just recently, everything in medicine is really based on this kind of maintaining of records and looking for these kind of models across\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content='things. And for medical things, of course, over time, not just recently, everything in medicine is really based on this kind of maintaining of records and looking for these kind of models across patients, across different countries, across generations, people maintain these things to look for these patterns so that we can identify quickly when certain symptoms indicate certain conditions.', metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"So what we really want to do is this. So this is our golden aim, which is to go from historical data to a model. So we want to build some kind of a model which we can use for prediction. And that's really going to be the focus of what we are going to do in this course. We are going to look at many different types of models that are used for this. So within this, there are two types of predictions that we make. So one type of prediction is actually a numerical value. So we are trying to predict, for example, board marks, an insurance company might want to know how much a house is worth, right? So somebody comes to insure the house. And they say, I want it to be insured for, say, 75 lakhs. And the question is, is this house really worth 75 lakhs or is it overvalued or undervalued? So the insurance company needs a way of looking at various features of the house and determining whether this is an accurate value. So again, it's a numerical question. And of course, for loans, indirectly,\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"insurance company needs a way of looking at various features of the house and determining whether this is an accurate value. So again, it's a numerical question. And of course, for loans, indirectly, what the bank is trying to assess is some kind of net worth. How much does this person own in terms of all the property and bank deposits and shares, and whatever else this person may have, minus whatever the person owes in terms of previous loans and other liabilities, what is the total actual net worth of an individual? And the other type of prediction that we make is something that we encounter quite often in our own life. So one is, of course, email. Junk mail, right? So almost every mail reading program or client or mail software like Gmail has a way of flagging some mail as being junk. So somewhere there is a machine learning model which is looking at different features of the mail and deciding whether this is junk or not. But the learning part of it comes because junk is not really\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"somewhere there is a machine learning model which is looking at different features of the mail and deciding whether this is junk or not. But the learning part of it comes because junk is not really something which is uniform. Not everybody perceives the same mail as junk. Similarly, this insurance thing, if you go to the next step, if somebody actually makes a claim, first we said we want to value the house and decide how much to insure for. But if something is already insured and somebody makes a claim, one of the first things to check for an insurance company is whether this is a genuine claim or not. Is there some fraud involved or is this a genuine claim? So how much time and how much effort do you spend in doing this before you pay out it? If it turns out that you spend a lot of effort and something turns out to be genuine, and then you pay it, you're paying double. It makes a lot of sense. If you spend some effort and time to check a fraud and you save on paying out something\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"and something turns out to be genuine, and then you pay it, you're paying double. It makes a lot of sense. If you spend some effort and time to check a fraud and you save on paying out something which you should not. So there is a trade off there. And this is, again, something where if you had an efficient system which was based on machine learning, you could possibly get some benefits. And finally, banks, we have looked at other examples like loans, but also credit cards. When you apply for a credit card, a bank will typically go by your income and various things and decide whether to give you the card or not. But nowadays, if you get the card, there may still be a further refinement. The banks issue many different kinds of cards. So they have some premium cards and they have some normal cards. So they might decide that based on your income profile or your economic profile, that you are willing to pay something extra for the card to get more benefits. So this categorical thing may\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"cards. So they might decide that based on your income profile or your economic profile, that you are willing to pay something extra for the card to get more benefits. So this categorical thing may not be just two ways. So the idea is that these are kind of more or less binary. Either it is junk or it is not junk. Either you pay or you say it's a fraud, but this is something which is a multicade. So it's not just yes or no, it is no. And then in yes, there are two parts. Another similar thing could be that if I try to tell you whether this news item is sports or arts or it is a book review, so if I look at some news feed which is coming from some press agency, and I want to put it in the right place, I need to know whether it's about politics or sports or something. So, topic classification of documents is also a multi category classification topic. So, as we said, these models that we use are of different kinds, and each of them is, there are two parts to it. There is a generic model,\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content='of documents is also a multi category classification topic. So, as we said, these models that we use are of different kinds, and each of them is, there are two parts to it. There is a generic model, and then there is some specific part of it which needs to be tuned according to the data. That is the learning part. So we have to fit the parameters to the model. So, for instance, the simplest kind of thing that you can do is to kind of, if you have some points, try to fit a line through the. All of you have seen this at various points in school and all that when you are doing experiments. Now, the shape of this line, so the model is a line. So that is known. But what is the unknown? What is the position of this line? What is its y intercept? What is its slope? And this will be determined by the point. So that is the sense in which you learn. The parameter fitting is the learning, as we said before, even junk mail, junk mail is filtered out based on some characteristics. It could be some', metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content='the point. So that is the sense in which you learn. The parameter fitting is the learning, as we said before, even junk mail, junk mail is filtered out based on some characteristics. It could be some terms which come in the subject, I have a great offer for you, or my husband has left me a million dollars.', metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"So these are all signals that probably this is some junk which is coming your way. But there may be more subtle things. Somebody might be talking about events, some kind of pop music events, and another person may be talking about some spiritual gatherings. And one person may be interested in pop music events, and another person may not be. And another person may be interested in spiritual gatherings, and the first person may not be. So then you can train these things. So in that sense, which words signal junk and which words don't signal junk is also a parameter of the model. Just like in this linear fit, the shape of the line is a parameter. So this is the learning part. So what we are going to do is look at different types of models, as I said, and then we will look at this parameter adjustment. How does the data actually determine the model, the concrete model? How do we build the best model that we can get for the given data? That's the algorithm. So we have a kind of model\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"adjustment. How does the data actually determine the model, the concrete model? How do we build the best model that we can get for the given data? That's the algorithm. So we have a kind of model template. So we have a model template on this side, we have training data, and what we have now in between is a machine learning algorithm which comes out and produces a specific model. So in a sense, it's different from a typical program that we write. So typically we have a problem to solve. The problem to solve is given to us in saying, say you want to compute some numerical quantity, you want to compute, say, the log of x. So your problem statement is given x return log of x. And then you come up with some calculational strategy which allows you to compute this quantity, and then you implement it as code. Here it's different. So here you are given some kind of a format. You're saying you want to fit a line, and then you're given data, and then you're supposed to now take this line fitting\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"it as code. Here it's different. So here you are given some kind of a format. You're saying you want to fit a line, and then you're given data, and then you're supposed to now take this line fitting thing and make the best line out of it. So it's not quite generating a program from a specification, but generating a concrete program from a generic program you can think of a model template as. So I just give you a line here which says a Y equal to Mx plus c. And here I want a specific m zero and C zero, which fits the given data in the best possible way, which is also part of the description of the algorithm. What does it mean for one line to be better than another line? So that's part of it. So the other side of this picture, as I mentioned, is when you don't have this training data, you don't have any examples that people have already labeled, but you are looking for some kind of patterns. So there is no guidance. So the thing about supervised learning is somebody should have gone\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"don't have any examples that people have already labeled, but you are looking for some kind of patterns. So there is no guidance. So the thing about supervised learning is somebody should have gone through and given you this information before. Now, this information could have been collected over time. Maybe it's not manually collected, like the school marks. The school marks over time. Everybody has. I mean, the school keeps a record of what happens in the model exams, what happens in the board exam. So it comes automatically. The bank will be keeping records of all the loans that it bought and what happened to the customers who bought loans. So this historical information is accumulated over time. On the other hand, in some situations, like the example I mentioned about classifying paintings, now, when you go into a museum or you go into some art gallery or something, the painting doesn't have a label on it, saying, this is a painting of this type and this is a painting of that\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"paintings, now, when you go into a museum or you go into some art gallery or something, the painting doesn't have a label on it, saying, this is a painting of this type and this is a painting of that type. So somebody, if you want to fit this training model to it, somebody has to sit and actually label these things for the algorithm. So there is a lot of work in it. So generating a lot of valid training data is itself a kind of computationally intensive task. How do you provide a lot of well trained examples? Because the assumption is that the labels that these training samples have are somehow correct so they can be trusted. Because if you can't trust those labels, and the model that you build will obviously not work. So if you want to write generalization of the labels that people have given you, you have to trust those labels. So that means somebody reasonably conversant with the domain has to be willing to put in the time to label it. Now, if it happens, as I said, because\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"have given you, you have to trust those labels. So that means somebody reasonably conversant with the domain has to be willing to put in the time to label it. Now, if it happens, as I said, because historically you're collecting data, it's fine. But more and more, we are finding that this is not the only source of data. A lot of data now actually comes out of automatic things like we have systems which are producing. For example, if you run a computerized device like a network switch or something like that, all these things generate a lot of diagnostic data. Even things like cars and planes and vehicles, which run with a lot of electronic components, generate a lot of diagnostic data. That's why whenever there's a crash, for instance, people are looking for the same as black box. Now, if you're trying to build a model based on that, then who is going to sit through and go through all this diagnostic data and tell you that, oh, this part of the data suggests that there's a fault here,\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"you're trying to build a model based on that, then who is going to sit through and go through all this diagnostic data and tell you that, oh, this part of the data suggests that there's a fault here, and that part of the data suggests that there's a fault there.\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"That's something that is not very easy. So that's where this unsupervised learning becomes actually important. So, as I said, a prototypical, almost canonical example of this is customer segmentation. So you have different types of people who are selling different products, who are interested in this. As a newspaper, for instance. You want to know what kind of people are reading your newspaper. Now, this has two reasons. One is, of course, that it guides what you write. People target their content. I mean, news is not in some sense an objective quantity anymore. So you can take news and you can interpret it according to what your customers would like. So some of them may be, first of all, politically left wing, right wing, and then old and young. So you might focus on certain types of news. If you are targeting a younger audience, you might spend more parts of your paper on items pertaining to sports and entertainment. And if you are targeting an older audience, you might have more\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"If you are targeting a younger audience, you might spend more parts of your paper on items pertaining to sports and entertainment. And if you are targeting an older audience, you might have more articles about the economy and politics and so on. But the other side is also advertisers. Right? So, advertisers also look at this. So who advertises in your newspaper will be determined by who reads your newspaper. So, for a newspaper, it is important to be able to figure out who their client is. And how do you find this out? Well, you, of course, figure out through some sampling or through some explicit customer information, details about all your customers or some large fraction of your customers. And then you have to look for patterns. You have to look for these groups. So the same thing might happen in a shop. If it's a shop which sells multiple types of things, it could be a clothing shop which addresses different age groups, different genders, different types of clothing, might be\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"might happen in a shop. If it's a shop which sells multiple types of things, it could be a clothing shop which addresses different age groups, different genders, different types of clothing, might be sending fashion clothing and sports clothing and baby clothes and various things. Then depending on how many people are visiting of different types, you might realize that one category is taking up a lot of space and it's totally useless, or you're making a lot of money in one category, but it's underrepresented. So, again, this customer segmentation helps. And, of course, something which has become almost a kind of a disease now is that we are no longer seeing entertainment because we would like it, but we are saying it because they know that we like it. Right? So Netflix, everything, Amazon Prime, Z, everybody is now producing tv shows and movies based on what they perceive that people will like. So it's not surprising that most of what they produce is popular with at least one large\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"Prime, Z, everybody is now producing tv shows and movies based on what they perceive that people will like. So it's not surprising that most of what they produce is popular with at least one large segment of the audience, because that's how it's designed. But this is a bit strange, if you think about it. But this is based initially on some kind of unsupervised learning. You look for what people know, and then you can also use it to recommend. So when you log into one of these things and it suggests that you watch something, how is it suggesting that? So you have not watched that thing. So there has to be some way of correlating what you like with what that movie is about. And there are two dimensions to this. So I am a user and I am recommended a movie. So why am I recommended this movie? It's because I am like some other user, and there is a movie like this which this person liked. That means I must have some way of computing similarities between users and movies. Or it could be that\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"It's because I am like some other user, and there is a movie like this which this person liked. That means I must have some way of computing similarities between users and movies. Or it could be that I am a user. I am like this, I like m prime. And this person liked m. And therefore, if these are equal, then I should also like. So there are many of these kind of pictures that one could draw, but all of it boils down to looking for these groupings. We want to look for groups of users who have similar tastes. We want to look for groups of movies which are similar in nature because similar groups of people watch them. People who watch one also watch the other. So these are some aspects of this unsupervised learning that are used quite a lot. So at the heart of unsupervised learning, one of the most core ideas of unsupervised learning is the idea of clustering. So you want to take a kind of. So this is the top, a kind of generic collection of points. And then you can see in this generic\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content='the most core ideas of unsupervised learning is the idea of clustering. So you want to take a kind of. So this is the top, a kind of generic collection of points. And then you can see in this generic collection that there are some obvious groups, right? So you can see that there are, of course, these three groups here which are quite evident. And then in this most fast thing, there are these two clones, right? And then if you formally run some kind of an unsupervised learning model on this, then it will try to partition this geometric space into groups of points which go together in such. So it will find, in this particular example, these five partitions. And this will also include places where there are no points. So there are no points here and here and so on. But this allows you to do some kind of prediction. If somebody produces a new point in one of these empty spaces, you can sort of assign it to be of the same type as the nearest group that you are from. So in that way, you can', metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content='kind of prediction. If somebody produces a new point in one of these empty spaces, you can sort of assign it to be of the same type as the nearest group that you are from. So in that way, you can use this unsupervised learning as a way of also doing some kind of classification, which is a typically supervised learning task.', metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"Now, when you do this, you need to also understand something about how the data looks. So there could be some data which is what is called an outlier, which is absolutely against the average. So typical examples of these are the people who are extremely wealthy. So if you are talking about something about to do with income, I mean, some property to do with the income of an individual, then if you have extremely wealthy individuals who are out of the scale in some sense, and then if you include their income in the calculations, then usually everything gets distorted terribly. But unfortunately, until you examine the data, you don't know this distorting factor, and usually you cannot. How do you know whether something is distorted or not? Usually you want to check whether it's very far away from the average, but then you have to compute the average. How do you compute the average if you have very large sense of data. So in some sense, how can you find these anomalous points without\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"away from the average, but then you have to compute the average. How do you compute the average if you have very large sense of data. So in some sense, how can you find these anomalous points without actually doing an exhaustive calculation? Can you do it without actually computing the mean and standard deviation of the entire data set? Another thing that happens usually in this kind of machine learning is that we are looking at different attributes. So, as I said, when you do email classification, you might classify an email as junk based on various factors. One is, of course, the words that appear, but the words that appear also appear in two places, in the subject and in the main text. And then there's a question of the attachments. Are there some attachments, large images or not? And then who is it from? Where is it coming from? So each of these is a different dimension to the data. And now if you look at a bank customer who comes for a loan, then you have even more dimensions\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"then who is it from? Where is it coming from? So each of these is a different dimension to the data. And now if you look at a bank customer who comes for a loan, then you have even more dimensions because you have their income, you have their previous year's income, you have some, whether they own a house, what is the value of the house, and so on. So you have this high dimensional data in which you're trying to do something, and very often it's very hard to work because it just multiplies out into too many possibilities. So one of the things that one can do sometimes is to actually reduce the dimensions. And sometimes when you reduce the dimension, the problem becomes easier. So here is an example. So you have this kind of, imagine this is a kind of a carpet, the thing on the right, like an exercise mat or whatever, you take this carpet, which is colored like this, and then you roll it up. Now, if I give you this rolled up version on the left, then if I ask you to decide, given a\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"an exercise mat or whatever, you take this carpet, which is colored like this, and then you roll it up. Now, if I give you this rolled up version on the left, then if I ask you to decide, given a point, whether it's green or yellow, it's kind of difficult, because there is a three dimensional twisting of this data, which doesn't make the picture very obvious. Whereas if I'm able to unroll it and produce this two dimensional thing, then there is a very simple line which separates the green from the other. So this is a situation where by reducing the dimension, somehow, you make the problem easier to tackle. But this is an unsupervised thing. Reducing the dimension is something that you do blindly in the hope that it gets you a benefit. So I can take another example of the same type, and I can start with something which actually, in the original thing, is separated by a simple plane, right? So I have this plane here which separates the green from the yellow. But then, believing that\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"I can start with something which actually, in the original thing, is separated by a simple plane, right? So I have this plane here which separates the green from the yellow. But then, believing that it'll be better to work in two dimensions, I actually unroll it. And then I find I have to deal with these multiple bands. So now it's not so obvious anymore because I do have lines, but I have many of them, and I have to decide between which bands it is, green, and which bands it is, this yellow color. So the peril of supervised unsupervised learning is that you don't know that the step that you are taking in advance, you don't know is going to be good or bad because it's unsupervised. You're just taking a guess in some sense, but then you have to have some way of evaluating whether the guess was good or not. And then going back and doing it again may be a different. So this is kind of a quick overview, a summary of the main things that we will be looking at. So, supervised learning and\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"was good or not. And then going back and doing it again may be a different. So this is kind of a quick overview, a summary of the main things that we will be looking at. So, supervised learning and unsupervised learning. So we are looking either to build a predictive model, which is broadly a classification model, or a numerical prediction model. As we said, in case you're trying to predict a number or category, the category is more like a classification problem, right? I give you something and I want you to classify it as yes, no, good, bad, or we have unsupervised learning where we are looking for some kind of structure, and usually this is in the form of clustering, but we also have to deal with these outliers, as I said. And then sometimes you want to simplify the problem by knocking off certain dimensions or unrolling the object or so. So if you look at the popular press and the popular literature, all the focus of machine learning is actually on the first one.\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"So supervised learning is by far the more attractive and the more, should I say, attractive, I guess it's the more attractive aspect of machine learning. This is what people are really looking for. But unfortunately, as I said, to do effective supervised learning, you need to actually get a lot of training data. And this is sometimes expensive. Very often it is expensive. So unsupervised learning is often the starting. A few. Couple of years back, there were three turing awards given for deep learning. So one of them was given to Jan Lakun. And his observation is that if you think of this whole thing as a cake, then actually most of the cake is actually unsupervised learning. So supervised learning is just a thin layer on the top, but it's like the icing on the cake. So when we go to a bakery and we see an attractive cake, what we're attracted to is the surface, the icing surface. But if the inside is made of sodas, then the cake will not taste good. So you have to be careful not to\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"and we see an attractive cake, what we're attracted to is the surface, the icing surface. But if the inside is made of sodas, then the cake will not taste good. So you have to be careful not to be fooled by the icing. So the unsupervised learning is also important. But supervised learning is where all the, in some sense, the jazz is. That's what people spend up spending a lot more time on. And we will also probably spend, in terms of lecture hours, more time on supervised learning than on unsupervised learning. But don't forget that the bread and butter work of machine learning actually rests on a strong foundation of unsupervised learning. Okay, so I'm going to start with something after this. So if there are any questions at this point, I'll just take a minute to pause and ask, then continue. Okay, you. Fine. So now let's start our first topic. So this is a topic which is actually of historical interest because it is how this whole idea about learning from data and data driven\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"continue. Okay, you. Fine. So now let's start our first topic. So this is a topic which is actually of historical interest because it is how this whole idea about learning from data and data driven decision making, as it's called, came into the forefront. So this is something which was originally suggested in the context of retail people who run supermarkets and department stores, that their various aspects of their business could be improved if they had information about this kind of thing. People who buy one product also tend to buy another product from one side. It could be in terms of deciding which products to focus on, could be deciding how to cross sell, as they say, how to design discounts so that people will buy more. It could also be something as mundane as how to organize the products in your shop. Where do you put. Now, if you go to any big shop, any reasonable size shop, you will find that similar products are in similar places. So that's for a reason that it's the same\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"products in your shop. Where do you put. Now, if you go to any big shop, any reasonable size shop, you will find that similar products are in similar places. So that's for a reason that it's the same way, the reason why library books are arranged, typically subject wise. So if you're looking for something and you don't find what you're looking for, then similar things will be nearby. But now, if people who buy X also buy Y, it may be that X and Y are not directly similar, but they are related in some functional way. So it's better to put them. So, for instance, if you go to, say, a supermarket, you might find that things like baking powder and things like that are kept close to sugar and chocolate and things which are used for baking. Now, these are not the same product, but they have similar uses. So this is one thing, of course, there is also a devious situation where if you know that everybody who's going to buy one will also buy the other, they can put them far way because then\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"uses. So this is one thing, of course, there is also a devious situation where if you know that everybody who's going to buy one will also buy the other, they can put them far way because then people are forced to walk through your shop because you know that they've bought baking powder, they're also going to buy something else related to baking, but they know they have to walk to the other end of the shop. But of course, more likely such people will not come to your shop. So it may backfire also. So you should be a little careful about how you do this. So there is a very legendary story related to this. So I just mentioning it because you will come across it in a lot of articles. So there's something about how in the 1990s, some shop discovered a correlation between diapers and beer. So it said that certain times, on certain days, people found that when men went to buy diapers, they also bought beer. A lot of speculation as to what this could mean. And the question is, of course,\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"So it said that certain times, on certain days, people found that when men went to buy diapers, they also bought beer. A lot of speculation as to what this could mean. And the question is, of course, whether it happened or not. So I would think you should go and read this reference which I've given here. So it actually gives a historical picture of this. But there are various accounts of this, whether it was true or not, whether it actually happened or somebody made it up. But it is certainly referred to in a lot of popular articles and even in some textbooks as to the motivation for doing this kind of market basket analysis. So the claim was that there was actually a shopping chain which discovered this and made use of this. Now, the more I would say, believable end of the story is that somebody found this out. There was no immediate way to make use of this fact.\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"Maybe it is true that at certain times, people who buy diapers also buy beer, but it at least gave some confidence to the idea that you can use this kind of information to make decisions. So from that perspective, it possibly had a role to play. But anyway, so these are the origins of this problem, and it's called market basket analysis because it's really talking in this context about shopping baskets. I have a shopping basket when I reach the counter to pay. So question is, what is in that basket? If it has x, does it also have y? So this is a very literal interpretation of markets and baskets. But the notion of markets and baskets can be a little bit more abstract. So, for instance, if you are thinking about, especially nowadays, given the number of elearning platforms, there's a lot of emphasis on trying to structure topics in such a way that students who find difficulty, you can identify why they are facing difficulties. So you want to think of these concepts that you're teaching\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"emphasis on trying to structure topics in such a way that students who find difficulty, you can identify why they are facing difficulties. So you want to think of these concepts that you're teaching as these items. And basket is now a set of concepts. And if you look at a concept which is difficult to learn, maybe there is a connection to another concept which is also difficult to learn. So you might want to group this. So people who misunderstand a also misunderstand b. And therefore maybe the problems has to start by fixing b. And another place where this is used is in kind of similarities of documents. So if two documents share words, then you can think they are similar. So you can think of documents. Or if two words appear rather in many documents, you can think they are similar. So if you say that wherever x appears, y also appears, then it may mean that x and y are actually related concepts. So this is the context. So the real question is this, people who buy x also tend to buy\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content='you say that wherever x appears, y also appears, then it may mean that x and y are actually related concepts. So this is the context. So the real question is this, people who buy x also tend to buy y. So this is what we want to formalize and try to do a preliminary round of calculations to see how one might determine. So everything, as we said, is going to be done by first abstracting out the problem into something which is a mathematical model. So here, abstractly, the items that are available in this hypothetical shop come from a set, capital I, and there are a large number of items. Let us assume. So. In this particular thing, we assume that n is large. So imagine a large supermarket. So n could be in tens of thousands, hundreds of thousands. So what is a transaction? A transaction is a set of items that somebody buys. So already, now there are some simplifications. So, first of all, it is a set. So if somebody buys two tubes of toothpaste, we are only going to count it as one', metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"is a set of items that somebody buys. So already, now there are some simplifications. So, first of all, it is a set. So if somebody buys two tubes of toothpaste, we are only going to count it as one item, namely toothpaste. Secondly, what constitutes an item also is up to us. It will depend on. So are we thinking of items in terms of generic things like toothpaste, toothbrush. Are we thinking in terms of brands like Colgate toothbrush, toothbrush of this type? And then are we thinking of, say, in toothpaste, are we distinguishing between 50 grams toothpaste and 100 grams toothpaste as two different items? So depending on the problem you're trying to solve, the nature of what constitutes an item will also change so many of these problems that we do in machine learning. First, you have to understand what it is you're trying to solve. You can't just blindly step in and solve some problem, because the level at which you're solving the problem may not be useful for the person who is facing\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"what it is you're trying to solve. You can't just blindly step in and solve some problem, because the level at which you're solving the problem may not be useful for the person who is facing the problem. So we have to decide what is an item. And then we have made the simplifying assumption that a transaction, we are only going to look at distinct items in the transaction. So we are looking at the shopping basket, we are only looking at different items. We're not looking at multiplicities of items. So now, over a period of time, we collect information about these transactions. So we have a set of transactions, some m transactions, and again, we can assume that M is large. And now we are going to base this as our starting point to kind of figure out this. People who buy X also buy y, so that we now need to formulate. So we want a kind of association. So people who buy x also by Y says that these are, first of all items. So in general, now these are sets of items, not individual items.\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"we now need to formulate. So we want a kind of association. So people who buy x also by Y says that these are, first of all items. So in general, now these are sets of items, not individual items. And very often in this area, these are called item sets. The question about m, no, m is not the number of items in a basket. M is the total number of baskets. So each Ti is a subset. So each Ti is a basket. And I observe many customers. I'm trying to generalize the data from a large number of such customers. So each customer buys a basket, and that is one subset of the items. And I'm looking at a large number of such subsets in order to determine whether or not a kind of observation about x connects to y holds or not.\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"So now, yeah, so what I'm saying is that it's not necessarily that only, like people who buy toothbrush also buy toothpaste. It could be something like people who buy toothbrush and toothpaste also buy soap and shampoo, right? So there could be sets on both sides. But these sets, if they are sets, even if they are items, it doesn't make sense to say, people who buy toothpaste also buy toothpaste. Right? So if they're individual items, certainly you would expect x and y to be different items. And if they are sets, you don't want them to have the same element on both sides. You don't want to say, people who buy toothpaste and soap also buy toothpaste and toothbrush and soap. It doesn't make sense. So X and Y can be assumed to be disjoint. And what we want to formalize is the idea that whenever the items in x are in some particular transaction TJ, then it is likely. So what does likely mean that y is this is what we want to do. So the association rule captures this intuition that I have\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"the items in x are in some particular transaction TJ, then it is likely. So what does likely mean that y is this is what we want to do. So the association rule captures this intuition that I have two sets which are disjoint, right? X and y two sets of items. And whenever I see all the items in X in a transaction, I am likely to also see the transaction from Y. Remember, X and Y are two separate sets, so they are two different things which come into the basket. So I need to talk about this likely. Now, clearly, likely is something which I must apply what I think is parameter for likely. It's not something which is universal. So I must give some kind of a ratio. So how often does it happen as a fraction? Right. How many times when I see x, do I see y? That's the first. So this is just saying, what is the frequency of doing this? So is it that 10% of the time when I see x, I see y? Is it 1%? What am I interested in? Am I interested in something which occurs? 10%, 20%, 1%, half percent? I\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"what is the frequency of doing this? So is it that 10% of the time when I see x, I see y? Is it 1%? What am I interested in? Am I interested in something which occurs? 10%, 20%, 1%, half percent? I have to give a threshold. And if I give that threshold, then with respect to that threshold, there will be certain x and certain y. The threshold is lower. There will be more x, which satisfies the threshold, because I'm asking something weaker if it is a higher threshold, if I expect 50% of the time, people who buy x by y, then there'll be fewer such x and y. Now, the other part of it is whether this is worth knowing at all. So, supposing I tell you that people who buy Rollsroyce often buy leather seat covers. Now this may indeed be an extremely significant thing in terms of percentages. That is, maybe 90% of people who buy Rollsroyce actually put leather seat covers on their car. But as far as the person selling cars, Rollsroyce are so rare and expensive that it doesn't have any\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"That is, maybe 90% of people who buy Rollsroyce actually put leather seat covers on their car. But as far as the person selling cars, Rollsroyce are so rare and expensive that it doesn't have any meaningful implications in terms of what you can exploit. So it is a negligible sample in some sense, of the total set of cars which are sold. So any observation I make about Rollsroyce may be just due to the fact that both are expensive items and people who are buying an expensive car are willing to put expensive seat covers and may not have any other indication. So it may not even be worth exploiting. So I need to talk about two thresholds, right? That is, is this pattern worth knowing at all? And if it is worth knowing, is it a pattern at all? How frequently does it happen? So, as I said, these are going to be percentages or frequencies or whatever. So I need to essentially count things. So if I take any subset of items, I can tell you how many times it occurs as a whole in my transaction.\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"are going to be percentages or frequencies or whatever. So I need to essentially count things. So if I take any subset of items, I can tell you how many times it occurs as a whole in my transaction. So, this is the set of all transactions such that z is entirely there in that transaction. And I want to know how many of them are there. And that is my Z dot count. Remember that m is the total. So z dot count is going to be less than or equal to n. So z is a set of items of. For example, I look at the end of the day or at the end of the month at a supermarket, and I ask you, how many transactions during this month did the person buy toothpaste and toothbrush that's what? Z out of the total. It's not a ratio. It's a total. It's a count. So what do I want to know now? I want to know as a percentage, how often does x imply y? So if I buy X and Y, then I buy x union y. So it's union because these are disjoint. So if my shopping basket has both x and y, there is a tendency to think in terms\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"how often does x imply y? So if I buy X and Y, then I buy x union y. So it's union because these are disjoint. So if my shopping basket has both x and y, there is a tendency to think in terms of intersection. That's why I mentioned it's not that x is there and Y is there. So X intersection. Y is there. So X is a set. Y is a set. But they are disjoint sets. And I want to see both of them. So this is really a disjoint unit. So I want to see baskets which have both x and Y and compare them to baskets which have only x. So these are situations where. So, of course, everything which has x and Y also has x. But there are baskets which have x and Y does not have Y has only x. And I want to know how many of those are there. So that's what this is saying. So we want that the number of baskets which have both x and Y as a ratio of the number of baskets which has x should be above some threshold that we specify. So this is, again, something. So this could be 1%, 10%, 5%, 0%, whatever. So it\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"which have both x and Y as a ratio of the number of baskets which has x should be above some threshold that we specify. So this is, again, something. So this could be 1%, 10%, 5%, 0%, whatever. So it could be zero, 1.1.3. It's going to be a fraction.\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content='This is going to be less than one for sure. Less than equal to one at best. Every time people buy x, they buy y. So x union y will appear as many times as x, but cannot appear more times than x because every time x union y appears in the basket, x alone also appears in the basket. Right. If I buy toothbrush and toothpaste, I am definitely going to see a toothbrush. So if I want to know whether toothbrush implies toothpaste, then I look for toothbrush and toothpaste on the top, and I look for just toothbrush at the bottom. But everything which has toothbrush and toothpaste implicitly has a toothbrush. So therefore, the top is going to be no bigger than the bottom. So this ratio is going to be somewhere between zero and one. And I can decide what, for me is a level which I deem to be interesting. And this is called confidence. So I fix this chi. So this is, this greek letter chi looks like an x. So greek letter chi. So I fix a confidence level, and I ask for some x and some y whether', metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"And this is called confidence. So I fix this chi. So this is, this greek letter chi looks like an x. So greek letter chi. So I fix a confidence level, and I ask for some x and some y whether this is true. The other part was that whether it's worth doing at all and this worth doing at all is what is called support. So it's like that Rollsroyce and the leather seats. How many of my cars that I actually sold were Rolls Royces with leather seats, because I don't see enough Rolls Royces with leather seats. Then this whole correlation that I'm discovering is not of much use. So here I'm taking the right hand side of my rule. So I have a rule of this one. So, I'm looking at things where this is true, where x and Y are both there, and saying, how many times is it true as a fraction of the total. Remember, m is my total. And I want this again, at most can be one. It could be that every transaction has x union y, but it can't be more than M. So this is again, number between zero and one. And\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"Remember, m is my total. And I want this again, at most can be one. It could be that every transaction has x union y, but it can't be more than M. So this is again, number between zero and one. And now I want to know whether this is above a certain threshold. So again, these are two independent. If I see at least 1% transactions which have this, then I will say so that is independent of the confidence. So, confidence tells me whether the rule is valid or not. Is it true that x implies y? And support tells me whether that rule is interesting or not. I mean, is it worth my while to actually pay attention to this correlation? So now, the interesting thing about this problem is that if I fix these things, if I fix the items, and if I fix the transactions, and if I fix these two thresholds, then for every x and every y, either this statement is true or it is false, right? Either these two inequalities hold or one of them fails. So there is a kind of correct answer in some sense. So this\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"then for every x and every y, either this statement is true or it is false, right? Either these two inequalities hold or one of them fails. So there is a kind of correct answer in some sense. So this gives me a fixed set of valid exercises. So there is a solution, which I am trying to find, which is fixed once I fix the basic data. So the basic data is only the items on the transactions, but my parameters, which are user defined parameters, this chi and sigma. Once I fix those, then the answer is no. So this is not, therefore, in the typical sense of a machine learning problem. Because usually in a machine learning problem, the answer is not known. If the answer were known, then you would not need machine learning. If you knew exactly how to tell whether somebody is going to pass their exam. You don't need to build a model, you just use whatever deterministic algorithm they have. Or if you know whether or not this loan is going to default, then you directly decide. The problem is that\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"You don't need to build a model, you just use whatever deterministic algorithm they have. Or if you know whether or not this loan is going to default, then you directly decide. The problem is that those things, there is no fixed way of knowing the answer. So it's also difficult, therefore, to tell whether the answer you are predicting is right or wrong. So here it's not yet, first of all, a truly learning problem. It's more an algorithmic problem, but still, it's an interesting problem nonetheless, because I just want to illustrate how it affects the way in which we calculate what might be trivial with small data becomes nontrivial with large data. So this is the problem. So given a set of items, capital n, which is large, and a, given a set of transactions m, which is again large, and given these two ratios between zero and one, find every pair x and y such that x implies y is a valid association. So we can break up this thing into two steps. I mean, we want to first check whether x\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"these two ratios between zero and one, find every pair x and y such that x implies y is a valid association. So we can break up this thing into two steps. I mean, we want to first check whether x implies y is worth looking at at all. So we first look at the support part. We want to know whether x, sorry, yeah, we want to know whether the count divided by m is bigger than the support, which is the same as taking this m to the other side and saying whether the count is bigger than a certain fraction of the total. So the first idea is to identify those sets whose count is frequent enough. So these are what are called frequent item sets. Find every z such that z dot count is bigger than or equal to sigma times n, where sigma, remember, is something that we provide. But then we want to know which sets occur at least this many times. So this is the first step. Then after this, we have to then decide whether z breaks up, as after this, we have to decide. That's so once we know that z is\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"which sets occur at least this many times. So this is the first step. Then after this, we have to then decide whether z breaks up, as after this, we have to decide. That's so once we know that z is worth looking at, then we have to check whether I can break it up into x and y and get a root. So the first problem we are going to look at is just this one. How do you determine all the z whose count is at least this much?\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"So how do you do this if I ask you to? For instance, if I'm giving you a stream of numbers, you have seen this kind of thing, which comes sometimes on the news, on tv or something. You see the stock market coming at the bottom, right? You have a row of numbers going past you. So supposing like that, I give you a row of numbers, and I ask you at the end to tell me how many times each number appear. So assuming that you can record it fast enough, one thing you can do is you can maintain a counter for each number that you see. Each time you see a seven, you say, there's one more seven. So you can keep, if you want to do it manually. This is how votes are counted, for example, by hand. So I'll keep 123456. And so on. And then every time I see, I do this, this and I cut it off, this cut it off, and so on, right? So every five. So I can just keep a count. So I have one count per value that I'm tracking. I'm maintaining a kind of frequency count. So think of it as a large list or an array or\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"and so on, right? So every five. So I can just keep a count. So I have one count per value that I'm tracking. I'm maintaining a kind of frequency count. So think of it as a large list or an array or a dictionary if you want to prefer that. But if it's a list or an array, then essentially you need to have one slot for every entry which could potentially come into your system. And then you need to maintain this counter. So for every z you maintain a counter. So what you do is for each transaction, you go through your transactions, and then for every potential subset that you see in that transaction, you increment account. So this is a standard way of doing this. And then after all the transactions are done, you go through all these counters and keep all those z's whose counter value is larger. So this is straightforward. So what is the catch? Here's anybody take a lot of time. Firstly, what is going to take a lot of time? Counting all the values till all the values are completed. And\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"So this is straightforward. So what is the catch? Here's anybody take a lot of time. Firstly, what is going to take a lot of time? Counting all the values till all the values are completed. And then we will, I don't know, there will be a lot of, in terms of time, what do you have? Right, so you have a loop which runs over all the transactions. So this is of size n, right. And then within that you have to go through. So we have to assume. So let us assume that each transaction is bounded by some size. So right now we are not looking at the rule part, we are not looking at the decomposition, right. We are only looking at counting which sets as a whole appear sigma m times. So this will be some m times. And what is this? Now if I have each transaction as small m items, then I have in each transaction, I have to look at each subset of it and increment the counter. So there'll be two to the m subset. So it will be m times two to the m. Now this may or may not be large. It depends. So that\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"I have to look at each subset of it and increment the counter. So there'll be two to the m subset. So it will be m times two to the m. Now this may or may not be large. It depends. So that is one valid concern. But somebody else pointed out in the chat that the other problem is that we have to do this for each z. So z is a subset of subset of I and I is of size n. So we have two to the n potential subsets. That means our counter space, our Dictionary, or our array or whatever we are keeping potentially has to be enormous. It has to be exponential in the number of items. And remember, the number of items is large. So this might be, I mean, if you look, imagine that it's a shopping situation. This might be small. This might be like 1020 kind of. But this is going to be like ten to the power six or ten to the power seven or something like that. So two to the ten to the power six as opposed to two to the ten. See, two to the ten is like thousand, which is not a huge. So the real problem\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"six or ten to the power seven or something like that. So two to the ten to the power six as opposed to two to the ten. See, two to the ten is like thousand, which is not a huge. So the real problem is space rather than time, although of course impacts time also. But the real problem is space here. So the real problem is we have to maintain these two to the number of items counters. And this is in general going to be infeasible. So the real problem is that we don't have enough storage space to keep this count. And if we cannot count this, then we cannot of course apply that criterion. So therefore we cannot check whether a rule is worth exploring or not. So the second problem is still there. We'll come to that separately, which is having found these z's, how to decompose them as x and y. So at the moment, this is where I'll stop for today. So think about this. So we want to somehow do this loop. We want to do this loop, but we cannot afford to keep one counter for every set. So this is\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"at the moment, this is where I'll stop for today. So think about this. So we want to somehow do this loop. We want to do this loop, but we cannot afford to keep one counter for every set. So this is what we will do next time. So I'll stop with this. Any questions at this point? I had a question regarding the example that you gave that if there is somebody who buys toll stars, there's a 90% chance. Can you just say that a little loudly? I can't hear you, sir. I had a doubt regarding the example that you gave where you said that the people who buy Rollsroyce, about 90% of them would end up buying a leather. In that example, when we are calculating, say we are calculating m, what would m be in that case? Like when we say x union y. So x would be rollsroyce and y would be leather skates. But what would be. So here now, n would be the total number of whatever par. These are our items, right? This is the size of the set of items. So these are the items that people are buying and say across\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content='what would be. So here now, n would be the total number of whatever par. These are our items, right? This is the size of the set of items. So these are the items that people are buying and say across different car showrooms.', metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"So they are buying car, but they're also buying these kind of things. They are buying fancy seat covers. They are buying maybe some whatever reflective film for the glass. They are buying some fancy number plate, whatever it is. So this is n is the total number of items which people are buying. Capital MD. And then capital m will be in this particular case, we are going to see across all these shops a number of people buying cars. And along with the cars they buy some other stuff. So we are looking at a large number of sales of cars, across different types of cars and across different dealers and across different customers. So the point will not include the n for nose. That will not include just Rollsroyce. It would include all categories of. Yes, exactly. That's the assumption. Right. So if I'm only looking at Rollsroyce sales, then it's fine. But if you're looking at car sales across a large fraction of cars, then these higher end, very expensive cars will occupy a very small\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"So if I'm only looking at Rollsroyce sales, then it's fine. But if you're looking at car sales across a large fraction of cars, then these higher end, very expensive cars will occupy a very small fraction of that spectrum. So it will not make sense to base any policy on this kind of. So in a sense, it's like an outlier we were talking about earlier. You would not base your policy on taxation based on the income of the extremely rich people. You will be looking more at middle. So, sir, in case that. Can we select our n and M based on the model we want? Like in case, for example, if we have something like a factory of somebody who makes car seats is trying to decide what kind of seats he should manufacture, what kind of covers. Sorry. He should manufacture for Rollsroyce. So for him it would be useful. Yeah, absolutely. So there is a context. Yeah, it depends on what is your setting in terms of items and transactions. So if you're restricting your setting to only, for example, high end\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"be useful. Yeah, absolutely. So there is a context. Yeah, it depends on what is your setting in terms of items and transactions. So if you're restricting your setting to only, for example, high end cars or even only Rolls Royces, then the problem changes. Then of course, as a fraction, the support will go. That's not. No doubt about. So there's really an assumption about what data you are originally starting with. So if you're starting with the full spectrum of data, then you will have this conclusively. We can decide n and M based on what our problem statement is. Well, see, the problem I have phrased now is independent of interpretation. See, that is the whole point of this mathematical thing. I am assuming that somebody has given me a set of items and they've given me some set of transactions. I am not asking you what these transactions mean or what those items mean. So, n and M now for me are just input parameters to my problem. And these are just abstract items. I one, I two. And\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"I am not asking you what these transactions mean or what those items mean. So, n and M now for me are just input parameters to my problem. And these are just abstract items. I one, I two. And each transaction is some I seven, I 13, I 15 and so on. But if you are setting up now to solve the problem in a concrete setting, you have to decide what is the set capital a and what is the set capital t. So in that sense it will come. But I'm saying the problem we are trying to solve is not specific to rollsroyce or diapers or whatever. Toothbrush or toothpaste or whatever. It doesn't matter. Yes, sir. I was asking for that specific example. So therefore, I agree that in order to claim something holds or does not hold for a specific example, you have to be precise about what exactly are the interpretations of these values in that example. So unless you specify that, you cannot say that this is unlikely to be true or this is likely to be true, because it will be specific to the interpretation.\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"of these values in that example. So unless you specify that, you cannot say that this is unlikely to be true or this is likely to be true, because it will be specific to the interpretation. But the problem now, we have now reduced it to one which is uninterpreted. We are just talking about abstract items. So there was a question in the chat about dimensionality reduction connected to PCA. So, yes, the PCA will be one part of it, but there are also other things that we will see. Sir, I have a question. Yeah. When we model this problem in this way, we didn't capture the frequency of items in a transaction. We were just looking at whether or not that item is present or not. Yeah, but I think for most of these examples, whether someone's buying from a shop or whether it's words in a document, that frequency will be a part of it. Right. Which we are not capturing here. Yeah. So usually this is how mathematical models work. You start with the most basic model, and then you add any features\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"that frequency will be a part of it. Right. Which we are not capturing here. Yeah. So usually this is how mathematical models work. You start with the most basic model, and then you add any features which you think are relevant. You can add them and then see what you need to do to improve the solution, to take that to account. So right now we are solving it in this set situation where the frequency is not counted. But it is true that you might want to do this. There are other things which you will look at later a little bit, and superficially at least. But for saying even this notion of what is interesting is somehow dependent on the items, because if something is being bought on a daily basis, then it might appear in a large fraction of transactions. That doesn't necessarily make it more interesting than something which is bought of higher value. So I'm saying that going back to something, I mean, shops might have different thresholds of interestingness for something like a pressure\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"interesting than something which is bought of higher value. So I'm saying that going back to something, I mean, shops might have different thresholds of interestingness for something like a pressure cooker compared to salt. Okay. The idea that there is a single notion, chi, is also a kind of oversimplification. Similarly for sigma, the fact that a threshold of transactions must appear before you look. The threshold might be much lower, again, for expensive items than for frequent items. So there are lots of variations that you can ask on this problem, and I will mention some, but we will not spend too much time on that. We will just try to solve this basic problem and then learn from it and vote. Right. There was some other question, one secondary. Any other questions? Sir, I have a question. Yeah, so the for loop we are running over here, basically, we are first fixing the z and then we are looking at each transaction, and then we are running the loop on all the subsets of the\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"question. Yeah, so the for loop we are running over here, basically, we are first fixing the z and then we are looking at each transaction, and then we are running the loop on all the subsets of the transaction, right? Because I'm getting confused between these two. Are they same or are they different? You can do it in two different ways. So what I am saying is that I'm overlapping the thing, right? So I'm saying for each p, this is what I'm doing for each transaction, for every subset. But now I can basically change this and say for each, because obviously I don't have to look at z, which is not mentioned in this particular transaction. So instead of looking at every possible subset z, I can look for only those subsets which are there in that transaction, because everything else does not get incremented. Now, what you are asking is the other way around. And this is potentially, I mean, superficially, they are the same thing. You have two loops and you are interchanging the order. But\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"get incremented. Now, what you are asking is the other way around. And this is potentially, I mean, superficially, they are the same thing. You have two loops and you are interchanging the order. But this is potentially much more expensive because there will be a lot of items, subsets which never occur. I mean, for instance, you might be looking for subsets where people are buying two items which are totally of different types of categories, and they might never buy these together. Like for example, a refrigerator and toothpaste, maybe. I don't know if a shop is selling both. Now, in principle, you are looking at every possible subset, and if you're looking at every possible subset, there'll be all kinds of implausible subsets also, which you will count, and you will only discover by looking through all the transactions, they don't. So in some sense, the left hand side loop is more pragmatic because it's only going to. So if you think about it in terms of dictionaries, for instance.\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"through all the transactions, they don't. So in some sense, the left hand side loop is more pragmatic because it's only going to. So if you think about it in terms of dictionaries, for instance. So don't think of it in terms of array in a dictionary. You will create a counter for a subset only if you see it in your transaction. Whereas on the right hand side, you are actually creating a subset for everything which could potentially occur, whether or not that appears anywhere in your transaction. So therefore, the left hand side loop, which is what I've kind of written here, is more pragmatic because you're only going to keep track of those z which appear in at least one transaction. You're never going to look at a z which never appears. Whereas if you start by looking at every possible z and putting it outside, then you have a problem. Is that clarify? Yes. Thank you. Okay, so next time we will look at this question and see how to do this calculation more effectively. So, Thursday,\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"z and putting it outside, then you have a problem. Is that clarify? Yes. Thank you. Okay, so next time we will look at this question and see how to do this calculation more effectively. So, Thursday, see you. There's.\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"So last time we started with supervised learning in earnest, and we were looking at this decision tree model. So, in a decision tree model, you have essentially a table in which the columns are the. So each item, a row in the table and one special column denotes the class that you are trying to classify. And we assumed it's a binary classification. So the class is zero, one, or yes, no, or whatever you want to interpret. The two categories are. So the algorithm was very simple. At a high level, what you want to do is choose an attribute based on the answer to the question that you're asking about that attribute. The data splits, and then you take each partition, ask another question. So that's the thing. And you want to come down and keep asking questions until either you have reached, like these situations here, a partition in which all the values of the class variable are the same. So you have a uniform class level or you have run out of questions, because we can only ask in any path\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"these situations here, a partition in which all the values of the class variable are the same. So you have a uniform class level or you have run out of questions, because we can only ask in any path one question per attribute. And if in case we have the second case where we run out of questions, but we have not reached a so called uniform node, then we just use the majority as a prediction. So we will see an example of this later as we go. Yeah, so what we also said was that, as we can see here, we have two different trees, and the trees are not the same shape. And we argued that a smaller tree is probably a more desirable one. It's more explainable. And we also claim, without at the moment justifying, that it also has a better generalization property. That is, it will move from the specific training data to the unknown data in a better way. So how do we build small trees? Well, computationally, we said, I claimed at least that it was infeasible to build the smallest tree. So you\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content='training data to the unknown data in a better way. So how do we build small trees? Well, computationally, we said, I claimed at least that it was infeasible to build the smallest tree. So you would have to do a search through all possible trees. So instead we wanted to use some heuristic. So the main question that we have to ask is, which attribute do we query next at any stage. So our goal is to achieve this partition, which is pure, where all the values are yes or all the values are no. So we try to accelerate towards that. So that is this greedy strategy. So we use a heuristic, which will reduce the impurity as much as possible. And when we move from one node to its children by asking a question, what we do is we compute a weighted average. So for each node we can compute the impurity. But then when we combine the children of a node to get the impurity at the next level, we use a weighted average. So going simple weighted average just assigns a weight one to every item. So you just', metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"But then when we combine the children of a node to get the impurity at the next level, we use a weighted average. So going simple weighted average just assigns a weight one to every item. So you just take the fraction. So for node one, you take the fraction of instances in that partition, multiply that fraction by the impurity and so on, and add it up, and we choose the one which reduces. So what we said was that this impurity function, if we just take misclassification as our impurity function, then we get this linear behavior, and it turns out empirically that we can do better. And so we suggested that there are two natural functions which are borrowed from other fields. So there's entropy, which is used in information theory, which talks about, in some sense, randomness. So the maximum randomness happens, as you would expect, when both possibilities are equally likely. And the maximum order or minimum randomness happens at the extremes when only one possibility is there. But the in\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content='randomness happens, as you would expect, when both possibilities are equally likely. And the maximum order or minimum randomness happens at the extremes when only one possibility is there. But the in between thing follows a much sharper curve. Actually, entropy goes like this between zero and one, but because misclassification only goes to zero to half, this scaled down by a factor of two. So the red line is a scaled entropy, where you just divide the entropy by two. So that is one thing which comes from information theory. And the other way of comparing these two fractions, that is the fraction of zeros and the fraction of ones, is using this thing called genie index, which is used in economics to measure inequality of, say, distribution of wealth, how much a fraction of wealth is distributed among the population. And there, again, the maximum kind of genie index happens when you have half. So here it is, this green line. So this green dashed line is the genie index. So the genie', metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content='among the population. And there, again, the maximum kind of genie index happens when you have half. So here it is, this green line. So this green dashed line is the genie index. So the genie index fits actually under the entropy. So, entropy, our argument is that the steeper the curve, that is, the faster the thing goes away from zero when we get impurity, the better it is empirically. So, in a sense, entropy is technically better than gini index, but because of the fact that entropy requires us to calculate logs, whereas Gini index is just simply in terms of multiplication, it turns out gini index is easier to work with. And actually, as we will see, the packages which are deployed in languages like the genie index, in order to. So I just want to emphasize. Yeah, sir. Like we are seeing in Grady, rusty case also. And that even if these are linear, then at every node they are reducing the impurity level, right? Yes. So the same thing is doing either gene index or the entropy. Both are', metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content='Grady, rusty case also. And that even if these are linear, then at every node they are reducing the impurity level, right? Yes. So the same thing is doing either gene index or the entropy. Both are also doing the same thing. Correct.', metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"So why we are considering that and not the greedy heuristic? Yeah. So you're saying that if I'm going to compare, then the comparison will be the same in all three cases, right? That's what you are saying right, sir, and also Thansen are also behaving in the same way, first increasing, then decreasing. So it will not. I think there are two answers. So you are right in one sense, which is that if I actually have to only compare one choice versus another choice, then because they are monotonic in the same direction, the choice will look better whichever index I choose. So that is a valid question. So really it's more to do with actually stopping. So basically, if you put a threshold for stopping based on the impurity, then the threshold will be different at this end, right? So if I say that this is my threshold, then what is the corresponding impurity level at which that threshold is achieved? So I think that will be probably the reason why the sharper entropy or genie index does better\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"this is my threshold, then what is the corresponding impurity level at which that threshold is achieved? So I think that will be probably the reason why the sharper entropy or genie index does better than the sir. But even we see this as a computationally, then I think that greedy heroistic will be much better. No, these are all greedy. Greedy is only referring to the fact that we are. But linear will be better. It's not a part of the choice. The chasing is which value are we using to make the greedy choice. So that's the point. Yeah. So I guess linear will be better, even computationally. Right? Well, this kind of computation doesn't take much effort, right? Instead of comparing two values, you're squaring it and adding it. And so arithmetic operations hardly make any. See, log is a more expensive calculation. So that is the point. Take logs of small values. So logs of small values are also painful to calculate. So that's one reason you don't want to take logs. But otherwise, if I'm\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"expensive calculation. So that is the point. Take logs of small values. So logs of small values are also painful to calculate. So that's one reason you don't want to take logs. But otherwise, if I'm just doing multiplication and addition, it doesn't really cost much. Okay, so in the case of genie index, we are achieving a stopping criteria much earlier, right? That's the case, yeah. What we are saying here is that we are actually going to, we say that we are going to go until we either use up all the attributes or we reach a uniform class. Now, this is a reasonable thing to do if the number of attributes is small. But if you have a very large number of attributes, then it may not be really practical, and you may not want to ask 100 questions. So you may want to stop when the incremental gain of asking one more question is not much. So now that not much will now depend on the actual gain. So that's where the value will act. It's now an absolute question. Right? I have so much impurity\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"of asking one more question is not much. So now that not much will now depend on the actual gain. So that's where the value will act. It's now an absolute question. Right? I have so much impurity now. And I have so much impurity after I ask this question. How much is the gain? So then it will matter what your impurity. Whether what you're saying is correct, that if I'm asking the difference between asking attribute one and attribute five, then the difference will be better in all notions. But if I'm asking you whether asking any question asking at this point, then it will depend on which measure I'm doing. So the actual improvement is an absolute value, and in that case, the choice of the value matters. Whereas if it's a relative value, is this value better than that value? Then all of them, I agree, will be the same. Okay, so one thing to emphasize is that I think in either the previous class or before, there was this question about whether interpreting the answers will help build a\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"I agree, will be the same. Okay, so one thing to emphasize is that I think in either the previous class or before, there was this question about whether interpreting the answers will help build a model. So if we know what these attributes mean, then can we build a better model? And I said that, of course, in practice that would be true. That if you know what attributes mean, you might focus on some attributes. You might, for example, if it is, say, some medical diagnosis, then you may not be very concerned about the date of birth. I mean the year of birth, yes, but you may not need to know whether it's January 27 or May 30. It may not make much difference. So there are some things which may be less or more relevant, or even place of birth may not be so relevant, and so on. So you might automatically throw out some. But in general, we don't know. So in general, we are running all these things. So the algorithm works just looking at the values without asking what does a one mean and\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"automatically throw out some. But in general, we don't know. So in general, we are running all these things. So the algorithm works just looking at the values without asking what does a one mean and what does a five mean. So this is something to keep in mind. So our strategy is to maximize the reduction in the impurity. And this terminology comes from that entropy interpretation. So in entropy information theory, entropy is loss of information. If you have more entropy, you have less information. If you have more information, your entropy reduces. Right. I discussed that if you know the message for certain, you have full information about what the message contains in advance. You don't even need to send the contents, you just need to say there is a message, like a missed call, and you would know what the message means. So, information is entropy going down is the same as information going up. So this is sometimes called maximizing. Reduction in entropy is sometimes also referred to as\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content='know what the message means. So, information is entropy going down is the same as information going up. So this is sometimes called maximizing. Reduction in entropy is sometimes also referred to as maximizing the information gain. But because this is an uninterpreted kind of algorithm, that is, we are just looking at the attributes and comparing the information gain without asking what the attributes really signify, we are just taking the one that works best. So the problem is that there could be some attributes which give information gain but have no significance. And a typical example of this is something which is like a serial number, right? So imagine that you have something like Aadhaar number or passport number, or in a class, you have distinct roll numbers. Every student has a different role number.', metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"So now, obviously, if I ask a question on that column, so the column has behavior like this, then what will happen is that for each, each answer, there will only be one person. There will only be one person with a given passport number and so on. So if I only have a partition of size one, then that one item is either yes or it's no. So there's no ratio to worry about. So the minority is always zero because there is only one item. So I have actually split my. So this is an extreme case where I've split my data into singleton partitions. So what I've done is I've taken some initial entropy or some initial information, theoretic notion of misclassification, and I've reduced it to zero weighted sum. But it's a weighted sum of zero. Right? Every branch has no impurity. So I get these partitions of size one. Each partition is guaranteed to be pure because it has only one item and the new impurity is zero. So I cannot do better than this. So such a question would then be picked up. And\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"of size one. Each partition is guaranteed to be pure because it has only one item and the new impurity is zero. So I cannot do better than this. So such a question would then be picked up. And remember, this reason that the question is being picked up is because we have not interpreted. So you might ask, okay, why passport number? But it could be something indirect, like, it could be some kind of an encoded timestamp. It could be some actions which are recorded in some system, and then one of the columns records somehow the timestamp, and no two events happen with the same timestamp. So there could be any number of hidden reasons why something is actually unique. And either you have to clean up the data and check it, or you have to build a safeguard against this happening. It may or may not happen, but I just wanted to bring this up because this is a byproduct of the algorithm that we have now. It's a shortcoming. It's a shortcoming in the sense that if we apply the algorithm that we\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"happen, but I just wanted to bring this up because this is a byproduct of the algorithm that we have now. It's a shortcoming. It's a shortcoming in the sense that if we apply the algorithm that we have now, right now, in such a situation, blindly, then we will end up with this kind of curious choice right up front. It will tell us that the only question you need to ask is, say, the passport number of the patient, and then you will be able to predict whether they have a disease or not, which is totally nonsensical. Right? So the greedy algorithm will say, take it, but from an interpretation point of view, it's a totally useless choice. I'm not saying that this will happen. I'm just saying that if you provide an algorithm in any machine learning situation, you have to be aware of the fact that this algorithm is going to be applied in general blindly to the data. So it has to be driven by the data. So if the data can produce some peculiar behavior, you have to be able to tailor your\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"that this algorithm is going to be applied in general blindly to the data. So it has to be driven by the data. So if the data can produce some peculiar behavior, you have to be able to tailor your algorithm to take care of this peculiar behavior. The question is, how would we adapt our algorithm to fix this? So right now, our tree building algorithm blindly picks the attribute that maximizes the information gain. So we need to somehow penalize those attributes which have a wide number of possibilities. Now this is also intuitively clear. So if I ask a question to which there are 25 different answers possible, and I try to now make a decision based on the 25 different answers, and I have a different question which has only two or three answers, it's more sensible to make a choice based on the two or three answers. So the splitting into a larger fraction of number of pieces of smaller size is intuitively not the best way to design a good decision tree rule. Ultimately, these are rules,\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"the two or three answers. So the splitting into a larger fraction of number of pieces of smaller size is intuitively not the best way to design a good decision tree rule. Ultimately, these are rules, right? So I can think of a decision tree as saying that if the value is this and the value if I walked on a path, right? Saying if the person is young and the person does not have a job, then the person should not get a loan. So I can interpret every branch in this tree as a rule in that sense. So the rules become less interpretable if the choices split like this. If it says if the passport number of this person is r 347852, then do something. If it is t nine, four, eight, it doesn't make any sense, right? So this natural way to do this is to actually extend this notion of information or randomness or entropy to the attributes themselves. So let me take a typical attribute. It might take some k different values. So the question is, how random is this attribute? So this k values, now some\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"or entropy to the attributes themselves. So let me take a typical attribute. It might take some k different values. So the question is, how random is this attribute? So this k values, now some frequency, n one, n two. So each value appears in my data. This is all with respect to the training data that I have. So each value vi or vj appears ni or nj times. So I can again interpret these ratios as probabilities and say that the probability of this attribute taking bi is ni divided by n. So n is the number of rows. So each attribute, we are assuming that these tables don't have the separate issue about what happens if you have missing values in the table and object. So we are assuming for the moment that we don't have anything. So all these tables are, the data is sort of cleaned up to the extent that there are no missing values. And so on. So if there are n rows, this attribute will appear n times and what fraction of them take the value vi. So that is PI. So then I can apply my usual\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"extent that there are no missing values. And so on. So if there are n rows, this attribute will appear n times and what fraction of them take the value vi. So that is PI. So then I can apply my usual things. So this entropy that we calculated was a binary entropy, right? Because it's a two category situation, we said take the fraction of zero answer and take the fraction of one answer and say p zero log p zero plus p one log p one, and add the minus sign. So in general, if I have k answers, I just take p I log p I and sum it up over the k and take a minus. So this is the generalized version of entropy where I have more than two choices. Of course, remember this, PiS add up to one, right? So because this is a ratio of these things, so I am really somehow thinking of them as being like probabilities. So this would be the entropy, and there is a corresponding version for gini. So remember the earlier on was one minus p zero squared plus p one squared. So now I'm just saying add up all\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"So this would be the entropy, and there is a corresponding version for gini. So remember the earlier on was one minus p zero squared plus p one squared. So now I'm just saying add up all the PI squareds, again, assuming that the summation of the PI squared. So in this way, I can apply the same criterion that I'm using to discuss the class randomness in this group of rows that I have. What is the split between yes and no? So that is, at the category level, I'm looking at the randomness of the entropy, and I can look at an attribute and ask, in this group of rows, what is the randomness of this attribute? And what we are saying is that we want to penalize those attributes which are scattered, which have lots of values. In the worst case, they have this one value per one item per value, like a passport number or something.\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"So if I take this extreme case that each of these values occurs exactly once, then each of the correspondence, remember that p I divided by so vi is ni divided by Ni is number of times. So each vi happens exactly once. So I have one by n. So if I have one by n, then the entropy, for instance, each of the PiS is one by n, so I'll have k is equal to n. So this is the summation one to n, one by n, log one by n. And if you work this out, because this n comes here, so it's n times one by n, and then log of one by n is the same as one minus log n. So this is minus log n. So if you work it all out, it turns out to be log of n. So what is log of n? Log of n is a slow growing function, but it's a growing log of n will keep increasing as n increases. So if you have something like this, if you have more passport numbers, you will have higher entry. If you have more values, you will have higher entry. And you can do the same calculation for genie index also. So if it is one by n and you add it\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"more passport numbers, you will have higher entry. If you have more values, you will have higher entry. And you can do the same calculation for genie index also. So if it is one by n and you add it up, then it's n into n minus one by two. So you're saying that, for example, if there are five possibilities, it's going to be four, there are six possibilities, it's going to be five by six, and this is six by seven, and so on. And these fractions are actually increasing in value, right? If you go n minus one by n, as you get n larger, the ratio becomes closer to one. So in both these cases, what we are seeing is that if you have a larger. So this is the extreme case. But you can generally argue that if I have a larger number of different values, each with small probability, then the overall entropy or genie index or whatever associated with that attribute increases. That attribute is actually very random, and we want to avoid choosing such random attributes for our question. That will be\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"or genie index or whatever associated with that attribute increases. That attribute is actually very random, and we want to avoid choosing such random attributes for our question. That will be a way to fix this apparent bug in our choice of attributes, where we blindly take the best one. So these increases and increases. So what we do really is we in some sense normalize. So what we want, we want to take. So the numerator is the one we were calculating before. That is, if I choose attribute a, how does the data split in terms of the categories? So that's the entropy or genie index of the old one that is in each group, how many s's, how many nodes, that's what display. But the denominator is telling me how good or bad a itself is. What is the scattering of a? So the argument here that I was trying to make is that as a gets more scattered, there are more values and the ratio of each value is smaller, then that denominator will become larger. So whatever gain you get on the top. So\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"I was trying to make is that as a gets more scattered, there are more values and the ratio of each value is smaller, then that denominator will become larger. So whatever gain you get on the top. So remember the problem was that these give us high gain on the top, but we are dividing that gain by the scattering of that attribute. So if we have high gain divided by high scattering, then the net normalized effect is lower. So you might choose an attribute which actually gives you a lower information gain overall, because the attribute itself is less static. And that's the thing that we want to achieve. Just to emphasize that when you have a machine learning algorithm, see generally, what is a machine learning algorithm doing? It's trying to describe the shape of the model based on the training data. So in this case it's literally the shape, because you have many possible trees you could construct, and the actual tree you construct depends on the data. And because you don't have any\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"data. So in this case it's literally the shape, because you have many possible trees you could construct, and the actual tree you construct depends on the data. And because you don't have any preconceptions about the data, you have to make sure that if there are these kind of patterns which disturb your algorithm, you will be able to counteract. So that's the purpose of this information gain ratio. It's not just the information gain alone, which is what we had initially. It's information gain divided by the entropy of the attribute. So is this clear what role this is playing? We are just saying that if you have a scattered attribute, then you discount the gain that you get from it by the scattering value. So more scattered attributes will, the net effect will be less because of the denominator. So in terms of our heuristics, what we said is that nonlinear functions work better in practice. And the genie index is in fact the one that is used in most libraries. But we should not blindly\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"So in terms of our heuristics, what we said is that nonlinear functions work better in practice. And the genie index is in fact the one that is used in most libraries. But we should not blindly use this because of this problem of these scattered attributes. So we will actually use this ratio of business to correct. So we have this information. So now let's get back to this question that we initially observed. That is that something like this age, it would be much more natural if this age were described in terms of numbers. So somehow this data has been arranged so that we get this data in terms of categories. But who does the arranging? So how do we realize that in this context? Sir, I have a question, sir. In this loan example, if we add attribute like is an indian citizen or not, then the attribute has only one class, that is Indians. Suppose a bank has, in this case, the impurity is low because we have only one attribute, but it does not give us. What will happen is if you use such\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content='attribute has only one class, that is Indians. Suppose a bank has, in this case, the impurity is low because we have only one attribute, but it does not give us. What will happen is if you use such a category, then it will split. As what? Because if everybody is indian, for example. Yes, sir. Then after I text, if I ask the question is the person indian? Then I will get two groups. One group will have zero rows and one group will have the same set. Yes, sir. So therefore the same set will have the same entropy, right. And so the entropy will actually not change. That question will be totally redundant. Right. So I will get the one times the entropy of the table plus zero times the entropy of zero, which is just the entropy of the table that I started with. So the difference will be zero. So that situation actually does not create a problem. So if I have a trivial, it will actually get filtered out by the algorithm as something which is not useful to ask. Okay, thank you. And the same', metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"So that situation actually does not create a problem. So if I have a trivial, it will actually get filtered out by the algorithm as something which is not useful to ask. Okay, thank you. And the same would hold, actually, if you think about it, if you have an attribute which is not trivial, but there are only very small minor, as you said, supposing one out of 15 is an Indian. I mean, not an Indian, maybe. Right. Then still 14 out of 15 will go into one category. So it'll have almost the same entropy as the original table. Yes, sir. It will not change much. So generally, if you have these kind of attributes which are not really meaningful because they actually cross across a lot of the classification boundaries, usually they will be autocorrect in that sense. Okay, sir. Okay, thank you. So now, returning to the question. The question is, how did somebody come up with this classification? Right? And now, as you can imagine, the notion of young, middle and old is actually a subjective\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"So now, returning to the question. The question is, how did somebody come up with this classification? Right? And now, as you can imagine, the notion of young, middle and old is actually a subjective thing, right? So if you're thinking in terms of, say, professional sports, then young may be somebody who's under 20 and middle aged, maybe somebody who is between, say, maybe under 25 maybe, and five to 30, 32. And then somebody who's in their mid 30s. You already consider NPO, but if you're looking at an organization in terms of the employees, then young people will be typically shifted up everywhere. 35 or so will still be young and middle aged will be, say, in their forty s and so on. And of course, in the extreme case, if you look at politicians and anybody who's under 65 is young and so on. So it really is a question of context. So how do you get this context? How do you break this? So, in general, we have to do one of two things. Either we have to be able to take these numerical\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"so on. So it really is a question of context. So how do you get this context? How do you break this? So, in general, we have to do one of two things. Either we have to be able to take these numerical values and assign, in some meaningful way, these groups, but these groups are also sensitive to the context, or we have to be able to work with the numbers. But then the question is, how do we ask questions about numbers? So let's look at an example. It's a very well studied example. So this is a type of flower called an iris, and it comes in three varieties. So these are the three varieties shown in pictures here. And there's a very famous statistician, one of the fathers of modern statistics, Fisher, who actually analyzed a data set consisting of these things. So he looked at 150 flowers, 50 of each type, and he measured two things in this flower. It's a little misleading. Normally, when we see a flower and we see the colored parts, we think of them all as petals. But botanically, in\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"50 of each type, and he measured two things in this flower. It's a little misleading. Normally, when we see a flower and we see the colored parts, we think of them all as petals. But botanically, in this, there are these leaf like structures, these things. And then there is something in the middle, which is something else, which is a separate bulb like structure sticking out. So these leaf like structures are called sepal, and the petal is this thing. So now you can see that in this one, for example, this is smaller compared to this as well.\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"So there is a distinction visually between the size of the relative size of the petal and the sepal in these three types of flowers. So his question was, can I use this difference to categorize by just measuring these parameters without looking at it? I mean, there's also obviously a visual botanical way in terms of color and all that, of seeing it. But is there a data scientific way of looking at it? So he looked at the steeple length and width, so that is this and this, and he looked at the petal length and width, and he did it for 150 flowers, 50 of each type. So if you then collect these 150 samples and you just focus on, remember there are four parameters, right? There is a petal, there is a sepal, and for each you have length and width. So if I try to plot four parameters, it's a mess. So let me just pick one of the two. So let me just pick petal. So for petal, I plot it in this way. So this is the scatter plot. So the x axis is the length and the y axis is the width. And each\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content='So let me just pick one of the two. So let me just pick petal. So for petal, I plot it in this way. So this is the scatter plot. So the x axis is the length and the y axis is the width. And each dot represents a flower with that width and that length. And these three colors, this yellow circle and the blue square and the green triangle indicate the categories. So you can see that there is a kind of clear statistical correlation between the sizes of these petals and the three categories. At this, there is a very clear separation here. There is a kind of gray area because there are these mixed up things which have, like here for instance, we have similar values and you have flowers from both categories with similar values. So this is this iris data set, and you can actually construct a decision tree for it. So this tree, which we will see later, this python library for decision trees, actually constructs from the iris data set, this decision tree. So what you do is you start with these', metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"tree for it. So this tree, which we will see later, this python library for decision trees, actually constructs from the iris data set, this decision tree. So what you do is you start with these 150 samples, and initially there are three categories, so there are 50 in each category. And remember that if there are at any point when you have an impure sample, you have to decide on something. So this particular algorithm, they're equal. It happens to just choose one of them more or less at random as the category to predict nation. So this last line is the prediction, this is the distribution of values. This is how many samples there are. And this is the actual genie index calculation. If you think about it, it is the three samples are each with one third. So you have one third squared plus one third, one third squared, one nine. So one minus one 9th plus one nine plus one nine, which is one minus one thirds. So that's why you get this in the index of zero, 67. So this is the initial.\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"one third, one third squared, one nine. So one minus one 9th plus one nine plus one nine, which is one minus one thirds. So that's why you get this in the index of zero, 67. So this is the initial. Now, what this decision tree is doing at this point is it is choosing one of the attributes, which is in this case the petal length. And it is asking whether the petal number, so this number, if you look at it, is somewhere here, it's asking whether the petal length is less than 2.45, which is a number between two and three, which splits the yellow from the blue and the green. So if it is, yes, less than 2.45, then all the samples are in the smallest category and it is 500 zero. So all the yellow flowers fall in this category. On the other hand, the remaining hundred are equally distributed and it basically picks the first category among those. And now I have a different genie index of 0.5 because it's half an hour. And now what it does is it looks at a different attribute, it looks at\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"and it basically picks the first category among those. And now I have a different genie index of 0.5 because it's half an hour. And now what it does is it looks at a different attribute, it looks at petal width at 1.75. So if you go back to this picture, it's basically asking something like this at this point. And because we have this area here which is kind of not clearly separated by these horizontal and vertical lines, you end up with an impure slip, right? So you end up with this situation where you have majority of the second type and majority of the third type, but not all. So you still have some nontrivial genie index. It's not zero or it's not zero, but here, in this case, we have stopped to stop, because if I am here, for instance, I've drawn this line and I've drawn this line now, I can still take this and decide to split here and then split there and split there so I can ask more and more questions. But we don't. So that's one reason, going back to the earlier thing, that\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"I can still take this and decide to split here and then split there and split there so I can ask more and more questions. But we don't. So that's one reason, going back to the earlier thing, that you might want to stop, because your genie index is small enough, and it may be different to stop for the genie index versus to stop for the entropy versus to stop for the linear misclassification. But our question is not that. Our question is where did you get these numbers from? Why did you choose 2.45? Why did you choose 1.75? Where did these numbers come from? Because obviously the algorithm is not seeing the picture. It's not like us interpreting that picture and saying these are natural places to draw the line. So how do you get that? This number will minimize the getney index? Is that. Yeah, but then how do you know it is Y 2.45 and not say 2.75 and not 2.15? So that's one question, and in this direction, it's a little more tricky, because the tricky is that if I move it slightly, if I\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"but then how do you know it is Y 2.45 and not say 2.75 and not 2.15? So that's one question, and in this direction, it's a little more tricky, because the tricky is that if I move it slightly, if I move it slightly up or down, I might get slightly more or less. So how do I actually decide on that? So that's the question, is, how do I specifically pick on these numbers? So, obviously, you want to pick on it in such a way that you will achieve some improvement, but how do you find those numbers? And the real problem is that you have a range, right? So, for example, the petal range here is roughly from one to seven. The width is roughly from zero to 2.5. So you can pick any number in that range. You can pick any value v in the range and say, is the petal length less than equal to v or the petal width less than equal to v?\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"The question is, how do you pick that? So, in principle, there are infinitely many choices. But even if I have a range in which I think I should choose, even in that range, I have infinitely many choices. How do we pick one? So the first observation is that we are always working with a finite data set. So even if the potential values. So we just used in this particular data set that there is a minimum and a maximum, there are actually very specific values for the length which appear in our data. So, in general, if I have some n data items for a particular attribute, they will take n values. For simplicity, I've assumed they take n distinct values. And these are numbers. So I can order, so I can order the numbers that I see in the smallest to the largest. So this is for one fixed attribute, then we will see what happens across. So, say we are thinking of petal length. So petal length across these 150 flowers takes potentially 150 values, and we can order them from the smallest to the\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"then we will see what happens across. So, say we are thinking of petal length. So petal length across these 150 flowers takes potentially 150 values, and we can order them from the smallest to the biggest. So now let's look at an interval, right, v I to v I plus one. So if I pick any point in that range, and my question is of the form a less than equal to u, and I pick any u which is from vi up to but strictly less than vi plus one, then all of these points will say yes, and these points will say no. If I shift the u a little bit, it's the same. These points will say yes, these points will say so as long as the u lies strictly less than vi plus one, and it is greater than or equal to vi, everything from b one to vi will say, all the points which have values from vi to b one to vi, will say yes, and all the points. So the data will split in exactly the same way. Remember, what we're finally getting is a split. We're getting some of the flowers go into one category, some of the flowers\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"yes, and all the points. So the data will split in exactly the same way. Remember, what we're finally getting is a split. We're getting some of the flowers go into one category, some of the flowers go into other category. So every question that I ask in this interval has the same net effect as far as the algorithm. So therefore it really doesn't matter. So to go back to our earlier question, if I have that empty space, as we saw in that particular example, it doesn't really matter what question I ask about the length, if it is anywhere between this and this, it's going to give me actually the same value. So it doesn't matter because all the yellow flowers are going to be smaller than that and all the non yellow flowers are going to be bigger than. So that is the fundamental observation that actually because of our data being like this, there are only a fixed number of intervals, that there are only a fixed number of different questions. The form of the question can be many. I can pick\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"actually because of our data being like this, there are only a fixed number of intervals, that there are only a fixed number of different questions. The form of the question can be many. I can pick many different u's that give me the same answer, but the number of different answers I can get is only equal to the number of intervals. Of course I could ask a stupid question which is to ask something here or something here. So if I ask something which is smaller than b one, everything will fall into the bigger. This will be like that earlier thing about indian citizen citizen I get no information. So if I ask for a petal width which is smaller than one, when I know that the range is one to seven, then I get no useful information. If I ask for something bigger than seven, I get no useful information. So only the intervals between these n values give me useful information. So there are only n minus one questions I can ask and these n minus one questions I can ask in many different ways. So\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content='So only the intervals between these n values give me useful information. So there are only n minus one questions I can ask and these n minus one questions I can ask in many different ways. So one canonical way is to just use the midpoint. I can choose any u, I can choose any u in this range to ask for that interval. That interval which says everything smaller than vi is in the set one and everything bigger than vi plus one is in the set. So one way is to just pick ui vi plus v I plus one. So I just pick the midpoint of each interval and I ask a question saying is this attribute a smaller than that midpoint? So for each midpoint that I choose, I get a different split. So if I split it by u one, I get unsplayed, I have v one on the left and v two onwards, if I split it u two, I get v two on the left and v one two on the left and n minus two on the right and so on. So each choice. So I have this many questions I can ask for this one attribute, each attribute, each question that I asked', metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"the left and v one two on the left and n minus two on the right and so on. So each choice. So I have this many questions I can ask for this one attribute, each attribute, each question that I asked splits in a different way. So remember that this is ordered value of a. So I could have an arbitrary pattern of zero one on top of this. So just because I split at a particular point, it doesn't tell me anything about what is the distribution of zeros and ones on the left hand side. That's what I'm really looking for, right? I'm looking for the split in which that entropy of what is on the left plus what is on the right is minimized. And that is something which is not directly connected to which value is smaller, because I'm just looking at the length. So the entropy is not directly connected in general. So I look at each of these, and I calculate for u one, I will get some more or less. Phrase it this way. If I choose the question as a less than equal to u one, I get some new entropy. If I\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"general. So I look at each of these, and I calculate for u one, I will get some more or less. Phrase it this way. If I choose the question as a less than equal to u one, I get some new entropy. If I say a less than equal to u two, I get a different one. Ui, I get it different un minus one, I get it. So for each of these, I will calculate the resulting entropy, or gb index, whichever one you want. So I have those n values, and I will choose the best one. So the best one amongst these is the most useful question to ask about this attribute. So this is how I will convert a numeric range into a question, which is the best question to ask about that numeric range. Is that clear? Yes, sir. So now what I can do is I can assign this. So let me, I think I'll make it more explicit. So I'll come back to this point in a minute. But remember that we said that any point, we chose the midpoint, any point can be used. In particular, the endpoint can be used, right? So we wanted u to be bigger than,\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"to this point in a minute. But remember that we said that any point, we chose the midpoint, any point can be used. In particular, the endpoint can be used, right? So we wanted u to be bigger than, equal to vi and smaller than vi plus one. So I could also choose this as a value. We have chosen the midpoint. I could choose that. So why would you use vi as a value? Well, it might be better to choose vi as a value if these things are not actually meaningful. So, classic example of this is supposing the answer to this question is how many, whatever, how many children are there in the house? Or how many rooms does the house have? So if I take the midpoint, I'm asking a question about a family which has less than or equal to three and a half children, which is not a very sensible question to ask. Whereas if ask less than equal to three children, it makes more sense. So depending on the nature of the attribute, it sometimes advantages to actually pick the endpoints, the lower endpoint of each\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"Whereas if ask less than equal to three children, it makes more sense. So depending on the nature of the attribute, it sometimes advantages to actually pick the endpoints, the lower endpoint of each interval and not the midpoint, because the question may not have an interpretation. If you ask the midpoint if it is salary or something, it's okay if it's an arbitrary number, but if it's some kind of a discretized number which only takes certain values, then you are guaranteed that the values that you see are possible values of that, but the values in between, the values you see may or may not occur in real life. So it may not make sense to ask. But the main point to notice that there are n minus one intervals and you can capture each interval by one question.\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"The only thing you have to agree on is what is the choice that you use for this question, which ui you pick. So now just to revisit the way we'll do this decision tree where now I have mixture of categorical attributes and numerical attributes to choose from. So for each numerical attribute, what I do is I run through this procedure I just described to pick for that numerical attribute the best question to ask. So I choose that. So I've written it as v, but that Ui in some sense, which gives me the most information gained for that attribute given the current range of values. So for now, for each numerical attribute, therefore, I have a best question, and for the categorical attributes I already have only one question. What is the answer? Because it's one of. So I can compute for every categorical attribute, I can compute the entropy directly for every numerical attribute I compute the best split and take the entropy of that best split, and then I compare now across the categorical and\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"attribute, I can compute the entropy directly for every numerical attribute I compute the best split and take the entropy of that best split, and then I compare now across the categorical and these best numerical attribute questions, which is the best one to use. So in some sense I've converted without actually assigning this artificial label of young, middle aged old. I've directly from the numerical values extracted somehow the best possible question. So that's how we now incorporate numerical values. And that's more or less what is happening in that decision tree that we saw for that iris data. So one thing to remember is that earlier we said that when we have a categorical value and we ask the question, then occur uniformly in each partition. So there's no point asking the question again because once I've partitioned by the categorical value, then everybody in the first set is, for example, young, everybody in the middle set is middle aged, and everybody in the last set is old. So\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"because once I've partitioned by the categorical value, then everybody in the first set is, for example, young, everybody in the middle set is middle aged, and everybody in the last set is old. So if I ask it again, I will just get this trivial split right, because there's only one value. But if I have a numerical attribute, I could ask the same thing again. So we saw that that earlier decision tree stopped here. But if I go back to that question picture. So that decision tree, the original decision tree was like at 2.45, right? And. .7. Now I am maybe interested in fixing this problem. So what I see is that maybe if I add a new line at somewhere here, then I actually divide this greens and the blues more correctly. So I'm saying this is, this. So I could ask a third involving the petal length. So I've already asked petal length here, but I'm asking petal length again. But this petal length is with respect to the constraint. So I'm not asking at this point. This is the non trivial\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"length. So I've already asked petal length here, but I'm asking petal length again. But this petal length is with respect to the constraint. So I'm not asking at this point. This is the non trivial split of everything which was bigger than 2.45. So I could ask the same thing again, but with a different value, because I need to shrink an interval which I have constructed after some steps. So in that sense, the number of questions that you could ask is not equal to the number of attributes technically, because these attributes actually have numerical values. So that's the difference between categorical attribute questions and numerical value questions, because numerical value questions you can ask again and again with the different right hand side and the different right hand side will come into play when you have narrowed it down to a smaller set. And now you want to again ask within that set. Okay, so this is the way that you handle numerical attributes and decision. So this is what I\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"into play when you have narrowed it down to a smaller set. And now you want to again ask within that set. Okay, so this is the way that you handle numerical attributes and decision. So this is what I wanted to say right now. We will come back to decision trees later. And decision trees are a fairly interesting model in many different ways. But for now this is what I have to say about decision trees. I'm going to move on to another question now. So if you have anything to ask about decision, we can stop for a minute and just look at. Okay, fine. So let's get back to this other fundamental question that we had, which is, how do we query the effectiveness of our model? So how do we validate whether the model we have built is right or not? So what I had mentioned earlier is that there is a fundamental difference between normal, say test software validation and this machine learning model validation, because when we have some software, we write some code in a traditional setting, we have\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"difference between normal, say test software validation and this machine learning model validation, because when we have some software, we write some code in a traditional setting, we have an expectation, we know something about how the inputs should be mapped to the outputs. So we can create this kind of a test suite which maybe checks the boundary conditions, as they are called. Some extreme cases where you want to make sure that the software is doing the right thing, and then we compare the output with what we know should be the output. Right? So we take the program output and compare it with the expected answer. So we have a notion of a correct answer to compare. So the problem with the classification model is that this correct answer is not known. I mean, if we knew the answer, if we knew a better way to come up with the correct answer, we would not build this model. So the reason we are building this model is we don't have a reliable way to compute the correct answer. So in this\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"knew a better way to come up with the correct answer, we would not build this model. So the reason we are building this model is we don't have a reliable way to compute the correct answer. So in this situation, when we do not have a reference value to compare our answers with, how do we measure whether or not our model is a good one? So on what basis can we evaluate? So what we need is a comparison where we know the answers. But the only situation where we know the answers is our training data.\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"Our training data comes to us with labels. So for each input in our training data, we know the manually or however classified correct answer. So we are assuming those are the correct answers because otherwise our model building process will not make sense. So assuming that those are correct answers, we are building our model. So that's the only source of inputs and outputs where we know the answer. So the only solution really is to use that. So we have to take that training data and keep some of it aside because we are optimizing our model, as we saw in the decision tree, we are optimizing it to give correct answers on the training data. So therefore it doesn't make sense to evaluate it back on the same inputs which are used to construct the tree. But if I withhold some data, so I basically take my training data as a whole and then I kind of split this and say, do not use this part, only use this part to build my model. Then the model that I built from here, I can ask, how does it do\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"take my training data as a whole and then I kind of split this and say, do not use this part, only use this part to build my model. Then the model that I built from here, I can ask, how does it do on that? And there is no problem with this because the model has not seen that data. So it's not been biased by the data on the right hand side. So this data has never been used in the model building process. So it is as good as unknown data as far as the model, but it is unknown to the model. But the advantages is known to us. We know the answer, so we can compare what the model says with what the data says. So one important thing about this is that we need to choose this split randomly, because even though the training set is given to us with no information, usually training data is collected in some way, right? For instance, if it is some data, what I don't, let us say today's everybody's favorite data set is COVID, right? So it might be collected statewide. So it might be that though the\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"some way, right? For instance, if it is some data, what I don't, let us say today's everybody's favorite data set is COVID, right? So it might be collected statewide. So it might be that though the data is anonymous and randomized and all that it's actually the first. So many tens of thousands of things are from a particular state, and then next state, and so on and so on will collect it and collate it into one large state. So now if I say take the first 80% and build a model and leave the last 20% for training, for testing, it might be that the first 80% all corresponds to one part of the country and the last 20% corresponds to a different part of the country. And then we have this problem that if there is a regional variation in the distribution of this particular classification, it will get badly affected. So we need to do some random sampling. So this is a general problem. But here specifically, we want to randomize. It's out of the scope of this course to do random sampling, but\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"get badly affected. So we need to do some random sampling. So this is a general problem. But here specifically, we want to randomize. It's out of the scope of this course to do random sampling, but it's a standard statistical problem. Sometimes you also want to do what's called stratified sampling. That is, you want to preserve other ratios. Like you might have some information about age or gender or something. So you might want to say that if in the overall thing, the ratio of gender is 50 50, then in the sample that I keep for testing also should be 50 50. If the age ratio is in some particular distribution, I should keep a similar ratio. So this is called stratified sampling. But these are sort of different things. But the good thing for us is that in the libraries that we will use, this is an automatic step. You just have to say, split the data into train and test, and it'll do. So, training set and test set. That's it. So this is a little bit, what should I say? Confusing,\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"this is an automatic step. You just have to say, split the data into train and test, and it'll do. So, training set and test set. That's it. So this is a little bit, what should I say? Confusing, because we actually call the whole thing also training, and then we split it. And then we again call this training and test. Right? So training data is a little bit of an ambiguous term. So training data is both the entire data set that you are given to start with. And then it is the specific choice that you make about how to split this data into a large part, which you will use for building the model, and a small part that you will reserve for validating the model. So depending on how much data you have, the usual practice is about 20% to 30% of it you keep aside. So you use about 70% to 80% of the data that you are given to build a model, and you keep a random subset of 20% to 30% away from the model building process. So it is unknown as far as the model or unknown data. It's the same as\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"the data that you are given to build a model, and you keep a random subset of 20% to 30% away from the model building process. So it is unknown as far as the model or unknown data. It's the same as getting new data from the real world in some sense, and you can use it to validate. So, of course, this is a little bit of a disadvantage, because one of the features I mentioned right at the beginning is that typically machine learning expects that the model improves in quality with the amount of training data you give it. So you are in some sense artificially, or because you have no choice, more or less. You are building a suboptimal model, you have 100% data, but you are forcing yourself to use only 70% to 80% of the data available to build the model, because you need to validate it against some unknown data. So, one other strategy where you don't want to, I mean, one of the other disadvantages of doing this is that maybe that there are some minority anomalies in this data, and the\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"some unknown data. So, one other strategy where you don't want to, I mean, one of the other disadvantages of doing this is that maybe that there are some minority anomalies in this data, and the choice of test data that you have made might have hidden all these anomalies, so you never see them, and maybe it's important to see them. So there may be many situations, or maybe you just don't have enough data as a whole to build a good model by only looking at 80%. So another strategy is to systematically do this with different subsets.\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"So what you're really asking at some higher level is that machine learning approach. So, remember, we have seen only decision trees, but we are going to see many models. So there are clearly many ways to build models. And the reason, whenever you see that there are many ways to do something, it's only because there is no guarantee that a given way is the best one. So more or less what you want to validate is, I have, say, seven different strategies to solve this problem using machine learning. And I want to check which one of them is good. So I need to compare. So once I compare, then I'm stuck with whatever I have. It may be that this gives me only 70% accuracy or some such metric, but the other models give me even less. So it's more really a comparative statement to start with, which is, is this method actually giving me something reasonable? And is it giving me better something than something else? So, for this one strategy is so called cross validation. So what you do is you take\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"is this method actually giving me something reasonable? And is it giving me better something than something else? So, for this one strategy is so called cross validation. So what you do is you take some fraction, say, in this case, it is one fifth. So 20%. So you take your data and you leave out. So you randomize it into five groups, which preserve, in some sense, the random behavior, so they partition it five ways. So if I leave out any one of these groups, it's as though I have taken the remaining 80% for training and this 20% for test. But because I don't want to actually think of it this way, I will actually keep doing this for each one. So I will leave out the first 20%, then I will leave out the second 20%. So I will do this. If I have like k chunks, I will do this k times in each chunk. Each model I will build using all but one of these chunks. So this then will solve this problem of many problems. It will solve the problem where you don't have enough data to begin with to\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"chunk. Each model I will build using all but one of these chunks. So this then will solve this problem of many problems. It will solve the problem where you don't have enough data to begin with to afford to not look at some of it, because the interesting things may be in that sum of it. Now, what will you do with this whole thing? Well, you have five. Typically, each of these models are going to be different because we saw with the decision tree, it's kind of the questions that you ask depend on the distribution of the different roles. And if the distribution changes slightly, one attribute might get better entropy than another attribute. And you will choose that question first. So the shape of the tree might vary a lot between these, so you may not get the same. Okay, so the question about cross validation, cross validation. What I'm saying is that you do not make a hard and fast choice once and for all to split your data this way. So you're taking your data and you're saying, hey,\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"cross validation, cross validation. What I'm saying is that you do not make a hard and fast choice once and for all to split your data this way. So you're taking your data and you're saying, hey, first let me leave out this 20% and build one model. So I am building multiple models. I'm building m one, m two, m three, m four, and m five. Each of these models is built using 80% of the test data, but a different 80%. So each of them leaves out some 20%. So I'm building five different models and comparing what I see in all these five. So the first thing is that if this approach is good, then in general, these five models should behave in a similar way with respect to the outcome. So if they all behave similarly, then I can decide what to do. So there are two strategies. I can, I can somehow combine these by saying, build a composite model, or I can say, okay, this strategy is working for me. So this approach that I have chosen seems to be a good way to build a model for this data. So now\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"these by saying, build a composite model, or I can say, okay, this strategy is working for me. So this approach that I have chosen seems to be a good way to build a model for this data. So now let me go back and take the 100% data and build from this whole thing, build a new model M, which will be hopefully better than any of these. M one, m two, m three, m four, m five. But I built those five models to allow myself to know whether this strategy I'm using is good or not. So once I've decided I have a good strategy, then I can do something to make the best possible use of the data to build a new model. Other strategies you can use is you can use these five models and vote. So any input that comes, you can pass them to each of these five models. Each of these five models will give you an answer, and you take the majority answer. So that averages out the errors across the model. So there are many different ways in which you can think of combining these models. In general, as I said,\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"answer, and you take the majority answer. So that averages out the errors across the model. So there are many different ways in which you can think of combining these models. In general, as I said, these models may not be the same. So I will not get a uniform answer to my question, but it will help me to decide whether what I'm doing is right or not. And then I can decide to do one of two things at least. There may be other strategy. One is to combine these models into a kind of a hybrid model which votes among them. Or I can just construct a new model which takes 100%. But now the second question, which this leaves us with, is, okay, I do all this testing, but what am I measuring? So what is the measure that I use to compare or evaluate a model? So, the most obvious measure is I'm given a set of correct answers and I'm given a set of model predictions. What fraction of these predictions are correct? So, this is what's called accuracy. So, on how much of that test set that I kept\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"a set of correct answers and I'm given a set of model predictions. What fraction of these predictions are correct? So, this is what's called accuracy. So, on how much of that test set that I kept aside, on what fraction of the test set did my model make the right prediction? So, the problem is that very often the choice, let us stick to our assumption that we are doing a binary prediction. The choice between the two parts is not symmetric. So the thing we are looking for is very often a minority. So this is a graphic. Blow it up a little bit. So this is saying that if I look at credit card fraud, which is one of the typical examples of where supervised learning is used, right? So we look at different parameters of the transaction, where it was made, what time of day it was made, and how does it compare to other transactions made by the customer and whatever, and the bank wants to know if it's a fraud. So if you actually look at the attempted fraud. So this is going into the future,\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"does it compare to other transactions made by the customer and whatever, and the bank wants to know if it's a fraud. So if you actually look at the attempted fraud. So this is going into the future, it's going into 2027.\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"But let us look at even the past data, which is presumably accurate. So I don't know from when this dates, but one can assume that at least, say, up to, say, the last ten years maybe is correct, say, up to here. So what we see is that if I look at the volume of credit card transactions in terms of value, then out of $100 worth of credit card transactions, the total volume of the fraud transactions is less than $0.10. If it was one dollars, it will be 1% or less. So if you assume that it is proportional to the number of transactions, this is the volume in terms of the value but if you can also reasonably extrapolate and say that the total number of transactions which are fraudulent, attempted fraudulent, are still very small, so it is safely less than 1% because the value is less than 0.1%. And why would people be doing low value frauds? They will prefer you doing medium value frauds. So the cheapest transaction, the one that you go and buy toothpaste, is not going to be likely to be a\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"And why would people be doing low value frauds? They will prefer you doing medium value frauds. So the cheapest transaction, the one that you go and buy toothpaste, is not going to be likely to be a fraud. It's more likely to be something where you buy some clothes or something. Somebody asked about shuffling before partitioning, right? Yes, the partitioning is done randomly, and I'm not going to talk about how it is done, but yeah, one way is to shuffle it, or you can hypothetically shuffle it by running through the index in some random way. Right. So sampling is a standard statistical problem, and it is not something I'm going to discuss most. But the libraries that we will use, which I will show you later on, have a standard way of saying taking this test data, split it uses, tell it how much you want, 70%, 40%, 30%. Users have to tell it, it will do. So it's a black box as far as they're concerned. But yeah, in practice, you will have to do some shuffling and. Okay, so now coming\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"much you want, 70%, 40%, 30%. Users have to tell it, it will do. So it's a black box as far as they're concerned. But yeah, in practice, you will have to do some shuffling and. Okay, so now coming back to the question, now I have only 1%. So now I want an accurate classifier. So what would I do? So I have a classifier where I know that in this case the frauds are less than 1% of the total cases, which the classifier will see. So I can build a trivially accurate classifier by just doing nothing. I can just say no. If I just say no, then I will have 99% accuracy, because I know that the distribution of yeses is less than 1%. So this is highly accurate, but highly useless. So really we want to force the classifier to find the thing. And the usual assumption is that, as I said, this minority is quite common. I mean, email, junk mail is a much higher proportion, but in many cases the minority, the classification that people use it for in real life, is a really small fraction. So the real\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"is quite common. I mean, email, junk mail is a much higher proportion, but in many cases the minority, the classification that people use it for in real life, is a really small fraction. So the real question is to flag the yeses because the no's are in a vast majority, and to say no and get away with it doesn't help me much. So I want to catch the yeses. So now, in any situation, we will see that you could have two types. You could have aggressive classifiers. So I have a borderline case, right? It could be yes, it could be no. So if I try to flag as many yeses as possible, I might be pushing some no's into yes. And these are called false positives. A false positive is when the test says yes. That is, a model says yes, but actually it's not yes. So the falseness is with respect to the output and the output. So this could be a bit confusing. But false positive means that the answer is false and the answer is positive. And similarly, the other way around. If I'm very cautious, I might\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"and the output. So this could be a bit confusing. But false positive means that the answer is false and the answer is positive. And similarly, the other way around. If I'm very cautious, I might push these borderline cases to no. Then I get false negative. So we have all heard about this in the context of COVID tests. So if a test comes out positive and you are not COVID affected, then it's a false positive. If you have COVID and it comes out negative, it's a false negative. The answer should say yes, but it says so. That's the false negative and false positive. So the thing is, accuracy is only answering two questions. I mean, it's only looking at a two way classification. Is the answer equal to or not equal to the expected answer? But what we are seeing is that the expected answers come in two asymmetric buckets. There's a correct yes and there's a correct no. So the actual answer could be yes or no, the prediction could be yes or no, and the mistakes come in two types, and the\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"come in two asymmetric buckets. There's a correct yes and there's a correct no. So the actual answer could be yes or no, the prediction could be yes or no, and the mistakes come in two types, and the correct answers come in two types. Right? So we saw this false positive and false negative. So these are the two types of wrong answers where I expect to see a no and I say yes, that's a false positive where I expect to see. So if I expect to see a no and I classify it positive, it's a false positive. If I expect to see a yes and I classify it negative, it's a false negative. So these two are the wrong answers. Answers could either be that it is yes and I say yes or it is no, and I say no. And our assumption is that this is actually the biggest category. Right. Most of the cases are no because of the way we have assumed that the minority answer is yes. And if our classifier is any good at all, it should get most of the no's correct also. So true negative will typically be the largest\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"of the way we have assumed that the minority answer is yes. And if our classifier is any good at all, it should get most of the no's correct also. So true negative will typically be the largest fraction. So what we are really looking at is these four things. So this thing is called a confusion matrix, this particular classification. So now we take this accuracy Question, and we split it into two separate questions. Right. The first question is that it's really something related to accuracy. It's a question of Reliability, saying that if the classifier says yes, then what is the probability that it's really yes? The classifier says yes. In this column, right?\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"These are all the entries which are classified as positive. That's a denominator. How many of these are actually correct? So that is called Precision. Right? So how many times, in some sense, does your classifier make a mistake on the yes? It's different from making a mistake on the no. That would be referring to this column. We are talking about this column. So the Precision says how many yeses that the classifier produces are actually correct YeSEs. And the issue you are dealing with with that trivial classifier is that I'm interested in finding out things in this row. These are the real frauds in the Clinical. How many of them does my classifier actually give me? So how many of these are reported in this box? So what is the fraction of true positives among all those that should be positive? So this is called recall. So this gives us, typically a finer demarcation between right and wrong. Now, the problem with this is that typically, as we said, we have this aggressive and kind of\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"So this is called recall. So this gives us, typically a finer demarcation between right and wrong. Now, the problem with this is that typically, as we said, we have this aggressive and kind of lenient classifier. So an aggressive, cautious classifier will not say, we'll come to examples in a minute. Will not say yes unless they show. So here it is very sure about one case, so it never gets it wrong. So here the precision is one. Every case that the classifier said is one. So there are people who can count it is 900 plus 99%. There are 1000. Right. And the split is there are hundred here and 900. So in the actual data, 90% are negative and 10% are positive. And the first classifier we build has this property that it is overwhelmingly cautious. It will usually say no unless it is very, very sure it will say yes. Now, I might want to make it more permissive. So I might want to move something from here. I want to give it some borderline cases I make. It stands to reason that some of these\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"sure it will say yes. Now, I might want to make it more permissive. So I might want to move something from here. I want to give it some borderline cases I make. It stands to reason that some of these will also. Whatever criterion I use on the top is also likely to affect the bottom. So if I make it more linear or more permissive, I say positive more times, I will improve the recall. Right? So the recall earlier was one by 100. Now I've improved the recall to 40 by 100. That is this ratio. But as a result of moving those 39 cases on the top row, I've also ended up moving 100 case on the bottom. So I've dropped the precision down to 40 by 140. That's this 40 by 142 by seven. And if I move it still more, if I really want to get to very high recall, I want to catch almost all the positives, then the precision will go down further. Now, it's gone down to like one by six, almost. So it now really depends on what you want. So do you want to catch all the s's or do you want to be careful? So\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"the precision will go down further. Now, it's gone down to like one by six, almost. So it now really depends on what you want. So do you want to catch all the s's or do you want to be careful? So there are situations where you want both. So think of hiring. Right. You're trying to recruit somebody. When you do an initial screening, you don't want to miss out any good candidates. You will pass on some dud candidates. So you don't mind getting some poor people passing your screening test, but you don't want to rule out anybody. So you want high recall. Everybody who is good should pass your screening test. Then you have an interview. Now you're actually calling in your organization. You don't want to hire somebody who will be a misfit whom you're stuck with for years. So now you will be very strict. So you might have an excellent candidate, but there is something about this candidate you don't like, maybe the personality, or you think this person will not get along with. There's\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"be very strict. So you might have an excellent candidate, but there is something about this candidate you don't like, maybe the personality, or you think this person will not get along with. There's something wrong. So even though this may be highly qualified and may have been a great success, you will say so here. You want high precision. Everybody you select must be definitely selectable. You don't mind losing out on some good candidates for whatever reason. So depending on the nature of your screening, you might want to do again, similarly, in medical diagnosis, if you want to make everybody immunized against something or if you want to give them. So if you have a disease for which there is a simple medication, then you may not be very specific about the diagnosis. Anybody who comes with certain symptoms, this is more or less what's happening now with this omicron, right? Anybody who has a cough, cold and fever is assumed to have omicron, and you just tell them to isolate. So this\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"certain symptoms, this is more or less what's happening now with this omicron, right? Anybody who has a cough, cold and fever is assumed to have omicron, and you just tell them to isolate. So this is something which is high recall. You want to catch everybody who's sick and make them less of a risk to people around them. On the other hand, if it's some really critical diagnosis, it's like pancreatic cancer or something where you're going to die in six months or three months. You don't want to tell somebody that unless you're very sure, because that whole person's life will be and their family's life will be uprooted by this. So then you will have a second opinion, third opinion before you actually make the decision. So you will not just go on an initial suspicion that, oh, this looks like pancreatic cancer, and blurt it out to the patient. You'll make them both. So in many situations, it's not clear, but what is generally clear is that these two things are usually not aligned. You\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"pancreatic cancer, and blurt it out to the patient. You'll make them both. So in many situations, it's not clear, but what is generally clear is that these two things are usually not aligned. You cannot normally do both together. You cannot have normally. If you have to trade off either you want recall or you want precision. And one will go up.\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"If you want the other one go up, the first one will come down. So now I just mentioned that these are not the only things so people also talk about. So there is, for instance, the corresponding thing to precision for the negative part is specificity. Right? So what is the ratio of correct negative results compared to all negative results as a true negative? So that's with respect to this column, then accuracy, of course, is just saying how many of the correct things are there with respect to the whole thing. So that also can be measured easily from this. And then there's something called thread score and so on. So I'm not going to talk about it. And just for completeness, I will mention that sometimes people want to say that we want a single number. It's not good to give me a precision separately and a recall separately and let me choose, I want a single number. There is this thing called harmonic mean. So the harmonic mean is one upon p plus one upon R. So remember, p is precision\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"separately and a recall separately and let me choose, I want a single number. There is this thing called harmonic mean. So the harmonic mean is one upon p plus one upon R. So remember, p is precision one and it's one upon that. So it's the reciprocal of the mean of the reciprocals. So if you work this out, you get two PR on two p plus. So this is sometimes called the f score. Right? So this is a way of combining the precision score and the recall score into a single number. But as I said, in most cases, it really depends on the application. So either you want something based on the context which has higher precision or higher recall, and you choose. So these are some of the ways in which we can actually. So, remember, our starting point was that we wanted to evaluate classifiers. So we said first we have a strategy for evaluating classifiers, so we can test them by using this test set and so on. But what is it that you want to measure on the test set? So that we have this wide variety\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"first we have a strategy for evaluating classifiers, so we can test them by using this test set and so on. But what is it that you want to measure on the test set? So that we have this wide variety of things, but most commonly this precision and recall are the ones that people are interested in, and they will choose which one to emphasize. Depending on the context. Depending on the context, you may want precision predominant. You may want recall to be predominant. So there, it really becomes a domain specific. So I'll stop there. Any questions, sir? Like, given a precision or recall value, what we need to change in the algorithm so that we could come to the certain value of precision and recall. So that is a harder question. Right. So what is it that you need to change? So that's a very difficult question. All you can say is that given what we are doing now, this is what, then it becomes really an experimental issue. So this is the hardest question in machine learning, which is, how\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"difficult question. All you can say is that given what we are doing now, this is what, then it becomes really an experimental issue. So this is the hardest question in machine learning, which is, how do I improve the quality of the classifier? So there is no answer to that question, which is obvious. So you have to start experimenting. You have to do something. So there is something called feature engineering. You might want to come out with some combination of features. So some of you may have heard of this thing, for example, which is used in medicine called body mass index. So, body mass index is some combination of some formula involving height and weight. So you might have a data set in which you have something like height and weight usually classify using height and weight. It doesn't work. But if you combine it in this strange way using body mass index, then you actually get a better class. So this is an example of what is called feature engineering. You make new features out\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"work. But if you combine it in this strange way using body mass index, then you actually get a better class. So this is an example of what is called feature engineering. You make new features out of the old one, or you might need to change your model, maybe decision tree is not good, you need to use another model. So these are all very open questions, and I don't think there is any. If there were a clear answer to that, then life would be a lot simpler. But unfortunately, that is the real challenge of machine learning. What is it? If I am given a certain measure of performance, what is it that I should do to improve that measure? It's not clear. Okay, got it. Okay. Sorry to keep you over time. So next time, what I'll do is I will start by showing you some decision tree code from that python library, which is there in thing, and then we will move on to another module. So that will be on Monday. Bye.\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content='So the problem we are looking at is this problem of market basket analysis. Like people who buy X also buy Y. So we are looking at these kind of association rules, saying that if there is a set of items X in the basket, then there is also likely to be a set of items Y in the basket. So formally, we have this set I of items, which is assumed to be a large set. We will talk about typical numbers and then also have 1. Second, we have a set of transactions. So each transaction is just a set of items. So we said last time that this is a simplification because a basket of items could have multiplicities. So you could have five of one and three of another. But we will look at it in this very simple case, and maybe I might mention later on about how to generalize it. But the question for us right now is just look at the simple case. So we have M items, I mean, m transactions over n items. And we want to find these two sets of items, x and Y, such that if X is a transaction, X belongs to a', metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content='now is just look at the simple case. So we have M items, I mean, m transactions over n items. And we want to find these two sets of items, x and Y, such that if X is a transaction, X belongs to a transaction, Y is also, like it, getting an echo from somewhere. Hello? Okay, so not every such set implication is relevant to us. So what we said is that we want to, first of all, make sure that this happens often enough. So as a fraction of times when we see X, how many times do we see Y? So that we said was one threshold and the other one was how significant? This pattern is over. So these are these two thresholds. So we have this confidence level, which is the ratio of times when we see the desired conclusion compared to the premise. So the premise is that X is there. The conclusion is X and Y are both there. So notice that this is X union Y, right? X and Y. Normally we think of as both X and Y are there. But X and Y are two separate sets. So we want both sets to be present. All of X', metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"and Y are both there. So notice that this is X union Y, right? X and Y. Normally we think of as both X and Y are there. But X and Y are two separate sets. So we want both sets to be present. All of X should be there and all of Y should be there. So that's why it's X union Y. So we want everything in X and Y to be there as a fraction of only X being there. Of course, if everything in X and Y is there, then Y, X is definitely going to be there. So the top is going to be a subset of the bottom, the count. So remember, the count is just a number of transactions in which a subset appears. So the number of transactions where you see both x and Y will always be less than those where you see x alone. Because if you see x and Y, of course, x is present. So this will be some ratio which is between zero and one, and we want it to be above this confidence level, chi. And finally, we want to say the number of times we actually see the conclusion. Both x and Y should be a significant fraction of\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"zero and one, and we want it to be above this confidence level, chi. And finally, we want to say the number of times we actually see the conclusion. Both x and Y should be a significant fraction of the total transactions, and this is a support level signal. So the problem now we have is actually in some sense an algorithmic problem. It's not technically a learning problem, because this is a fixed problem once we fix these quantities. So given the set of items and the transactions, and given these two thresholds, find every x y which satisfies these two constraints. This is what we are trying to do. So we will see what it means for learning at the end. Sir, I had a question. So therefore, what we said is that a rule is interesting only if this ratio is bigger than sigma. Right? The fraction of transactions where we see the target of the rule must be at least sigma. If I take m to the other side, this is same as saying there's a number of transactions with x and Y. So this is just the\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"of transactions where we see the target of the rule must be at least sigma. If I take m to the other side, this is same as saying there's a number of transactions with x and Y. So this is just the number of transactions. X and y exceeds sigma times the total. It's some whatever, 1% of the total, 10% of the total, and so on. So what we said last time is that this would be our first criteria. So we want to find x union Y, which is plausible. Once we have x union Y, which is plausible, we will try to break it up as x and y. So we are in some sense looking for every subset which satisfies this constraint. And what we were saying was that the simple way to do that, a naive way to do that, is to just maintain a counter for every subset and run this loop. So for every transaction, you enumerate every subset that appears in the transaction and increment the counter for that subset. So in this way, if you're maintaining it as, say, a python dictionary or something, then you will be only\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"every subset that appears in the transaction and increment the counter for that subset. So in this way, if you're maintaining it as, say, a python dictionary or something, then you will be only maintaining a counter for every subset which actually appears in your transactions. But even then, the problem is that there are, compared to the number of items, the number of possible subsets is very large. So you cannot really be sure that you are going to be able to keep it under control. So the question is, how can we do it better? So before we see how can we do it better, perhaps the first question is. So the question is, how can we do it better? But before we say how can we do it better, we should first have some realistic expectation that we can do it better, or it should be possible to do it better. So we can see that just by looking at a numerical example.\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"So suppose, first of all, we discussed this last time also, somebody asked this question, which is how big is a transaction? So remember that in this market basket thing, a transaction is like a set of items that somebody proceeds towards the checkout counter, to the billing counter. So it's not going to be very large, certainly much smaller than the total number of items, and certainly also not very large in general. Also, from the point of view of using the fact that x implies y, right? So if I say that, if I say that x implies y, supposing this has some 20 items and this has some seven items, right now, this is unlikely to be something that we are able to take any action with. I mean, how can you say that somebody who bought 20 items together also bought these remaining seven items? What will you do with this information? So usually we are looking at it when x and y are small from a practical point of view. So even from a market basket, retail example, if people want to use it to\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"will you do with this information? So usually we are looking at it when x and y are small from a practical point of view. So even from a market basket, retail example, if people want to use it to sell items, they will say that, okay, toothbrush and toothpaste means they're also going to buy some soap or something, but you're not going to put together large set. So it makes sense to think of small sets. So let us just put this, this is just an example. Anyway, let's assume that there is an upper bound of ten items. The other bounds, I'm assuming, is that we have ten to the nine, that is 1 billion, that is 1000 million transactions, and we have 1 million items, ten to the power six, one with 60. So this is 1 million, this is 1 billion. So we have a large number of transactions. And now we need to remember we are only dealing with the frequent item set problem. So the confidence level is not yet an important factor for us. We are just trying to find this thing. All the z such that z dot\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"to remember we are only dealing with the frequent item set problem. So the confidence level is not yet an important factor for us. We are just trying to find this thing. All the z such that z dot count is bigger than sigma times m. So the only factor that is important to us right now is the sigma. So sigma, let's assume, is very small. Sigma is just 1%. We just want to know which subsets appear in 1% of the transactions. So as we said before, if you actually naively count the possible subsets that you have to keep track of, then it will be for every size up to ten, right? Because we have at most ten items. For every size up to ten, we have to choose that many items, I items from the 1 million. Each of those is a different subset. So we will have that many possible subsets. If you are counting every possible subset, on the other hand, just look at a particular subset. So fix an item. So fix x belonging y and ask this question. So because our threshold is 0.1. It must appear in 100 of\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content='every possible subset, on the other hand, just look at a particular subset. So fix an item. So fix x belonging y and ask this question. So because our threshold is 0.1. It must appear in 100 of the transactions. So since there are ten to the nine transactions, this particular x must appear in ten to the seven transaction. This is just equal to 0.1 times ten to the power. So this is a straightforward statement. But now notice that we have ten to the nine transactions and ten items per transaction at most. So this tells us that the total amount of total volume of items which are mentioned in our entire transaction history is at most ten to the nine times ten. Each transaction possibly has ten different items, in which case across the ten to nine transactions I am mentioning ten to the ten items. Of course, in general some items are going to be duplicated, but this is a complete upper bound. So ten to the ten is an upper bound on all the items that can appear in my transaction database.', metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"Of course, in general some items are going to be duplicated, but this is a complete upper bound. So ten to the ten is an upper bound on all the items that can appear in my transaction database. It's not saying distinct items, it's saying the number of items, which number of slots. So if you just think about your thing as a matrix, right? So this is a transaction and these are the items. So there are only ten to the ten slots grid. And now what we have done is we have filled up somewhere in this x appears here. X appears here. X appears here because x is frequent. How many slots does it appear in? It appears in ten to the seven slots, right? So now if there's another y which is frequent, that will also appear in ten to the seven slots. So every item that is frequent individually, forget about sets. Any single item which is individually frequent must appear in ten to the power seven slots. So if I keep dividing this into ten to the power seven slots per item, then you can only have 1000\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content='sets. Any single item which is individually frequent must appear in ten to the power seven slots. So if I keep dividing this into ten to the power seven slots per item, then you can only have 1000 items which appear at all. So if every item that appears in this transaction history is actually frequent, then there are at most thousand items. In general it will be even smaller because some of the items will not quite make it to pretend to the seven, in which case I can ignore. So this is just an illustration as to what happens in reality, which is that because of the numbers there is a practical bound on what will actually be frequent, even though theoretically the number of frequent items is very large. So we had 1 million possible frequent items. Actually 1000 of them can only be frequent. In this situation, of course you can tweak the numbers and get different answers. But for this particular choice of numbers, this is what comes. So the important thing is that you bring down this', metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content='In this situation, of course you can tweak the numbers and get different answers. But for this particular choice of numbers, this is what comes. So the important thing is that you bring down this even at an individual level, you bring down 2000, which is a huge improvement. So is the calculation in this problem clear? So this is saying that with large number of transactions, large number of items and a relatively small threshold, I still cannot have too many frequent items.', metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content='So this is a motivation for thinking that there should be a better way to calculate these frequent subsets, rather than just maintaining a counter for every possible subset that could be frequent, because the number of subsets that can be frequent are actually relatively few in numbers. So the question is, how can you identify these thousand things or these small number of things easily? So we already said that if x and y are both present, then x must be present, right? So we talked about this x union, y dot count divided by x dot count. And the reason that this is less than equal to one is because if every time I see the top, I must see the bottom. So this is a general property that if I see a set, then I see everything that is inside that set. So any subset of that set which is frequent must also be frequent. This is a very simple observation, because y must appear every time z appears, and z appears often in. So the interesting thing that we can do is turn this question around so', metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"must also be frequent. This is a very simple observation, because y must appear every time z appears, and z appears often in. So the interesting thing that we can do is turn this question around so we can take what is called a contrapositive. So we can say that if this is not there, then this is not there. Sir, can you just explain the previous slide again? Previous slide. This one. The calculation. No, the a priority. Yeah, so all I said is that if z appears, say if z appears some k times, right? Then I'm just saying that any y which is subset of z appears more than equal to k times. Because every time z appears, y is big part of z. So y also appears. Y will also appear sometimes when not all of z appears. So the frequency of a set y which is a subset of z must be more than the frequency of z. Okay, I got it. And what we are interested in is something of this point, right? Z dot count is bigger than equal to something. So if this is already bigger than that, then y dot count will\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"of z. Okay, I got it. And what we are interested in is something of this point, right? Z dot count is bigger than equal to something. So if this is already bigger than that, then y dot count will also be already bigger than that because it's bigger than z. So we'll have. So y will also be a frequency. Yeah. So what we are saying is that if some subset of z is not frequent, then z cannot be frequent. So this is another way of saying it is that. So I've just soaked the fire axis. Poor terminology. But you can fix it if you want, by changing this. So you can say that if y is not frequent, then no superset z of y can be. So if. If something is not frequent, then it cannot form part of a frequent set because it will be a subset of that set. And if that set appears often enough, this thing must also appear often enough. So you can only build up frequent sets from frequent parts. That's what it's saying. You cannot build up a frequent set from components which are not themselves frequent.\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"also appear often enough. So you can only build up frequent sets from frequent parts. That's what it's saying. You cannot build up a frequent set from components which are not themselves frequent. And this principle is called a priority. A priority means latin phrase. You may have come across it. So a priority. You assume something. So in the beginning. So basically it's talking about something that if in the beginning something is not true, then something cannot be true. So this is called a priority. So the principle is very simple. It just says you cannot build a frequent set from infrequent components. So that gives you this. From the previous example, it says that we only have 1000 frequent items. Every item is in some sense a trivially, it's an element of the set. It's also a one item subset of any set it belongs to. And you cannot build a set from infrequent components. So every item that belongs to a frequent set must itself be individually frequent as a one item set. Correct.\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"subset of any set it belongs to. And you cannot build a set from infrequent components. So every item that belongs to a frequent set must itself be individually frequent as a one item set. Correct. But we saw that there are only 1000 frequent items. So all the frequent sets we could ever build can only use those 1000 items. It cannot use any items outside that. So we had 1 million items in that example. But every frequent set, by doing this first approximation of one item sets, cannot have more than 1000 different things. So for instance, if I look at a pair, then both x and y must be frequent. X must be from that thousand, and y must be a different element from that thousand. So when I'm counting pairs, I only need to look at pairs of the form x comma y, where x and y have both been known to be frequent. So first, I count the sets which are the elements which are frequent items individually. Then I can say, okay, now remember, our whole problem was maintain. I mean, counting is an\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"to be frequent. So first, I count the sets which are the elements which are frequent items individually. Then I can say, okay, now remember, our whole problem was maintain. I mean, counting is an obvious thing, right? When I go through the transactions I go through for each transaction, for each was our basic thing. The problem was, at this point, how many z's are we counting? So what we are saying is, we don't need to count any z, which is not a candidate. And a candidate will mean that it was already made up of frequent parts. So I'm only counting. For instance, if I go from here, I only have 1000 times 1000 possible. Actually 1000 times 999, because that would be different, right? So this is like, even if you don't, even if you count it for everything of this possibility, that's still thousand times thousand is 1 million. So 1 million is like 1 size array. So even if you had an array and stored an integer, it'll be like four megabytes of data, which is easily achievable in any\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"thousand times thousand is 1 million. So 1 million is like 1 size array. So even if you had an array and stored an integer, it'll be like four megabytes of data, which is easily achievable in any standard program. So this is the UpRI principle. So let's formalize this. So what you do is you start with, you start building up these frequent sets level by level. So fi is the set of frequent item sets of size I. So these are all the sets which you know to be frequent, but whose cardinality is exactly I, not at most exactly I.\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"So what we said initially was, you do f one. That is, you find all the frequent items by maintaining a counter for every item. So you have to assume that this much counter space you have. So you maintain a counter for every item. So in our previous example, the items were ten to the power six. But even if you have something bigger, like even if you have 100 megabytes of data, ten to the power eight items, that also will work. But there's no problem storing that much information. I mean, nowadays we have four GB RAm and all that. So it's not a big problem. So we can maintain a single, we have to count pairs. So when we count pairs, what we do is we first make a list of all the pairs which are worth counting, and we call these the candidates. So the candidates at level two are pairs of which both components belong to the first one. Otherwise I'm not interested, right? If I have x comma y, and y is either x or y is not in f one, then I know that x comma y is not going to be frequent. So\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"both components belong to the first one. Otherwise I'm not interested, right? If I have x comma y, and y is either x or y is not in f one, then I know that x comma y is not going to be frequent. So there's no need to maintain. Accountant. So, sir, f one is just singleton sets, right? F one is just Singleton. Just think of singleton sets. Yeah. Items singleton sets. So it's sets of size one which are frequent. So every subset of a set that is frequent must already have been frequent. That's what we are saying. So therefore, if I have pairs, the nontrivial subsets of a pair are the two individual sets. So I want x to be frequent and y to be frequent. So maybe if you are asking that it should be maybe x and y if that is the question, right? Yes, sir, I got it. So we will maintain a counter for every x comma y of this form. And then we will scan the transactions one more time. So we scan the transaction the first time to count f one. Then we'll use c two as our set of counters. So c two\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"for every x comma y of this form. And then we will scan the transactions one more time. So we scan the transaction the first time to count f one. Then we'll use c two as our set of counters. So c two depends on f one. And then we will scan this and again, maintain account. And those which cross this count will become f two. So for every set which belongs to c two, if the count crosses the threshold, we will include it in f two. So now we go to three. Right. So what does level three tell us? Level three tells us that I am looking for X-Y-Z such that every X-Y-Z all the three two item subsets are in f. So this basically restricts so naively, we have all triples, roughly all three elements, tuples, three tuples of I. Yeah. And then we filter it and form f two. Yeah. So f two is now going to be a subset of c two. So f two is going to be some subset of c two, and it's usually going to be a small subset, just like f one was a small subset. That's what we got in the previous thing, right? So\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"a subset of c two. So f two is going to be some subset of c two, and it's usually going to be a small subset, just like f one was a small subset. That's what we got in the previous thing, right? So 1 million came down to 1000. So now I maybe have 1001 thousand, 1 million choices again for f two. But again, by the same logic, in fact it will fall by even more, right? Because every pair that is frequent will again occupy that many slots. So how many different pairs can occupy the slots? Again, you'll find that only, I mean, even if they overlap, it will be less than 1000. So it could be that x y is frequent and y z is frequent. But still between x y and y z, they have occupied three times ten to the power seven slots. Remember that table I drew of all the transactions times the number of things are done? So whichever way you think about it, the number is going to drastically reduce compared to the candidates when you actually count. So in fact, in reality, this is going to be the\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"of things are done? So whichever way you think about it, the number is going to drastically reduce compared to the candidates when you actually count. So in fact, in reality, this is going to be the largest signal. The items you can usually count individually, but then when you restrict it, the frequent items of f one times f one is actually going to be your largest normally or largest candidate set that you have to actually keep track of. Remember, for each candidate set, the bottleneck is you have to keep a counter for that candidate set. That's the reason that this is something that we need to keep track of. So now c three says do this, take all xyz such that every two item set has been certified to be in f two. So this is a nontrivial thing. I'm going to come to this in a moment. Right. So you have to construct c three before you can proceed. So it's not straightforward. I mean, you have to. The naive thing says, look at every xyz and check whether X-Y-Z and x z and y z are all in\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"So you have to construct c three before you can proceed. So it's not straightforward. I mean, you have to. The naive thing says, look at every xyz and check whether X-Y-Z and x z and y z are all in f two. If so, throw it into c three. Otherwise go to the next. So you have to enumerate. So even though I'm saying you restrict this to this, the naive way of doing it is by first enumerating every possible xyz and checking whether it should be in c three or not. That's what we have right now. But modulo that, which will, we will address that in a minute. Okay, but modular that, this is how it goes. So if we generalize this, if I come to ck, so ck is, I'm going to try to now check for all frequent sets of size k. So what does this mean? I must have every k minus one subset. So notice that this is downward. So here it's not so obvious, but here I don't have to check. So what? That up priority thing says that every nontrivial subset must be in frequent set. That means every two item subset of\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"downward. So here it's not so obvious, but here I don't have to check. So what? That up priority thing says that every nontrivial subset must be in frequent set. That means every two item subset of the c three item set must be in f two, and every one item subset of that must be in f one. But we already know that if a two item set is there in f two, then its components must have been in f one because they're all.\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"So this uprithing is closed transitively that way. So if a bigger set is there, you can assume that everything smaller is there. So you only need to check the biggest ones. So when I'm looking at k, I only need to check k minus one. I don't need to check k minus two, k minus three, k minus four, and so on. Because if a k minus one set is there, I know that all its smaller sets must be there. But the point is I have to look at k element sets and check every k minus one subset. So just like here. So this is like saying, so in general, if I say, say x one, I enumerate a k element set, then I'll have to say, okay, is this set x one two? Is this set there? Then if I drop this, is that set there? And so on, right? I have to drop each element one by one and see if the corresponding k minus one element set is there in fk minus one, which I've already calculated in the previous step. And if all these, how many of them are there? There are exactly k of them. Because if I take k elements and I\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"element set is there in fk minus one, which I've already calculated in the previous step. And if all these, how many of them are there? There are exactly k of them. Because if I take k elements and I generate k minus one element subsets, each subset consists of dropping one element. So if I drop the first element, I get a set. If I drop the second element, I get a set. So there are k ways of dropping one element from this set to get a k minus one subset, and all those k ways of dropping one element must still give me a subset in fk max. But I have to enumerate everything. This is the problem. I have to enumerate everything of size k in order to do this and explicitly check each of the k subsets of that, propping one item at a time. But once I've done that, then I can run through my skin again and do right. So that's the situation right now. So what we want to do is figure out how to make this little better. The question really is, how do we generate this ck at size k? At two, it's\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"again and do right. So that's the situation right now. So what we want to do is figure out how to make this little better. The question really is, how do we generate this ck at size k? At two, it's easy, two, it's just we have no choice. It's f one, cross f one. But at three already we have a problem in some sense, because we have to go through all triples and then choose those which every pair is there. And at higher k it's even more because if I enumerate every set of size k, I'm doing something like. So if I have, remember that this is n, I'm doing some n, choose k. These are the number of sets of size k, and this is something which grows exponentially in k. So I don't want to do this. So do you see the problem? So this is k factorial into n minus k factorial. So this is for the other way. So this is going to become very large. So we don't want to do this. So what is our goal? Our goal is to count everything that is potentially frequent. So the idea is that Fk should be a subset of\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"So this is going to become very large. So we don't want to do this. So what is our goal? Our goal is to count everything that is potentially frequent. So the idea is that Fk should be a subset of CK. This is our goal. So if we count here, we don't miss anything. So it should not be that I did not count a subset because I thought it was not useful, and as a result I did not enumerate it. So that should not happen. So, so long as I choose my ck in such a way that anything which is potentially frequent after counting is present in Ck before counting, because I'm counting only those sets that are in CK. So in our case, because of a priority, we know that anything that is frequent must have all its subsets frequent. And that's why Ck is a valid upper bound for Fk. So what I'm saying is that if I'm not very insistent on that, I could pick anything which is bigger than Ck and count it. In particular, I could count every k tuple. But that is the problem, right? We don't want to count every k\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"I'm not very insistent on that, I could pick anything which is bigger than Ck and count it. In particular, I could count every k tuple. But that is the problem, right? We don't want to count every k tuple because that is too many. So can we generate something which is easier to calculate, but which includes everything in Ck and is not too big? So this observation is clear that any Ck prime which is bigger than CK can also be used. Our goal is to identify a suitable version of CK prime. So let's make an assumption which is not unreasonable. Let's assume that these items is not just a set, but it's like a set with some order on it. So you can think of, for instance, if it's a supermarket type of retail kind of example, you can think of these numbers as being, say, some serial number. At every checkout counter of every one of these supermarkets, they scan the barcode. So the barcode is a number. So you can associate. So this numbering has no meaning otherwise. I mean, it doesn't matter\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"checkout counter of every one of these supermarkets, they scan the barcode. So the barcode is a number. So you can associate. So this numbering has no meaning otherwise. I mean, it doesn't matter if toothpaste is smaller than soap or bigger than soap. It's just that every item has a distinct barcode. And these barcodes are in some natural numeric order. So once we have this numeric order, the problem with writing a set is you can write it anywhere, right? I can write x, comma Y. I can write y comma X. But if I say you must write it in order, smallest to biggest, there's only one way. So I will make sure that I list out every set that I ever talk about in increasing order with respect to this arbitrary order on items.\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"So I'm just assuming that all the items have some kind of a serial number associated with them, which allows me to write out any set in a fixed order in some canonical way. So now I will look at two sets of size k minus one. So remember, what we are trying to do is that we want k size set such that every k minus one subset is an f k minus. So I'm looking at k minus one subsets which come from some k size set. So they belong to some k size set. So if they belong to some k size set and they are different from each other, they must differ in one element. If x is a subset of z and y is a subset of z, and this is of size k, and these are both of size k minus one, then I claim that these two must, if x and y are different, then between them, when you add them up, they have only k different elements. So they can differ at most one element, right? You drop one element from X and add one element, you'll get one, right? So if I look at the k minus one set, that's how I said you take x, you take\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"So they can differ at most one element, right? You drop one element from X and add one element, you'll get one, right? So if I look at the k minus one set, that's how I said you take x, you take a set of size k and you drop one element at a time and you get a k minus one set. So they all differ in only which element was dropped and which element was added back to get the other. So I'm going to say that I'm not interested in them differing in too many ways. They differ in only one element, but the element in which they differ is the last element. So if I write out X and x prime, these are two different k minus one sets. If I write out X and x prime, they differ in only one element, and that element is the last element in this specific order in which I have assumed all elements are ordered. So the serial number order of the elements or the barcode number, whatever. So the first k minus two elements in both sets are the same, and then the k minus one element, the last element, is\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"So the serial number order of the elements or the barcode number, whatever. So the first k minus two elements in both sets are the same, and then the k minus one element, the last element, is different. So now if I take the union of these, then in order it will be up to k minus two will be the same, and then in some order. I don't know. I mean, I've written it this way, it could be the other way around also. So assuming that I k minus one is smaller than I prime k minus one, then the canonical order of the union will be this. So I will call this the merge of x and x minus. So I'm only merging sets of size k minus one which differ in the last position. Last position is a well defined thing because I'm writing out every set in this ascending order in which I've assumed I can enumerate the elements. So in some barcode order. Is this clear? Just stop me anytime if something is not. In this case, the remaining k minus two element should be same always in x and x dash, only the last element\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"So in some barcode order. Is this clear? Just stop me anytime if something is not. In this case, the remaining k minus two element should be same always in x and x dash, only the last element should be different and different. Yes, exactly. That's what I was saying here. So that's what the point is. This is what I was saying. If they combine to form a k element set and they are different from each other, if the union of two sets of k minus one is k, then so therefore these two sets must be the same. Only part is different. So therefore here I get the same, these two combine to remain. So that's what I'm calling the merge operation. The merge operation is the point really is that the merge is defined only if x and x prime are k minus one sets which differ exactly in the last position. They cannot differ anywhere else. So I don't want a situation where I have I one prime, I one, and then up to the same I k minus. I don't want this to differ in any position. I don't want them to differ\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"cannot differ anywhere else. So I don't want a situation where I have I one prime, I one, and then up to the same I k minus. I don't want this to differ in any position. I don't want them to differ in any other position. So this is my merge operation. So this is a merge operation where I just have one question. Yeah. So if we define that merge can only happen on sets which only differ in the last element. So if we think about the k minus one subsets that we think of Ck from the previous Fk set, then that will be the case. That there are only two k minus one subsets that will satisfy this case, right? Exactly. So that's what I'm just going to claim, right? So the claim here is that if I now look at this as my candidate, remember what I said was that I want Ck, but I'm allowing myself to take any bigger ck prime, because I know that if Fk is sitting inside Ck, then it's also sitting inside ck prime. So I'm going to say that. So I start with what I know. What I know is the frequent sets\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"any bigger ck prime, because I know that if Fk is sitting inside Ck, then it's also sitting inside ck prime. So I'm going to say that. So I start with what I know. What I know is the frequent sets of k minus one. And then in that I do the merge of everything which is compatible, and I'll define that to be this dk. So this ck is this one. So now the question that you asked is a valid one, which is that, why do I know this? The question is, why is this the case? I must guarantee that the ck prime that I'm considering by. So what I'm doing is simple. Right now, I'm taking all the frequent sets of k minus one. So instead of going backwards, instead of first enumerating everything of size k and checking are its subsets in k minus one, I'm saying take everything in k minus one and go forwards and try to generate candidates of size k. We do this at the first level. When we say that go from, we say go from f one, cross f one to give me c two. So I want to say go from fk minus one, cross f k\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"and try to generate candidates of size k. We do this at the first level. When we say that go from, we say go from f one, cross f one to give me c two. So I want to say go from fk minus one, cross f k minus one in some sense, and give me ck prime, right? This is how I'm doing it. So rather than going backwards from k to k minus one, in which case I have to enumerate everything of size k and then filter it out, I'm saying directly only construct those things which I think are useful. So I'm starting with only, I mean, the left hand side is much smaller than the right hand side in some sense. So starting with a huge set and filtering it down and saying take the smaller set and build up combinations which are plausible. Okay, so this is what we have to claim now. We have to claim that CK is actually sitting inside Ck print. And now what you said is exactly right, which is that supposing that Ck has a candidate, I know that every k minus one subset of that Ck belongs to Fk minus one,\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"is actually sitting inside Ck print. And now what you said is exactly right, which is that supposing that Ck has a candidate, I know that every k minus one subset of that Ck belongs to Fk minus one, right? So that's my original Ck, right? Ck is all k subsets whose every k minus one subset was frequent. But in particular, these are two concrete k minus one subsets. The one where I dropped the last element and one, I dropped the second last element. So even this k subset.\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"When I write in canonical order, it has one fixed order, I one to ik. So if I drop ik, I get one set of size, k minus one. If I drop I k minus one, the second last element, I get a different subset, and if I merge those two, I get back this set. So if I have this set which y which is supposed to be in Ck, then it will actually be got by merging back these two subsets of y. So for every y that should be in Ck, it's going to be in Ck prime. Now I'm also going to get things in Ck prime which are not in Ck because they merge correctly. But there may be some other k minus one subset which is not frequent and which I'm not checking. So there will be some spurious entries in Ck prime, some things which should not be frequent because there is actually a known k minus one set which is not frequent. But I don't bother. I just am interested in saying that Ck prime is good enough for me. So the main advantage of this is that it's relatively speaking, given that our assumption is at every level,\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"But I don't bother. I just am interested in saying that Ck prime is good enough for me. So the main advantage of this is that it's relatively speaking, given that our assumption is at every level, this fks are small. This is much faster than going to the bigger set and then filtering down, building up the bigger set from the smaller set. Even if we over approximate is going to be much faster and it's not going to be too much, because we know that the smaller sets are actually quite small. So, so the, if you want to think about how to do this algorithmically. So what we have is we have our set f k minus one. So supposing we just sort it in dictionary order, right? We think of these as k minus one tuples, and we sort it in dictionary order. Meaning that if the first position where two entries differ, if one is smaller than the other one, then that tuple is smaller. The first letter in the word which differs is smaller. So that's a dictionary. So what I will have is I will have a bunch\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"differ, if one is smaller than the other one, then that tuple is smaller. The first letter in the word which differs is smaller. So that's a dictionary. So what I will have is I will have a bunch of things where I have the same I one, two, I k minus one, and then I have a different j one. Jk, jk. So I will have a block of things at the beginning of my sorted order, where the first k minus one entries are the same and the last entry is different. Then I will get to. And so this is one block. So you understand what I'm saying. These are, think of a dictionary. These are all the words that start with a. The next will be all the words that start with b. So I'm looking at this whole thing as one fixed chunk, right? So everything which starts with I one to ik minus one. The smallest such chunk which is there in fk minus one will form the first block. Now I will have a second block, so I'll have some I one ik. And in general, maybe this position is bigger. And I will have a bunch of these\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"which is there in fk minus one will form the first block. Now I will have a second block, so I'll have some I one ik. And in general, maybe this position is bigger. And I will have a bunch of these which have that start. So these are all, all the green ones have the same first k minus one elements. Then the first yellow one is bigger than that in dictionary order, so it differs somewhere. Let's assume it for simplicity. It differs in the last position. So the first k minus two are correct. But the last entry is, this should be off, right? So this should be k minus two because this was already, these are entries of only size, k minus. So the first k minus three entries are the same and k minus two at the entries. Now, the merge only happens inside a block, right? I never have to take something here and something here and merge it because they differ at two positions, not one position. They differ in the second last position. I only want things to differ in the last position. So the\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"here and something here and merge it because they differ at two positions, not one position. They differ in the second last position. I only want things to differ in the last position. So the ones that differ in the last position are actually only in the first block. So I've actually kind of divided up my large set into these blocks or bins or whatever you want to call them. And I then only have to construct pairs within each block. And within each block I construct every possible pair. I don't have to filter every possible combination within this block. So within this block, if I take any two items and merge them, any two sets and merge them, any two items and merge them, they will be this in the ck prime. So I do that block, then I do this block and so on. So this is how you would typically calculate it. And this will be significantly better than enumerating every possible thing of length k and filtering it down for every k management. So that's how this priority algorithm is\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"typically calculate it. And this will be significantly better than enumerating every possible thing of length k and filtering it down for every k management. So that's how this priority algorithm is actually implemented. Any questions about this? Sir, I had a question. Yeah. What about the elements? Or what about once in which like a second last element differs, but the last element is still the same? So where does that fall in this system? So the sets in which the second last element differs, but last element is the same. Okay, if we have a shopping list, all the elements are the same. Yeah, but you are thinking about it backwards. You think about it from here. You want something in this set, right? You should not think about it as starting from. So think about, the question is, are you counting everything that you need to count or not? Right? So if you start with something that you need to count of size k, then every k minus one subset of that must be an f k minus one. So it is true\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"everything that you need to count or not? Right? So if you start with something that you need to count of size k, then every k minus one subset of that must be an f k minus one. So it is true that there will be something possibly where the second last thing is dropped. But I don't care because there will be one where the last two items are the ones which are dropped one by one. And if those two things are merged, then I will get it back. So I will be counting it, right? So I will not miss anything. Because if you say that there is something here which necessarily requires something from this and something from that, if this is what you are saying, something which differs, say something here and something here, this is the question that you have. But then look at that set, right? So that set differs in two positions. Now if I look at the corresponding set which it generates. So maybe I should do it in a concrete.\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"Sir, I think I understood like. Yeah, but let me just do an example with three elements of. So supposing you're saying that you have X-Y-Z and x y prime z, right? And I am telling you that I am not going to count this. So let me put it in alphabetical order. Does this get counted in the second block? No. So supposing this is the thing, right? According to my calculation, these will be in two different blocks because they do not differ only in the last position. But of course, if you were to merge them, you would get wxy z, which is a valid four item. But now I'm saying that if you take this and go backwards, you should have got wxy and wx. So therefore I claim that this set should have been here. Because I'm taking every. If this is frequent, if this is frequent, then these are both frequent. And therefore I should have merged this and this. So that's why I'm saying you should not go forward. You should think of the proof backwards. That is, if I'm going to generate something of size\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"And therefore I should have merged this and this. So that's why I'm saying you should not go forward. You should think of the proof backwards. That is, if I'm going to generate something of size k, then is there a witness for it in my ck prime or not? And I claim it must be because if I take anything which is valid in Ck, then if I drop the last two elements, then those two witnesses will be mergeable. It cannot be that the only witnesses that can be generated are ones which differ elsewhere. So it is true that you might have something in this box and this box which merges, but they should also be therefore a partner for this box, this guy in the same box if it is really frequent. So that's what I mean. So just go back and digest this. So this is just a proof that you're not missing out anything as long as you generate everything that should be in Ck. Because ck is what you're counting what you want to count. As long as you generate everything that should be in Ck, you're fine. And\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"anything as long as you generate everything that should be in Ck. Because ck is what you're counting what you want to count. As long as you generate everything that should be in Ck, you're fine. And that's what we are. So this gives us now a kind of refinement of that prayer algorithm. Just to make it explicit, I can think of c one, the candidates of size one as everything, all the singletons, right? And f one is just saying count everything in c, one whose count is above the threshold, sigma times n. And for every other k, you merge the subsets that you have already found. So, in the case of the first case, if I look at x and y, so this is of size k minus one. So I'll basically merge these. So one to k minus two is nothing. So it's a kind of a trivial case of that. So the merge of this is just x comma one. So if the k minus one is actually the only position, then if I merge them. So when I do this merge, what happens is that these two parts are the same, and then I add these two. So\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"just x comma one. So if the k minus one is actually the only position, then if I merge them. So when I do this merge, what happens is that these two parts are the same, and then I add these two. So in the one element case, there is nothing from one to k minus two. The only element that is there is the k minus one. That is the last element, right? So you get x comma Y. So even that pairs is a special case of this merge, where I just take everything in f one and pair it up with everything else in f one. But in general, as I go up, then it will start making sense. If I have three elements, then we are saying you take every set of the form, say for instance, you look at x comma Y, next comma Z, and you merge this to xyz. But I will not look at x y and y z. So this I will not look at, though they form an xyz, they don't differ in the last. So if xyz is indeed frequent, all three of these sets will be there in my f two. But I will only look at the ones which share the first element. So I\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"they form an xyz, they don't differ in the last. So if xyz is indeed frequent, all three of these sets will be there in my f two. But I will only look at the ones which share the first element. So I will look at Xyxz and merge them to get xyz. I will not look at X-Y-Z-Y-Z will be compared to y something else, but not to x y. So I keep doing so. That's basically my priority. So this is fine. So we do this for one, two, three. But what about this question is, how long do we, when do we stop? For our example, it is ten. So, one obvious thing is you cannot have a frequent set which is bigger than the largest transaction, because if not more than so many items. So we said ten uniformly. But if you give me any concrete shopping data set, every transaction is going to be a finite transaction. So maybe one customer bought 100, but still, therefore no set is going to be bigger than 100. That's the largest transaction. So there's always going to be a largest transaction. So that's one\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"So maybe one customer bought 100, but still, therefore no set is going to be bigger than 100. That's the largest transaction. So there's always going to be a largest transaction. So that's one possibility. So once you exceed the size of the largest transaction, you don't have any possibility of generating, because there are no, that size set is not there. The other possibility is that at some point, and this is intuitively going to happen, right, that as the number of items becomes larger, the possibility that they were all bought together is going to shrink. So in general, the number of items, number of sets, number of frequent sets is going to drop as the size increases. Because why would somebody be buying eleven items together and twelve items together very frequently? Not that they don't buy it in isolation, but frequently it's very unlikely. And the moment you hit a situation where there are no frequent sets of a particular size, then obviously there cannot be any frequent sets\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"buy it in isolation, but frequently it's very unlikely. And the moment you hit a situation where there are no frequent sets of a particular size, then obviously there cannot be any frequent sets of a bigger size. So this is more likely to be the stopping point than the other. And another thing, as I said, is that you can also stop.\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"So typically, if you are looking at this from a perspective of offering a sale on items or something like that, you're not going to be, I mean, you look at anything that you see in the newspaper, like big Bazaar or something, it'll say buy five kilos of Ata and you'll get one liter of oil free. Or it says buy two kilos of sugar and you'll get some, one kilo of salt free. So typically these type of cross selling or whatever you do will not be on some humongous thing. So there might be a basket, buy Ata and salt, and then you'll get something else. So you might be only interested in sets, usually of size small, two, three, four. After that things may be frequent, but you really don't care because you can't take any action. So there might be a practical consideration in addition to these theoretical. Although in principle this algorithm does not have a natural termination criterion, there are exact criteria based on the size of the sets. But also there might be a situation where you will\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"Although in principle this algorithm does not have a natural termination criterion, there are exact criteria based on the size of the sets. But also there might be a situation where you will just stop so you won't run this. So this is going to take time in addition to the time to compute at each set, each time, that pairwise thing that I said, that merge operation, I have to do one scan of the transactions, but I'm doing a kind of number of scans I do. The transaction is smaller. It's not like size of the transaction squared or something. It is some fixed number like five times the number of transactions. So these things are acceptable, right? So you can do some, because number of transactions is large. So you don't want to do something which requires you to do size of t times size of t. This is going to kill you. But if it is some c times size of t for small c, this is okay. So that's what we are promising now, that you will make some number of scans of your because every time you go\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"is going to kill you. But if it is some c times size of t for small c, this is okay. So that's what we are promising now, that you will make some number of scans of your because every time you go from k to k plus one, you have to scan the transactions one more time. But you'll do that only for small numbers of k, so you won't do it too often. Okay, so what we have achieved right now is part of our problem, right? We have found all the frequent item sets, but our goal was to find these actual association. So we have one more step to finish this market basket analysis, which is to. So we know that any association rule that we construct must add up to a frequent side. The left hand side plus the right hand side must be frequent. Otherwise it was not worth considering. But now how do we find the left hand side and the right hand side? So what we have done is this part. So we have ensured that we have found all the z which satisfy this. And now we have to actually do this part, decompose\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"the left hand side and the right hand side? So what we have done is this part. So we have ensured that we have found all the z which satisfy this. And now we have to actually do this part, decompose into x and y. So what we have done in our priorities, to find z such that z dot count is bigger than sigma times n. But now z has to be decomposed into a left hand side and a right hand side. So what we can do, of course, now if brute force thing would be okay, pick every item set that you have found, break it up. Remember, x and y must be disjoint. So you break it up into x and y such that x and y are disjoint. And check whether that x union y dot count divided by x dot count exceeds this threshold. This support level, as it's called confidence level. So it has enough reason to be a valid group. So this again is not very appealing, because anytime you have to enumerate every possible subset of a set, that's an exponential calculation within that set. So even though these z's are going to\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"So this again is not very appealing, because anytime you have to enumerate every possible subset of a set, that's an exponential calculation within that set. So even though these z's are going to be typically small, still it's a lot of work, right? If we take a ten element set, then I have to enumerate all possible ways of breaking it up into smaller pieces. So maybe I want to do something better. So first of all, this is not very clear, but because the x intersection y is empty, first of all, they cannot have any overlap. And the claim is it's enough to take z and just divide it into two parts. So supposing the rule is such that this is not all of z. So supposing there is a part which is left out. So I have a rule of the form x implies y and x and y together sit inside z and z is frequent. So in this case, x is not x and y do not partition z because there's this red part which is. So partition means that they are disjoint subsets which add up to z. But I claim that if this is indeed\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"So in this case, x is not x and y do not partition z because there's this red part which is. So partition means that they are disjoint subsets which add up to z. But I claim that if this is indeed the case, then that picture which I had. Right? So now in this picture which I had, if I look at this, this part, this is some z prime. So x plus y, x union y is a smaller set, z prime, but z prime is sitting inside z. And because of the priority rule, if z was frequent, every subset of z will be frequent. So z prime will also be frequent. So in other words, if I'm doing this for every frequent z, I will also do it for Z prime. So wherever I actually have the rule x implies Y for the set which is x union y, I would have been doing. What we are saying is that if I pick up every candidate that is possibly a rule, it adds up to a set z. It's enough to partition that z as left hand side and right hand side. I don't need to look at sets which are smaller than a partition. That's the first\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"is possibly a rule, it adds up to a set z. It's enough to partition that z as left hand side and right hand side. I don't need to look at sets which are smaller than a partition. That's the first observation.\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"Okay, so now supposing. So this is the notation. So this is called disjoint union. So X and Y are disjoint and their union is z. Right? And suppose that there is a rule of the form X implies Y that is valid. That is, it meets that threshold. So now what happens if I take something from the right hand side and move it to that? So I'm taking a Y from there and moving it here. So now I've got a different rule. In some sense, I've taken a rule and I've modified it. So remember that. So if you want to take me to the pictures. So this is my situation. So I have X and Y. And now what I'm doing is I'm taking a Y here and I'm adding it to z. So I'm making this x union Y. I make this Y minus. So I'm partitioning it differently by taking something from the right hand side and moving it to the left hand side. And my question is, is this also a rule or this is not a rule by rule, meaning, of course it's a rule. But is it a valid rule? Does it meet the required count? So we know that the previous\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"hand side. And my question is, is this also a rule or this is not a rule by rule, meaning, of course it's a rule. But is it a valid rule? Does it meet the required count? So we know that the previous rule is valid. That means that this x union Y dot count divided by X dot count has become. So this is what it means to say that this is a valid. For the second one, the left hand side is x union y, but the numerator is x union y. It's the left hand side plus the right hand side. Right? But whichever way you do this, this whole thing adds up to the full set, right? So this is just x. I've just transferred the small y from the right hand side to the left hand side, but it adds up to the same. So the numerator is the same. So I know that x union Y dot count divided by x dot count meets the threshold. My question is, does xunion Y count divided by x plus the element y? The larger set dot count meet the threshold? Remember, I'm not telling you anything more about that small y, right? It's some\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"My question is, does xunion Y count divided by x plus the element y? The larger set dot count meet the threshold? Remember, I'm not telling you anything more about that small y, right? It's some arbitrary small y that I moved from one side of the thing to the other side. So, will this meet the threshold? Yeah, it is very true. So y. Yeah. So x union y, the count of X union Y should be greater than the count of X, right? Should be greater than the count of X, should be lesser than, should be lesser than the count of X. And why is that? Very prime. The set increases. So every time I see x union y, I will see x, but sometimes I will see x without seeing x union Y. Right? So therefore, this is the same thing as earlier we said numerator, denominator. That ratio is going to be less than y. So the ratio of the bigger set to the smaller set is going to be less than one. Every time I see x union, small x union, the element y, I will certainly see x. So that count is going to be at least as\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"the ratio of the bigger set to the smaller set is going to be less than one. Every time I see x union, small x union, the element y, I will certainly see x. So that count is going to be at least as big and it could be more. So x dot count is going to be bigger than or equal to x union y dot count always. So therefore, this numerator, I mean this denominator is bigger than this denominator. So the denominator is decreasing. So as a fraction, the value is increasing. If I take one by four and make it one by three, the fraction value increases. So it's going to exceed the first one exceeds the threshold. Second one is also going to exceed the threshold. So this is that if I have a valid rule, if this is valid, and if I take something from here and move it there, then the resulting rule is also valid. Okay? So now this tells us that if you look at it the other way, that if something is not valid, then I cannot take something from the left hand side to the right hand side and make it\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"is also valid. Okay? So now this tells us that if you look at it the other way, that if something is not valid, then I cannot take something from the left hand side to the right hand side and make it valid. So if I have this is not valid, then if I remove this is also not valid. Because if the second half after moving x from left to right, if it were valid, then moving x from right to left will also keep valid. Moving from right to left does not disturb the validity of a rule. So therefore, if something cannot be is not valid, then I cannot take anything from the left hand side to the right hand side and make it valid. Okay, so the way you use this is by saying that, remember, the problem is that we need to enumerate, even though we now know it's partitions, we still need to enumerate a lot of different partitions of a set, different ways of splitting a set into two and checking them. So what we want is a systematic way of doing that. So just to look at this appropriate again, so we\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"lot of different partitions of a set, different ways of splitting a set into two and checking them. So what we want is a systematic way of doing that. So just to look at this appropriate again, so we start with singletons. So basically what this is saying is that the appropriate thing earlier was saying that bigger sets in general must be made up of smaller sets which are frequent. Here what we are saying is right hand sides of rules which are bigger must have been justified by right hand sides of rules which are smaller. If I have a right hand side of a rule which is bigger, then something was added to it. And where was it added from? From the left hand side, because the whole thing together adds up the side. It's always a partition. So if I add something to the right, the only way I can add it to the right is to remove it from the left. So I'm using that left to right shift. But if the original rule was not good, this rule cannot be good. So if I want to build up a bigger rule,\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"can add it to the right is to remove it from the left. So I'm using that left to right shift. But if the original rule was not good, this rule cannot be good. So if I want to build up a bigger rule, bigger in the sense that the right hand side has more elements, I must necessarily have had the rule with fewer elements already in my set. So if I want this rule, for instance, if I want x implies x comma y, then I either got this by moving the x small x to the right or the small y to the right. If I use the small x to the right, then before this I had this rule. And if I move the small y to the right, then before this I had this rule. So both these rules I would have already checked because they have a smaller right hand side. So both of them must be there in order for this rule. So this is very similar. You can see, right? I'm saying that two element subset on the right has to have corresponding one element subsets on the right. Once I fix what's on the right, the left is fixed, because\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"similar. You can see, right? I'm saying that two element subset on the right has to have corresponding one element subsets on the right. Once I fix what's on the right, the left is fixed, because the left is just z minus what's on the right. So you can think about this.\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"I'm not going to formalize this anymore, but it's very similar in idea to the prior year. So you can formalize this algorithm for yourself, but it's essentially saying the same thing, except here the thing is constraint, because we know that z is fixed. So once z is fixed, then we just have to start with each individual element in z as a possible right hand side, and then work forwards to that. We don't have to look at all. So for each z, we do this calculation within that set. So that's how you generate association rules once you have a frequent set. So within that set, you play this a priority game with respect to the size of the right hand side, and then you generate all the rules. Is this clear? Yes. So it's exactly the same as the previous one, just a slightly different presentation of the idea. Okay, so now, as I said, this whole thing so far has been largely a kind of algorithmic problem, right? In some sense, if I fix the set of items and the set of transactions and the set of\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"the idea. Okay, so now, as I said, this whole thing so far has been largely a kind of algorithmic problem, right? In some sense, if I fix the set of items and the set of transactions and the set of the two thresholds, the support and the confidence threshold, then for any x, comma y, either it is true or it is not true. So there is a fixed answer. So the answer is known. So this is something where I can actually validate that you have either enumerated all the association rules or you have missed out something. So technically there is no learning involved. It's more an algorithmic question. So how does one connect this to learning? Well, in specific context, you can interpret these rules as classification rules. So, remember we said supervised learning says, if this is there, then it is a fraud. If this thing is not there, then it is not a fraud, and so on. So, let's look at a typical example. So, supposing we have some question which is about labeling the topic of a document. So our\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"fraud. If this thing is not there, then it is not a fraud, and so on. So, let's look at a typical example. So, supposing we have some question which is about labeling the topic of a document. So our items now are words, and our baskets are collections of words, sets of words. So for each document, we have one word which describes its topic. So here is an example. On the right. So the topic words are education and sports. And the other words that appear on the document are like students, school, city spectator, and so on. So some words have been discarded. Maybe there are common words discarded. But with respect to some set of words which we are interested in tracking, what we see is that a word document which has the word student, teach and school in the first line says that this document. There is a document which has these words and it has education. Now, another thing which has teach and school, but it has two different words. City and game is also education. But here, for\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"document. There is a document which has these words and it has education. Now, another thing which has teach and school, but it has two different words. City and game is also education. But here, for instance, we have city and game, but it doesn't have teach and school. It has football and team, and it's about sports. So this is now our transaction database, except that there is a kind of separate category within each transaction. So, each transaction has words of the generic words, the words that make up the document, and a special word which is the topic of the document. So the documents consists of regular words and topic words. And each transaction consists of a set of regular words and one topic word. And now we are looking for these kind of association rules where the right hand side is one item and the left hand side is a set of items. But the right hand side is always an item, a singleton item which consists of a topic word. So what this rule is saying is, if the document\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"side is one item and the left hand side is a set of items. But the right hand side is always an item, a singleton item which consists of a topic word. So what this rule is saying is, if the document contains student and school, then it also contains the word education, namely its topic is education. And if it has game and team, then it is. So this is a special kind of association rule, where the right hand side of the rule is always a singleton, and it corresponds to a special type of item which is a category, in this case a topic. So when you have a table like this, the rows may have different number of entries. Or if you want to think about it as a table, then some columns may be blank. So not all columns are filled, but there's one last column which is filled. And now you want to now construct these rules corresponding to the first k minus one columns, implying the last column. And this now becomes something where these rules that you get out of this give you a kind of classifier.\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"these rules corresponding to the first k minus one columns, implying the last column. And this now becomes something where these rules that you get out of this give you a kind of classifier. So this is a kind of a simplistic way of doing supervised learning. So that kind of brings me to the end of this basic treatment of classes, this market basket analysis. So what you're really looking for is correlations between sets of items across transactions, which we formalize in terms of these association groups. X implies y. And what we saw is that as such, combinatorially, this is a very complicated thing to calculate, given the sizes of the sets involved. But we can use this appropriate principle both to identify the sets which are interesting, those which are frequent enough to be considered, and also to decompose them into rules. So you use the approoriate twice, once to find the frequent z, and second to decompose each frequent z into possible x implies y. And using this, we can build\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"and also to decompose them into rules. So you use the approoriate twice, once to find the frequent z, and second to decompose each frequent z into possible x implies y. And using this, we can build our first simple supervised learning model. So these rules now become a very simple way of prediction. Now, if you give me a document, I will look at what words are there in the document in this previous example. And if it contains the word student and school, I will predict education. If it contains game and team, I'll predict sports.\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"Now, of course, if it has all four of them, all four of these words, then I have to decide. So either I have to say that it is one of two or I have to give some priority or something. So it's not a very refined model in that sense, but it is a model. So you can imagine how you can predict. Now you get a new document, you check the words that are there and you see which rules apply. So I am not going to get into these refinements. But you can think of, you can. So last time somebody asked about multiplicities, another related refinement, which is not quite, but if it is not a set, but a sequence. So supposing I say that somebody who buys a computer and buys a router is likely to later subscribe to an Internet plan, it makes sense. Why would you buy a computer and a router if you're not going to do that? But there is sometimes a sequence involved, right? So you're not going to buy it all in the same transaction. So you're going to do something and then something and then something else.\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"to do that? But there is sometimes a sequence involved, right? So you're not going to buy it all in the same transaction. So you're going to do something and then something and then something else. So that's a sequential thing. So that's a different type of prediction, which is also interesting, which also sometimes can be addressed in this market basket thing. And then the other last thing is that not all items must, may be of equal importance in terms or equal value if you think about it in terms of supermarket. So if you set your thresholds too low, if you are trying to catch low frequency things, say for example, things involving expensive items like say pressure cookers or something like that, then you will trigger a lot of spurious rules involving day to day items. People who buy milk also buy sugar. People buy milk also by butter. People buy bread, also by eggs, which are not epidemically useful. On the other hand, if you set your threshold to catch only these very high\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"buy milk also buy sugar. People buy milk also by butter. People buy bread, also by eggs, which are not epidemically useful. On the other hand, if you set your threshold to catch only these very high frequency items, then all the low frequency things which may have high value to you, you may want to really make offers on expensive items. You will not see them. So sometimes you might want to work with this market basket analysis where you assign different thresholds to different items. So if it is pressure cooker, you want to say something. If it's something else, you want to say something else. So you have not one sigma, but you have a different sigma for each item. But then the problem is what happens with mixed baskets. If I have a basket in which I have a high value item and a low value item, and they have two different thresholds, if I buy butter along with the pressure cooker, then how should I measure the frequency of this particular basket? Should I measure it with the butter\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"item, and they have two different thresholds, if I buy butter along with the pressure cooker, then how should I measure the frequency of this particular basket? Should I measure it with the butter frequency or with the pressure cooker frequency? These are discussed in that book where I have given you the reference for the first lecture and for this lecture, so you can read about it. It's not particularly relevant to the work that we want to focus on in this course, so I won't get into it in any detail, but there is a very accessible introduction to that, in that chapter later on. So you can look at it and see a lot of it is some kind of heuristics, but you can look at it. So I'll stop with this. Any questions? Is there any way to predict how better our association rule is? So that is a general problem that we will come across. There is no real good way to predict how good it is. You can, of course, associate with a concrete data set how reliable it is for that data set. But this is\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content='general problem that we will come across. There is no real good way to predict how good it is. You can, of course, associate with a concrete data set how reliable it is for that data set. But this is going to be one of the challenges, as we will see going along, which is that you are always building rules or any model with respect to a fixed data set, and you are going to apply it to some other data set which you have not seen. So will that generalize well or not is the biggest question in machine learning. How do you know that the training data that you have used to build your model is actually representative of the data that you see in the real. And there could be many reasons why it is not valid. It could be because of sampling. So, for instance, you might be building a model for some consumer behavior by looking at one segment of. Maybe you are looking at the behavior of people who are in their, are building a model of how people behave, but your actual market consists of people', metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"consumer behavior by looking at one segment of. Maybe you are looking at the behavior of people who are in their, are building a model of how people behave, but your actual market consists of people who are maybe in their 40s, who have a very different way of dealing with things. So you need to be careful that the data that you're looking at is actually representative. So this is a huge problem and there's no correct answer. You can measure it, but you cannot. In some sense, if you could validate it, then this whole problem becomes much simpler. But the whole difficulty with machine learning is exactly this, that there is no sensible way to predict the behavior or even when it will fail. Even if you could say that this is likely to work well here and not likely to work well there that itself would be a huge improvement, not even necessarily a very precise quantification, but just some indication. But all these are very speculative. So that's a huge challenge. So there are some\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"itself would be a huge improvement, not even necessarily a very precise quantification, but just some indication. But all these are very speculative. So that's a huge challenge. So there are some theoretical things you can say which we may not, may or may not get to in this course. But it's a big problem. This is the so called generalization problem. So that's what basically, if you build association rules, is it going to hold or not? I don't know.\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"Okay, sir, I had a question. We have been talking about items and transactions, but we haven't anywhere spoken about whether one customer, suppose if you're talking about somebody buying groceries, whether the behavior of one customer is what we also trap. So for example, what I mean is like you gave the example of a person buying a computer and a modem, and then they are most likely to buy Internet. So here we need to trap the behavior of the customer. So when I said sequences, it implies that there are connected transactions. Right. As you said, involving one customer. So we have not done it. I agree. So what we have here looked at is aggregate behavior. And we have ignored whether all the customers are different or whether some customer is coming repeatedly. So we are really looking at it anonymously in that sense. So we are not connecting any one transaction to any other transaction. Every transaction is independent. Yeah. So every transaction essentially could be a different\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"at it anonymously in that sense. So we are not connecting any one transaction to any other transaction. Every transaction is independent. Yeah. So every transaction essentially could be a different person. So we are really looking at some kind of behavior of a population as a whole without tracking subgroups or sub, we don't have any identification of transactions belonging to other categories in terms of who it is. So that's right. So if you wanted to do that, then you have to do something else. So that's what I meant. So when I meant sequences in this particular thing, one of the things it includes is a fact that there is some kind of a linkage between the two. Right. There are transactions which are kind of connected and we want to track in some sense the sequence of those transactions, or there are a bunch of transactions which have come together, and then you want to look at each of these groups as a whole and do some kind of. So not just one consumer, but across consumers, you\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"or there are a bunch of transactions which have come together, and then you want to look at each of these groups as a whole and do some kind of. So not just one consumer, but across consumers, you want to say, as a rule, consumers who do this also do that and so on. So these are all refinements. And there are many situations where these are important and interesting, but this model as such, this kind of market basket analysis approach is very naive. And so that's why these things will come up in different contexts. But we are not going to explore it anymore in this context, because in this context, the kind of model that you build, this association rule model is rather simplistic. So we are going to look at more sophisticated models as we go along. And there you can ask the same question, but it'll take a slightly different format. Does this also include tendencies like, if there is one person who tends to buy all his groceries once a month, then they will have a bigger basket size.\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"but it'll take a slightly different format. Does this also include tendencies like, if there is one person who tends to buy all his groceries once a month, then they will have a bigger basket size. But for people who buy their groceries, say like per week or every other day, they will have smaller basket size, but their transactions would be spread out. So that will be treated as a different transaction every time. But for somebody who's buying everything together, doesn't that give rise to a lot of. Yeah, so there are all these situations, I agree, which are not directly addressed in this. There are many different variations of this that you could ask. So there's no doubt about that. So some of them people have looked at because they have kind of natural solutions in this. Some of them maybe you cannot do. So that's another thing about this whole model building thing is that the same model may or may not be capable of tackling every different question that you ask. So this model,\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"maybe you cannot do. So that's another thing about this whole model building thing is that the same model may or may not be capable of tackling every different question that you ask. So this model, some of these things maybe you can segregate and answer. But if, as you're saying, you want to compare behaviors across customers of different types, you cannot, as you said, sensibly aggregate them into a single market basket model because they all have different profiles. So you will have to, in the example that you gave, you would have to first separate out the data for these, say, the people who buy one month groceries at a time from those who buy at more frequent intervals and then analyze them separately, because together the data doesn't actually make sense. So last time, if you remember, I talked about this unsupervised. So maybe one thing that you need to do when you have something large is to basically look at the transactions and categorize them according to some criteria, maybe\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"I talked about this unsupervised. So maybe one thing that you need to do when you have something large is to basically look at the transactions and categorize them according to some criteria, maybe by the size of the basket. And then you will find that there are maybe a lot of people who buy five items at a time. There are people who buy ten to 20 items at a time. There are people who buy 40 items at a time. And maybe you need to then apply the model separately to each of these clusters. So all of these things are not a one size fits all solution. And also, it's not that one approach on its own is going to solve the problem at any given time. So you need to have a kind of think of it as a toolbox, and then you need to apply some combination of tools for a given problem based on what you can do about analyzing the problem. So what we are looking at is the question of what these tools are. But they will not work out of the box in some sense, in every situation, like you said. So that's\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"do about analyzing the problem. So what we are looking at is the question of what these tools are. But they will not work out of the box in some sense, in every situation, like you said. So that's a valid point. Okay, so, thank you. Okay. I've kept you long enough, so see you on Monday.\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content='So we were looking at this market basket analysis, and towards the end of the discussion on association rule rules, if everybody who buys X is also likely to buy y. We said that one application of this association rule idea is to have these so called class association rules. So the right hand side was a category. We looked at an example with documents and we said, if a document has these words, it is also likely to belong to this category. So that brings us to the topic that is one of the central ones in machine learning, which is the idea of supervised learning. So in supervised learning, we have a set of items, and each item belongs to some category or class, and we distinguish these items in terms of some attributes. So you can think about this, as we will see shortly, you can think about these attributes as columns, right? So the attributes will be things like name, age, date of birth, whatever, income, stuff like that. So each row in the table will be one item, and there will be a', metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content='these attributes as columns, right? So the attributes will be things like name, age, date of birth, whatever, income, stuff like that. So each row in the table will be one item, and there will be a special column which indicates what category, depending on the classification problem that we are dealing with, what category it is. And the goal of supervised learning is to now construct, given a new row, a new item, a new combination of these attributes, which is not seen before, try to predict what would be the most appropriate category to assign. So these items which are given with the labels. So these are called labels. So these items are called training data. So we have labeled training data, which consists of some information which has been collected in the past, some historical information about items and their classification. And we want to find a model that generalizes the training data. So since normally this categorization corresponds to some kind of classification, this is', metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"about items and their classification. And we want to find a model that generalizes the training data. So since normally this categorization corresponds to some kind of classification, this is typically called a classification problem. So classification problem could be a binary problem. It could be, yes. No. This mail is junk. This mail is not junk. It could also be multi way, especially when we are thinking of what we did last time with the class association rules. For instance, where you're trying to assign a topic, topic will be one of many different types of topics. So there may be four or five or six or ten, depending on the diversity of topics that you want to include. But of course, you can always do multi way classification by doing it two at a time. You can say, is it category one or is it not category one? Then if it's not category one, you can say, is it category two or is it not category? So you can basically simulate by a cascading choice, right? So you could say not C\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"one or is it not category one? Then if it's not category one, you can say, is it category two or is it not category? So you can basically simulate by a cascading choice, right? So you could say not C one, then not C one. You would say C two or not C two and C two. You would say C three, and maybe there are four classes. So if it's not C one, not C two, not C three, then it must be C four. So by doing a sequence of these classifications, you can do it. Now, of course, this is not necessarily the most efficient way. It would be nice if you could actually build a one shot classifier to do all these four things in one time. But from a theoretical perspective, to understand how these models work, we will start by assuming that all our classification is binary. That is, we want to classify something as being class one or not class one. And later on, we'll talk about extending it. So, as I said, the best way to think about this training data, especially when the training data is uniform, is\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"as being class one or not class one. And later on, we'll talk about extending it. So, as I said, the best way to think about this training data, especially when the training data is uniform, is uniform, meaning all the items are similar, is in terms of a table. So the rows in the table are the items. So here we have 15 items, and the columns are the attributes. So here we have five columns, and the fifth column is special. So this column is actually the category. So it's either yes or no. So it is what's called the class label. And the other four items describe the data. So this is supposed to be some hypothetical small for a loan data set. So here what we have done is collected four types of information about each applicant. Their age, whether they have a job, whether they own, possess a house, and what is their credit rating. Credit rating is a kind of long term evaluation of this person's past history in terms of finance and repaying loans, et cetera. So one of the things that you\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"house, and what is their credit rating. Credit rating is a kind of long term evaluation of this person's past history in terms of finance and repaying loans, et cetera. So one of the things that you can see here is that some of these values, so it is very clear that these are boolean. So there's no problem. Either you own a house or you don't own a house. Either you have a job or you don't have a job. So these are poor faults. But here, for instance, we have this aggregation, right? So instead of saying precisely that somebody is 45 years old or somebody is 72 years old or somebody is 32 years old, we have grouped these age values, which are logically numbers, into these three categories. So we will talk about this little later in this discussion about how you can go from numerical values to these categorical values. But for the moment, it's simpler to work with these categories because you only have a kind of finite range of choices.\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content='So here there are three choices. For the first column, the age could be one of three categories. Young, middle, and similarly have multiple categories. We have fair, we have good, we have excellent. So we have fair, good and excellent. So there are three values. So the first and the fourth column are attributes which have this three choices. In general, you could have some finite number of choices. And the second and third column are actually boolean values. So again, you have finite number of choices. Just the choices are two or false. And so now what we want to do is really build a model which kind of tells us if I get a new row which says somebody is, for example, old and does not have a job and does not own a house. So this, for example, is a row which is not old, does not have a job, and does not own a house. Now, you can see that in this particular segment, which is old, there is somebody. So the last row actually has, maybe we say, does have a job and does. So this is not', metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"job, and does not own a house. Now, you can see that in this particular segment, which is old, there is somebody. So the last row actually has, maybe we say, does have a job and does. So this is not right. So if you look at these five rows, which correspond to old, the combination true. True is not. There's. Maybe this person has a fair credit rating. Now, the question is, as per this historical, whatever decision process this data set has been following, would this person have got a loan on? So that's really the prediction problem. So I give you a new item which corresponds to a new combination of values in these columns, and then I want you to predict the class column. So, just going forward, we will need this information. So there are 15 rows, and if you count the yeses, 123-45-6789 so there are nine yes in this specific data set, there are nine yeses and six no's. So, I mean, if you think that this is a representative sample of the applications processed by whichever bank or\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"so there are nine yes in this specific data set, there are nine yeses and six no's. So, I mean, if you think that this is a representative sample of the applications processed by whichever bank or organization is handing out these loans, then you might infer from this that more than 50% of people who apply get a loan. So maybe there is some other criterion which prevents people who are not going to get a loan from actually being considered. Maybe this is after some. But here it really looks like some 60%, nine out of 15 or 60% get a loan. So this is one of the issues that we have to deal with, which is that this training examples that we are going to base our model on should be somehow indicative of what we are going to predict. If we are going to take data pertaining to one group of individuals and extrapolate it to a completely different profile of individuals, then it's unlikely that this model that we built has any relevance. So there is this fundamental assumption. So we are not\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"and extrapolate it to a completely different profile of individuals, then it's unlikely that this model that we built has any relevance. So there is this fundamental assumption. So we are not going to make this assumption mathematically precise in this course. So, in the advanced machine learning course, we will talk about a kind of theory of learning in which you can make this thing rigorous. But we are going to assume that there is a correlation between the distribution of training examples and the distribution of unseen data. So these are probabilistic, of course. Even if you have a similar set of data, you could draw samples which omit a few cases and have a few other cases. So it's always going to be a kind of probabilistic argument. But what you're saying is that this is a representative sample. So you're not saying that the examples have every possible situation that you are likely to use for judgment, but you're really talking at the level of probabilities. You're saying that\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"sample. So you're not saying that the examples have every possible situation that you are likely to use for judgment, but you're really talking at the level of probabilities. You're saying that the probability distribution of the training is somehow identical to that of the unseen data on which you are going to make the prediction. So this is going to be your training and this is going to be your prediction. So this is a fundamental assumption. If this assumption does not hold, then whatever you try to do doesn't really have any basis. So this is something that we have to assume, and we will talk about later on what this assumption means. Maybe not in this course, but in later courses. But for now, let us assume this. So the next question is, what does it mean to learn? Right? So, as we saw in this loan data set, 60% of the loans were granted, so nine by 15. So if I were to just say yes, supposing I don't have an interesting model and I just say yes all the time, every answer that\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"saw in this loan data set, 60% of the loans were granted, so nine by 15. So if I were to just say yes, supposing I don't have an interesting model and I just say yes all the time, every answer that every person who comes, I just say yes, give this person. And if the previous assumption is actually valid, that the distribution of training examples and the distribution of unseen data are actually the same, then if I used to give 60% of the training people loans, then giving 60% of the unseen data loans is something that I would expect to do. So if I just keep on saying yes, then 60% of the time I would be correct, which might suggest that you're doing better than half, but actually the answer is biased, so you're not doing any better than random guessing. It's like if I toss a fair coin and I keep saying heads all the time, then I should expect to call it right 50% of the time. So doing 50% of the time in a fair situation is no better than random guessing in this kind of situation,\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"and I keep saying heads all the time, then I should expect to call it right 50% of the time. So doing 50% of the time in a fair situation is no better than random guessing in this kind of situation, where you know that the answer is biased because the trading data tells you that you have more guess than no, then if you go for the majority answer, then you will obviously do better than 50%, but you're doing no better than. In some sense, this is the same as random, so you should actually do better than that.\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"And ideally your performance should improve with more training data, which will not happen. In this trivial example, if I give you 30 cases instead of 15 cases in the previous. If I give you 15 more rows, and again they have 60%, your performance will still be the same. But maybe you see different combinations in those 15 new examples, which should give you a better note. So we really can think about learning as doing better than random guessing. Now, another big issue with this whole problem is that what we are building the model on and what we are going to test it on are two different things. So we would like a model which does well with respect to the training data. So if you think about it as a situation where you want to minimize the errors, some kind of quantity, right? So you have a model which makes some predictions with respect to your training data, and you have the actual training data answers. So in a way, you would like to make these match as closely as possible. So it's\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"a model which makes some predictions with respect to your training data, and you have the actual training data answers. So in a way, you would like to make these match as closely as possible. So it's a kind of optimization problem. You want to minimize the discrepancy between what your model does and what your training data tells you is the true value. So this is all possible for the training data because we know the answers. But when I move to a situation where I'm predicting the answers, the whole situation changes. Because if I could tell you what the right answers were, I wouldn't have to use this approach. So if I had an algorithmic way of telling you that this person should get a loan and that person should not get a loan, then I did not need not build a data driven model for it. I could just apply whatever algorithm the algorithm might just tell me that if this person owns a house, give them a loan, or this person has a job with a salary beyond something, give them a loan. So\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"could just apply whatever algorithm the algorithm might just tell me that if this person owns a house, give them a loan, or this person has a job with a salary beyond something, give them a loan. So this would be a very direct way of characterizing these things. And for these, I will not have to look at past patterns or past data in order to do something. So since I don't have the correct answers, what do I say? Whether the algorithm that I have come up with, that is the model that I have built, is it a good one or it's not a good one, right? So there's a fundamental problem here. So I just want to emphasize that normally when I build anything using software, you build any kind of a program, you have an expectation that this will, if given this input, will give you that corresponding output. So there is a correct output that you expect for every input, and then you can validate whether or not the program does it by feeding it different inputs of different types and seeing that it\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"output. So there is a correct output that you expect for every input, and then you can validate whether or not the program does it by feeding it different inputs of different types and seeing that it gives the expected output on a. But here the problem is that this expected output is not something that you know how to compute, and yet you need to make some statement, obviously, as to whether the model is good or bad, because if you can't validate that your model is doing well, you have no basis to use it. So this will be another issue that we will take up. How do we deal with this problem of evaluating performance when actually we don't have a yardstick measuring standard in some sense? So please stop me at any time. So if you think something, I'm going to get into a specific module very soon. So what we are going to look at are different. Start with very simple models. So what I'm going to do today is something called a decision tree. Then we will look at probabilistic models, and\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"soon. So what we are going to look at are different. Start with very simple models. So what I'm going to do today is something called a decision tree. Then we will look at probabilistic models, and then we will start looking at models which are based on a geometric idea. So you think of these items or these columns as coordinates. So you can think of these items as points in some high dimensional space, and you're trying to actually separate them by something which separates categories a from category, not a. So that will get us to more complicated models, starting with something called a support vector machine. And then later on, of course, the model that we most commonly find these days, which is neural networks. So these are the different types of models that we will consider during the course of this particular DM. So the kinds of issues that we need to address in the context of these models is how to evaluate them. The fact that there are many models already tells you that there\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content='course of this particular DM. So the kinds of issues that we need to address in the context of these models is how to evaluate them. The fact that there are many models already tells you that there is no best model. So for different purposes, different models are preferred. Then you need to solve this generalization. So, generalization basically says that the models do well on unseen data. See, here is a trivial model, which is a. So, I just memorized my training data, right? So I just take that table that I gave you before those 15 rows, and I just memorized it. So I just put it into, say, for want of anything better, I put into a python dictionary where the first four columns are the key, and the value that I store with that key is the class label, which is a table text. So now, whenever you give me something from the training data, I just look it up in the dictionary, and I always give you the right answer. So, in this sense, I can minimize my discrepancy of the model with respect', metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"you give me something from the training data, I just look it up in the dictionary, and I always give you the right answer. So, in this sense, I can minimize my discrepancy of the model with respect to the training data very trivially, by just exhaustively tabulating the answer. So I just mug it up in a very literal sense. But now, if you give me a new combination which is not there in this table, which I have not memorized, I don't have any principle way of giving you chance. So this model, in the terminology of machine learning, does not generalize very well. So generalize in this sense means that if I make some number of mistakes on my training data, and if you give me some random new data, I should make a correspondingly similar ratio of mistakes, which, of course, comes back to this question of how do I know which mistakes are made? Process is part of that. But this is the fundamental thing, right? I want to trade off this optimization. I don't want to make it fit my training data\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"of how do I know which mistakes are made? Process is part of that. But this is the fundamental thing, right? I want to trade off this optimization. I don't want to make it fit my training data very closely, because then maybe it is learning things which are peculiar to the sample, which I have, which are not representative of the general case. So I might want to intentionally, in some sense, make mistakes so that I generalize well. And then, of course, there are these problems of the training data itself. Many of these models, especially the more complicated models like neural networks, they require a large amount of training data, because essentially, as we will see, the process of learning is one of setting parameters. And the more parameters you have in your model, the more complicated your model is, the more data you need to adjust those parameters well. So how do you get this training data? Is another question.\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"Okay, so, let's look at decision. So, here is our table. So, just to remain so, our table has these 15 rows and it has these four columns. So, if you have ever played this game called 20 questions. So 20 questions, typically, you play with one or a group of people, and one person guesses a name, name of a person and person, or the rest of the group has to figure out who this person is by asking questions. And the questions will have some limited format, and typically they commonly answer yes or no. So you might ask, first question might be, is the person female? Then if the answer is yes, then you know that you have to focus all your questions, assuming that the person, that the hidden name is a female's name. Otherwise you will guess a male's name. So, depending on the first answer, you will ask a second question. The second question may be, is this person alive? Then answer may be no. So now you know that you're talking about a dead woman. And then you might ask, was this person in\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"will ask a second question. The second question may be, is this person alive? Then answer may be no. So now you know that you're talking about a dead woman. And then you might ask, was this person in entertainment show business? So these are the kinds of ways in which you narrow down the space. So you start the space of every possible person whom you could be guessing. And then by cutting it down, by asking these questions, you cut down the possibilities to narrow it. So you do the same thing here, right? So you say, okay, maybe it is that the age has some implication on the category. So you ask, what is the age of the person, and depending on the answer, you will ask a second question. So, for instance, if the person is young, then maybe the correct thing to ask is whether the person is working or not. Maybe they have not earned enough money to buy a house yet, but at least they should be working. Seems to be reasonable, because if you look at this, if I look at young, and if I look\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"working or not. Maybe they have not earned enough money to buy a house yet, but at least they should be working. Seems to be reasonable, because if you look at this, if I look at young, and if I look at those who are working, they are given a look, and those who are not working are not given. So this seems to be actually something. But of course, this is something I've done by inspection. But this is the kind of thing that we want to do automatically. So we want to know which is the first question to ask, which is the second question to ask, which is the third question to ask, and so on. So here is a decision tree. A decision tree starts by asking a question. So the first question that is asked here is, what is the age? And there are three possible answers, because that column had three answers. So I'm asking it of a particular individual in the training data. So I'm asking these questions of these specific 15 books. So if I get the answer young, then the next question I ask, as we\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"answers. So I'm asking it of a particular individual in the training data. So I'm asking these questions of these specific 15 books. So if I get the answer young, then the next question I ask, as we indicated, is whether a person has a job. If they get the answer middle, this person is middle aged, then I get asked the question whether this person owns a house or not. And if they are old, by whatever old means, I will ask whether they have a cult. Now, these, in turn, have their own answers, right? So if the person is young and has a job, then it narrows down to two rows. So this says that two out of two rows which follow this trajectory, out of the 15 rows that I start with, there are two rows which follow this path. And both those things come out to be yes. If I follow this path, those who are young and don't have a job, three of them follow that path, and all three of them turn out to be no. Similarly, if somebody's middle aged and owns a house, it turns out there are three of them\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"are young and don't have a job, three of them follow that path, and all three of them turn out to be no. Similarly, if somebody's middle aged and owns a house, it turns out there are three of them and they have yes. And there are two which have, without a house, middle age. Both are no answer. So this is our model. So when do you stop? You stop. When you reach this situation, we will come to it formally, but once I have asked enough questions to be sure of the answer, I don't need to ask any more questions. So in the case of the 20 questions thing, once you are clear who the individual is, you will say, I guess that the individual is so and so. Now here what you're saying is I have asked enough questions to narrow it down, so that anybody who satisfies the criteria I have discovered is going to be always given alone or is always going to be denied alone. I don't need to know any more for this combination of attributes, I don't need any more information. So you query the attribute, and\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"to be always given alone or is always going to be denied alone. I don't need to know any more for this combination of attributes, I don't need any more information. So you query the attribute, and as we discussed in the context of that 20 question series, what it does is it really partition data? So based on that first question there, I decide I'll end up partitioning this training data into three sets. Then I will look at a particular partition. So I look at this set, and as we said, by asking a question about has job, I partition that further. Similarly, if I ask this set and I ask the partition about own the house again, I get these three rows, three rows below with the house, and two rows above which don't have a house. So at each step I start with a bunch of data items. The answer that I get to a question focuses on those data items where the answer is that line, so it will partition. Because the answer has to be exactly one of the options, there are no overlaps. Then I can look\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"I get to a question focuses on those data items where the answer is that line, so it will partition. Because the answer has to be exactly one of the options, there are no overlaps. Then I can look at the smaller data set. It'll have one less column in some sense, because I've already fixed one column's value. I've already decided that in this five rows, I already fixed that the age is young, so there's no point in asking the age again. So now I'm only looking at these three columns. So in these three columns I should ask a question, and I choose whether they have a job or not. So you partition the training data based on the answer, and then you choose a new question on this reduced set of rows. And this new question will depend on which set of rows you're looking at. And you keep repeating this until you reach a partition where every row says yes or every row says no. That's the ideal situation that you hope to get to in a decision. The question that we have to ask is, how do you do\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"this until you reach a partition where every row says yes or every row says no. That's the ideal situation that you hope to get to in a decision. The question that we have to ask is, how do you do this? How do you actually decide which question to ask Next? So, as we saw, the questions are adaptive.\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"So it means that in this path I ask a different question in this path, I ask a different question in this part. So depending on the answers I have seen so far, the next question that I ask will be different across these different paths. So I'm not always going to ask the same second question. The second question depends on the answer to the first question, the third question depends on the answer to the first two questions, and so on. So, to formalize this as an algorithm, so we have a set of attributes so these are all the columns other than the class label, which is not counted as an attribute. These are all the column names in my table. I pick some attribute, one of the columns, and now I will create these subtables, these children, corresponding to the value of that column, and then I will recursively do this. I will take those rows which fall into that subtable, and I will again, now I have attributes which exclude the one which I chose, because the one that I chose is already\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"I will recursively do this. I will take those rows which fall into that subtable, and I will again, now I have attributes which exclude the one which I chose, because the one that I chose is already uniform for that particular set of rows, right? So I have all the young ones, all the middle aged ones, all the old ones. No point asking the age again. So I stop when the current node has a uniform label. That's what happens here when I reach a subtable in which everything is yes or everything is no. But there is another option that could happen, which is that I might reach a situation where. So, remember, how many questions can I ask? The number of columns, exactly. So this is at most, this is all the information that I have. I can't ask anything more than what is given to me in the table. So I can ask one column at a time. But the moment I ask a column, that value of that column is now zero, because the utility of that column is zero, because I've already asked the question. So in the\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"So I can ask one column at a time. But the moment I ask a column, that value of that column is now zero, because the utility of that column is zero, because I've already asked the question. So in the new table, which I have, that column value has been fixed, no point asking it again. So you never need to ask the same question twice in some sense, and therefore you can only ask as many questions as there are columns. So now it is conceivable that this was not three by three, but it was, say, two by two. So that is, among people who were young and who did not have a job, there were three rows, but two of them they got no, and one of them they got yes. So then I had to ask one more question. Now, it is conceivable that the next row goes to one out of two and so on. Now here, because the numbers are such, it's not really going to happen, but it could be that you run out of questions. So at each point one attribute is getting knocked off. So at some point the set of attributes is going to\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"are such, it's not really going to happen, but it could be that you run out of questions. So at each point one attribute is getting knocked off. So at some point the set of attributes is going to get exhausted, because I've used up all the questions that I could possibly ask, and yet I have not reached a situation where everything in that group of rows that I have left is all yes or all no. So why would that happen? Can you imagine? So what this is saying is, so let's assume there are three, four columns, right? So I have one row which says, so these are the concrete values, and it says yes. So I've asked whether a one is equal to x one. Then I asked whether a two is equal to x two. Then I asked whether a three is equal to x three, and so on. Right? So I've done this. So my four attributes are used up and I have this. But I also have, for the same thing, at least one other row which has the same values for these four attributes. But the answer is different. So can you suggest why this\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"are used up and I have this. But I also have, for the same thing, at least one other row which has the same values for these four attributes. But the answer is different. So can you suggest why this might happen? Because you're really saying that from our perspective, these two items are identical because they have the same four attributes, but one has been given alone and one has not been given. There may be another attribute. Exactly. So the point is that we are not claiming that, and we cannot claim in general that the attributes that we capture in this table or in this training data are exhaustively the ones which determine the answer. So just in terms of loans, or even more apt for many of you, is consider a job interview, right? So you would ideally like that a job interview determines the job or not based on your qualifications. The qualifications could be your performance in your previous degrees and in CMI degree, on your performance in some maybe selection test, that the\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"the job or not based on your qualifications. The qualifications could be your performance in your previous degrees and in CMI degree, on your performance in some maybe selection test, that the company gives you some questions that you answer in the interview, and so on. But as if any of you have ever given an interview, you also do other things, right? So one thing is you tend to try and possibly dress up more neatly than you would if you were just going to class. Why would you do that? Because clearly, even though this is not something that is going, nobody is going to write down on your application form, that person came with crumpled clothes, that's not going to be recorded anywhere. Or if you happen to have an interview in which you maybe said something inappropriate or rude, in the perception of the interview, that may not be recorded. So there may be different factors which are not recorded in these attributes which are actually influencing this. So you cannot claim that these\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"perception of the interview, that may not be recorded. So there may be different factors which are not recorded in these attributes which are actually influencing this. So you cannot claim that these things are exhaustive. So essentially, when you have this kind of thing, there is, as whoever pointed out, there is some kind of implicit missing criterion, but you have no idea what that is. So you have to accept that this will happen. So in such a situation, also you have to stop and say something. So your decision tree, finally, what is the goal of decision tree is going to say? If this is the combination of attributes that I see in an item, is my answer yes? Or if I am this case, the answer is clear. Whatever label I have reached uniformly in that group, that is the answer. So if I have, as we said, if the person is young and has a job, I will say yes, I don't have to say anything else. If the person is middle aged and does not own a house, I will just say no. But if there is a mixed\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"as we said, if the person is young and has a job, I will say yes, I don't have to say anything else. If the person is middle aged and does not own a house, I will just say no. But if there is a mixed thing, then how do I choose? So there is no good way to choose. So all we can say is that at that point, hopefully there is some uneven distribution. So maybe there are a few unusual cases who had all the correct attributes as far as what is written down, but due to some mess up and some hidden kind of features, they didn't get it. So you could basically just take the majority, right? So supposing you do all this and you come to a situation where you have eight customers who have similar attributes, and five of them are given a loan and three are not given, then if you get a new case which looks like this, you will say, okay, five out of eight were given. So let me predict that the new case will be given, because that's a nature. So that's really the only sensible thing you can do in the\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"like this, you will say, okay, five out of eight were given. So let me predict that the new case will be given, because that's a nature. So that's really the only sensible thing you can do in the absence of any other information. So that's typically what a decision three algorithm. So it'll ask question by question. The real we have to figure out is this part, which is how do you pick the next attribute to question. But once you pick an attribute, then the next step is automatic. It will filter out the rows according to the values attribute. And then you have to ask the same algorithm applied on each of those subtables, and these subtables be destroyed. And when you stop, you ask whether it's uniform or whether there's a majority. So this is what we were discussing. So a non uniform leaf node where I have run out of questions, but I have different classes. So basically I have identical combination of attributes, but different classes is something which indicates that our data is\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"leaf node where I have run out of questions, but I have different classes. So basically I have identical combination of attributes, but different classes is something which indicates that our data is somehow incomplete. The attributes are not capturing all the criteria which are actually used for the classification. There are some hidden attributes which are not being captured, and this happens quite often. So we can't derive this. This might happen for a variety of reasons. It might happen in these kind of manmade scenarios, like giving loans. But even when you're trying to classify, this is happening around us. Every example can be traced to COVID, but right now we see that, right? So there are lots of situations where we have to realize that different people with the same bit different symptoms. So conversely, two people with the same symptoms don't necessarily. So there are asymptomatic COVID patients. There are people who have a sore throat and a cold and a headache who don't\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"symptoms. So conversely, two people with the same symptoms don't necessarily. So there are asymptomatic COVID patients. There are people who have a sore throat and a cold and a headache who don't necessarily have COVID. So I cannot just say that if you have these symptoms, you must have COVID, and if you don't have these symptoms, you cannot have COVID. Right.\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"So clearly there is something going on in the background, and that background in our case is revealed through a test. Even tests have errors and all that. So it's really a problem that we have to grapple with in real life. And we will just assume for now that when we reach such a situation, we just take a majority decision. But realistically, if we get too many of these situations, and it's very difficult to decide, we have to ask whether the data has actually been collected correctly. So as we said, the main issue is really how to ask the questions and what this means. Sorry, sir, I have a question. Yeah. Sir, once we know that the data is young, we are making a prediction and it come out that young. So like other attribute is not relevant other than it has job or not, and then we are not going to look at other attributes. Yeah, but remember that which attributes to look like vary from one path to another. So maybe in this case, right, so what we are saying is that if they are young,\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"going to look at other attributes. Yeah, but remember that which attributes to look like vary from one path to another. So maybe in this case, right, so what we are saying is that if they are young, we only need the job, right? But if they are middle aged, we need to know the house. So for different combinations, so they may not occur together. I agree, in any decision, but each of these plays a role somewhere. Now, if there is another attribute which never plays a role, as we will see the next tree as that example. Right? So if you look at this, this is a different tree for the same data set. So instead of first asking the age, we ask whether you own a house or not. And it actually turns out that everybody in this data set who owns a house, six. Out of those, remember we said there were nine yeses and six no's overall, but out of those nine yeses, six of them own a house and all six of them get yes. And nobody owns a house who's told no. And if you don't own a house, then if you have\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"yeses and six no's overall, but out of those nine yeses, six of them own a house and all six of them get yes. And nobody owns a house who's told no. And if you don't own a house, then if you have a job, you get it. If you don't have a job. So in some sense, if you don't have a job and you don't own a house, you're not going to get a loan. If you have either of the other two, you're going to get a loan. Now, in this situation, this attribute doesn't appear at all. So then you could argue that that attribute is irrelevant to this thing, and maybe you could flag it and say that maybe you need not. That's possible. So we need to make like deployment tree too, in order to. Sorry, I didn't hear you. Can you say louder? We need to take all the possible tree. That could happen. Is it not necessarily all the possible trees? So that is some issue that we have to deal with. So what we are going to try and build, we are not going to build all the possible trees because that would be too many\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"all the possible trees? So that is some issue that we have to deal with. So what we are going to try and build, we are not going to build all the possible trees because that would be too many trees. So we want to build a good tree as far as possible. And then we have to ask, what is a good tree? And with respect to that good tree, what we are saying is there is a kind of ranking of attributes. So even in this order, the tree will tell us a little bit about which attribute is more important. So there is a kind of implicit ranking which is happening, and an attribute which does not appear at all in our tree that we construct. If it turns out that otherwise it's a good tree, then we could argue that that attribute has no significance. But even otherwise, one of the things, byproducts of decision tree will be a kind of implicit importance, ranking among the attributes, which ones are more significant. So not all columns are equally important. So these are all things that have come up. So\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"tree will be a kind of implicit importance, ranking among the attributes, which ones are more significant. So not all columns are equally important. So these are all things that have come up. So maybe when we come to it later on we'll discuss it. But these questions that you're asking, sir, when we form this decision tree, is it important or in some sense beneficial if we know the data? Insight of the data. Insight of the data, meaning what? As in the relevance of each column? Yes, normally that is, I mean, I cannot say it will always help. Normally it will help. But on the other hand, you have to assume that you do not have. So what you're really asking in a more general setting is that if a person who is kind of aware of the context, who's a specialist or a domain expert in this, so say somebody who has been associated with this, whatever, this bank, this particular bank, which is giving out loans, if they build the tree, they might make choices which are given based on their\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"so say somebody who has been associated with this, whatever, this bank, this particular bank, which is giving out loans, if they build the tree, they might make choices which are given based on their expertise. Whereas what we are doing is trying to do something blind without understanding, except for the data that's given to us. And generally, yes, it is true, but the other problem with that is that in general, one cannot really. So in specific situations, this does help. So if you have domain knowledge, you should use. But sometimes the domain knowledge is very hard to capture in terms of the data, and it's not easy to give an example in this tabular form, but it is very easy to give examples in terms of images, as it doesn't require a great expert to distinguish dogs from cats. So anybody who is above a certain age can look at a picture and tell you whether it's a dog or a cat. But if I ask you as a domain expert to give me an algorithm to distinguish a dog picture from a cat\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"anybody who is above a certain age can look at a picture and tell you whether it's a dog or a cat. But if I ask you as a domain expert to give me an algorithm to distinguish a dog picture from a cat picture, or one which is neither of the two, it's not very easy to quantify that. And there the attributes in a picture are typically just if you think of a picture as a digital picture, it's a bunch of pixels and their colors and their arrangement, their relative arrangement. So even I'm saying, even if you have some domain knowledge, it's not very clear that the person's domain knowledge is aligned to the way the data is available. So what we are going to be doing is agnostic to the domain. So we are not going to believe anything or assume anything about the domain, except that we will want to validate at the end somehow that what we are doing is right. But in practice what you're saying makes sense that really to do supervised learning, it's much more realistic to do it in collaboration\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"validate at the end somehow that what we are doing is right. But in practice what you're saying makes sense that really to do supervised learning, it's much more realistic to do it in collaboration with somebody who understands a domain rather than just doing it blindly. Because also when you do it blindly, you might come up with solutions which make no sense. You might actually identify features which contribute to a decision purely by statistical coincidence, and no human would actually do that. So there's another issue which comes up of interpretability or explainability of your answers. So there are various factors which get into this. It's only not only accuracy which matters, but also whether your model is sensible in something, so that you can justify it. So these will all come as we go along. So let's proceed with this. So the first thing is that because the choice of attribute to question will determine the structure of the tree. So here are two different trees. So this is\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"we go along. So let's proceed with this. So the first thing is that because the choice of attribute to question will determine the structure of the tree. So here are two different trees. So this is the tree we started with, and in a sense this tree, if you remember that table, consists of asking the first column first. So the first column in our table was age. So we asked age, and then maybe we adapted our question based on age, whereas here we have directly jumped to own a house. The question to ask is which of these trees intuitively is better secondary? What is the definition of better over here, a shorter tree or a tree which predicts better? So here both trees are predicting. Exactly. So there is no mistake, at least with respect to the training data. Of course we have not seen what it does with the unknown data, but if you are just looking at its behavior on the training data, all of these things reach nodes in which I don't have any confusion, any majority to choose. So in that\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"does with the unknown data, but if you are just looking at its behavior on the training data, all of these things reach nodes in which I don't have any confusion, any majority to choose. So in that sense they are equivalent in terms of the prediction. So therefore, the question is that if they're equivalent in terms of prediction, whatever that means, then the question is somehow. So it's not even shorter, right? Because this is also like.\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"Depends on what shorter means. But I ask two questions here. I ask two questions here. So, normally, the height of a tree is the longest path. So in the longest path, I ask two questions, but clearly this is fewer questions overall. And there are some situations where I can get an answer in one question and so on. So for many reasons, the secondary looks simple. So generally we prefer small trees. So there are many reasons for this. The one reason is it's perhaps easier to explain when I have fewer questions to answer. So there is this general principle which is called. So this is from the philosophy of science. So William of Auckham was a philosopher from several hundred years ago. So his general principle was that if you have two explanations for something and one is simpler than the other explanation, then the simpler explanation is a better one. And this has been a guiding principle for a long time now. It's not always the case that the simpler explanations are correct. So, do you\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"explanation, then the simpler explanation is a better one. And this has been a guiding principle for a long time now. It's not always the case that the simpler explanations are correct. So, do you know an example from science? Newton and quantum mechanics. Right? So newtonian mechanics works, but it doesn't really do a good job at atomic levels. So you have to go to quantum mechanics, but under certain assumptions. At macro level, you would not want to do quantum mechanics for measuring how to predict, to put a satellite in orbit or something like that. At that level, you will just be doing newtonian mechanics. So it depends also on what kind of problem you're trying to solve. But in general, this is the principle that smaller explanations are better. And one of the kind of intuitive reasons for that is that smaller explanations are easier to convey what they are doing. So this explainability. So another kind of illustration of this is supposing you go to a doctor and the doctor asks\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"for that is that smaller explanations are easier to convey what they are doing. So this explainability. So another kind of illustration of this is supposing you go to a doctor and the doctor asks you to take a battery of tests. So you come back, I'm sure many of you have done this. So you get some medical report and you say, this is this. So you will have some, depending on how many tests you have done, you will have anything ranging from five to 15 to 20 numbers on that piece of paper which the lab has produced. Right? Now, if the doctor were to say, okay, looking at this piece of paper, I declare that you have XYZ condition. It will be very hard for you to expect to believe that. What the doctor will say is that, look at this number. Look at that number. So those two numbers are outside the range that I would normally expect, and they typically indicate something. And therefore, I believe you have. So you narrow down the suspects. So it's the same as saying that you have a complex\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"the range that I would normally expect, and they typically indicate something. And therefore, I believe you have. So you narrow down the suspects. So it's the same as saying that you have a complex explanation which says, test whatever. Number one is this and number two is this, and number three is this, and number 15 is this, and therefore. Or you say, number two is this and number seven is that. And therefore, in any context, generally, a smaller model is usually better, but everything is. And this is a common feature of anything we do, machine learning. Nothing is. You cannot say it universally, as I said, like newtonian mechanics and quantum mechanics, you could always have a situation where the more complicated model is required. So, yes, certainly small trees are more explainable. It'll turn out that small trees also solve the other problem that we mentioned earlier about generalization. So, question is whether or not this model is better for the data you have not seen compared\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"turn out that small trees also solve the other problem that we mentioned earlier about generalization. So, question is whether or not this model is better for the data you have not seen compared to the data that you are training on. So is it too specific for the training data that you are building. So if you ask more questions, then you are really focusing on more individual eto synchronicity or complications in the data that you see. That might be an isolated case, and it may not appear in general, so maybe you should ignore it. I will show you later on an example of this. But the problem is that as a computational task, so I have not yet told you how to do this, but if you wanted to make this your goal, build the smallest tree, which, among other things, gives you this kind of a nice boundary of uniform answers. The problem is that this is a task which is what is known in computer science as NP complete. Now, you don't need to know what this is, but if you are interested, I will put\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"of uniform answers. The problem is that this is a task which is what is known in computer science as NP complete. Now, you don't need to know what this is, but if you are interested, I will put up a reference. But this is basically saying that sort of the only way to guarantee that you'll get the smallest tree is to try every tree, and there are going to be enormously number, exponentially number of large number of trees, because I can choose anything I want for the first attribute, and then I can choose anything I want for the second question on each path and so on. So I have to try out all these combinations and then evaluate which one comes out smallest. And short of doing that, assuming that this p is not equal to NP, there is no guarantee or no known way of doing it. So this is a little bit of a disappointment that we cannot actually hope to find the smallest tree in a reasonably efficient way. And this doesn't matter how you define smallest, right? So smallest could be in terms\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"bit of a disappointment that we cannot actually hope to find the smallest tree in a reasonably efficient way. And this doesn't matter how you define smallest, right? So smallest could be in terms of how many questions you ask, in terms of the depth, number of nodes, that are there in the tree. It could be any of these. So instead of this, what we are going to do is to try and approximate this by choosing the next question as best we can. So this is what is known as a greedy status. So you have a number of choices to make. You have a number of columns to choose from. You look at the column, which appears to give you the best benefit right now, and then this is not always going to give you an overall optimum strategy. Sometimes you need to make a suboptimal choice now to get a better choice later on. But we are going to follow this greedy thing where we will have some criterion to choose one attribute and we will apply it one by one at each step. So what is this greedy strategy? See,\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"later on. But we are going to follow this greedy thing where we will have some criterion to choose one attribute and we will apply it one by one at each step. So what is this greedy strategy? See, first, to understand the greedy strategy, we have to understand what we are trying to get to. So we are trying to get to this situation as fast as possible, right? So what do we want? We want to reach a partition in which everything is yes or everything is no. As soon as we get to that, we can stop asking questions. So that is our goal in some sense. So we start with our training data and we want to ask questions so that every path terminates as quickly. These uniform partitions. A partition which is uniform is something that we will call pure. It is either pure yes or pure no. Remember, we are just assuming everything is binary, so we can just assume the categories are yes or no, right? Whatever they are supposed to be, it's yes or no. So our goal is to achieve all yes or all no as fast as\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"just assuming everything is binary, so we can just assume the categories are yes or no, right? Whatever they are supposed to be, it's yes or no. So our goal is to achieve all yes or all no as fast as possible, no matter what trajectory we choose. So given this answer, I need to choose the next question so that it will converge to all yes or all no as fast as possible. On the other hand, if I reach a node where I do not have all yes or all no thing impure, then at that point, if I have to stop and make a prediction, then I will choose the majority. In the worst case, it could be the last node. I don't have any more questions to ask. Then I have no other choice. But if you stop me midway, supposing you stop me here, I found out that this person is young, right? So what should I answer? Supposing I'm not allowed to ask one more question, right? So I've just done this and I've come here no, because there are three no's and there are two yeses, so it is no's and two yeses. So I should\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"Supposing I'm not allowed to ask one more question, right? So I've just done this and I've come here no, because there are three no's and there are two yeses, so it is no's and two yeses. So I should answer no, because that's a majority. Whereas here, if I say the person is middle, aged. Then without asking any more questions, I say three yes and two no, and I'll say yes. And if the person is really old, then I have four yes and one no. So with more certainty I can say, so I could stop this algorithm and answer even earlier.\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"So we have this impurity, right, the majority value, and then we have the minority value. And we will basically want to minimize the minority to zero. If the impurity becomes zero, then I have a pure node. Okay? So our goal is really to go from some. So I start with obviously some kind of impure node, and I want to go to a pure node along each path. So the idea is to keep purifying in some sense, this node, by reducing the impurity as much as possible. So that is the heuristic. The heuristic is to try and make the resulting partition nodes that I create as pure as possible. And so I choose a question according to that. So I have an impurity here. So what is the impurity here? I had nine yes and six no. So the impurity is six by 15, which is 0.4, right? So the impurity is the ratio of the minority to the total. So it's two by five, which is zero, four. What is the impurity? Here it is again, two by five. The minority is two out of a total of five. What is the impurity here again? Now\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"minority to the total. So it's two by five, which is zero, four. What is the impurity? Here it is again, two by five. The minority is two out of a total of five. What is the impurity here again? Now notice it's minority. It's not saying yes or no, right? It's saying which of the two answers is the minority and how many of those are there as a fraction. So again, the impurity is zero. And here impurity is one out of five. Okay? But now I started with an impurity of zero four. And I produced three nodes with impurity, zero four point. Let me look at this situation. I started here again with the same impurity .4 because I had the same starting point. And now impurity on the left hand side is at this point. How much is impurity here? Zero. Right? I have no minority. What is impurity here? Look at the bottom. There are nine things. One third and three are wrong. So it's one third. So we can say 00:33 just for sake of. So now the problem is, how do I compare this combination with this\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"Look at the bottom. There are nine things. One third and three are wrong. So it's one third. So we can say 00:33 just for sake of. So now the problem is, how do I compare this combination with this combination, which is better? So that's why we now say you take the weighted average. So we say that here I have one third of the nodes end up with zero four impurity, one third of them end up with zero four impurity, and one third of them end up with zero. Here I have six by 15, right? So two fifths of the nodes end up with zero four impurity, zero impurity and three fifths of the node end up with. So this, maybe it is better to write it as a fraction. So this now resolves to zero plus one fifth. So this gives me zero. This plus this plus this is zero plus one third times three fifths is one fifth. On the other hand, here I have two by 15, plus two by 15 plus one by 15 is equal to five by 15 equal to one five equal to zero point. So what I'm saying is, now I've started with an impurity of\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"On the other hand, here I have two by 15, plus two by 15 plus one by 15 is equal to five by 15 equal to one five equal to zero point. So what I'm saying is, now I've started with an impurity of zero four. If I ask the age question, then my resulting weighted average impurity reduces to 00:33 but if I ask the own house question, it reduces to zero two. So the reduction or the purification process is moving faster in the own house case. So if I have to choose between age and own house, own house is a better question to ask right at the beginning. So I will ask this of all the four options. So, initially I have four options. So there is nothing magical about this. I ask for each question I can evaluate, because the training data is completely known to me. So I can ask each question and see how the data splits. And the data will split into some ratios. The ratios need not be equal. That's why you're taking the weighted average. So we are saying in one third of the cases, this happens, in\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"the data splits. And the data will split into some ratios. The ratios need not be equal. That's why you're taking the weighted average. So we are saying in one third of the cases, this happens, in two thirds of the cases that happens, and so on. So you take this weighted average, you get the average impurity after you ask this question. And what you want is that average impurity after you ask this question should be as low as possible. So you choose, among all the attributes which you could ask, you choose the one which reduces that thing as much as possible. So any question about this. So we have to check for every possible order of asking. I'm only asking one question at a time. So for the first question, I'm saying I will ask all four. I have four columns in this case, so I will apply this at each question separately. So initially I have four. So I will ask for each of these four. What's going to happen if I ask this question as the first question? So, notice that I'm not really\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"this at each question separately. So initially I have four. So I will ask for each of these four. What's going to happen if I ask this question as the first question? So, notice that I'm not really asking this question. I've not asked this question, right, really, I'm just saying that if I ask at the top level itself, maybe I should. So what we are saying is that if I ask age, then I end up with. If I ask house, it end up with six years or end up with three years, six years. So similarly, I would ask what happens if I ask what is it thing, John. So there will be something. And if I ask credit. So I will evaluate these four things. So I will now take a weighted average. So the weighted average is saying here each is five. So it's one third, one third, one third. So I take the average, I take the impurity of each combination, times that as the weighted average. Here I take, in this particular second one, I take six by 15. So two fifths and three fifths. So again, if I look at job, we'll\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"the impurity of each combination, times that as the weighted average. Here I take, in this particular second one, I take six by 15. So two fifths and three fifths. So again, if I look at job, we'll have to see if I look at the job column. So look at the job column. False, false, false. 56789. So there are nine where the job will be false, and there are six where the job is true. So whatever impurity comes out of the true set will be six by 15. And whatever impurity comes out of the false set will multiply with nine by 15. I'll add it up and I will get the net impurity of that. So that's how I do it. So I will do it for all these four cases, and then I will base on, based on the answer I get for that. So based on the answer for that, I will choose one of these. Now I will come to the next question. So now I have only three columns. So I will choose among those three columns on the current impurity, what happens. So it will depend on each situation what happens, because I'll be working\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"question. So now I have only three columns. So I will choose among those three columns on the current impurity, what happens. So it will depend on each situation what happens, because I'll be working with different subsets of. So yes, it is exhaustive, but it is not something, I'm doing it one level at a time and one node at a time. So we choose the minimum one. So that's basically the idea. But it turns out that this is not the best heuristic to use, and that's only by the fact that it's not the best heuristic to use is more by people trying to build these trees and finding that this doesn't necessarily do a good job. It's not, theoretically, not easy to explain why this is bad, but there is a certain logic to it, which is that this misclassification rate is linear. What it means is that if this is the, so this is the fraction of so the two classes, let me say this is yes. Everything is either yes or no. If I tell you that 60% of the columns are yes or rows are yes, then you know\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content='if this is the, so this is the fraction of so the two classes, let me say this is yes. Everything is either yes or no. If I tell you that 60% of the columns are yes or rows are yes, then you know that the remaining 40% are no, right? So you know the other side. So the other side is fixed. So if I have zero yeses or if I have all yeses, right? If 100% or a fraction of one, then the misclassification rate is zero, because the minority case must be zero. It must be one.', metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"I mean, basically, if this is the case, then the minority is. So I'm plotting the fraction of things where the answer, the class is one. The class is yes. So if the fraction is zero, that means this is the minority. The majority is the other class, and therefore, there is no misclassification. If everything is C equal to one, then there is nothing of the other class. And in between, as this grows, the misclassification rate grows. When both are 50 50, then that's the maximum. Right? So I have. Maybe I should blow this up. So this is the picture, right? So at 0.5, when half the rows are one and half the rows are zero, the misclassification is 50%. And then the answer flips, which is the majority, and which is the minority flips. So, once the ratio of category one crosses 50%, this becomes a majority and the other one becomes a minority. And as the number of rows with one grows, the minority shifts. So you understand this linear curve, right? So this is basically talking. So the y axis\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"a majority and the other one becomes a minority. And as the number of rows with one grows, the minority shifts. So you understand this linear curve, right? So this is basically talking. So the y axis is the misclassification rate, and the maximum misclassification rate is half, because at most, the minority can be half. If it is more than half, it's a majority. Okay, so, empirically, it turns out that it is better to have a function which looks like this, where if you deviate from a pure state, so notice that either everything is zero or everything is one. So if you deviate from the pure state, your misclassification, or whatever penalty you are associating, should grow faster than linear. That's what you really want. Now, why? As I said, this is basically an empirical observation that if you can design such a function where you can. So what are you trying to do? You're just trying to associate a quantity given the split of yes and no, you're trying to associate a numerical quantity\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"you can design such a function where you can. So what are you trying to do? You're just trying to associate a quantity given the split of yes and no, you're trying to associate a numerical quantity with that which is different from just the ratio of the minority. The ratio of the minority has this linear thing. You're trying to find a different function which gives you a better nonlinear curve. So it turns out that there are two such functions which have been studied. So one comes from information theory and it's called entropy, and the other one comes from economics and it's called genie index. So let's just quickly look at what these are. So entropy comes from information theory. So it comes from a fundamental piece of work by Claude Shannon related to how many bits you need to transmit a message. So what Shannon said is that you need to really look at the relative frequency. So if you are taking a typical message, which is, say, written in a natural language like English, and you\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"a message. So what Shannon said is that you need to really look at the relative frequency. So if you are taking a typical message, which is, say, written in a natural language like English, and you have to encode each letter as bits, then you should take the frequent letters and give them shorter encoders. Because there are more E's and t's going in your message than there are, say, Z's and Q's. So you use longer sequences for z and Q, shorter sequences for E and T. And overall, your message length will be smaller. So that was really the motivation. So, formally, he tried to compute the other way around. What is the minimum number of bits that you will need to send a message? And that is the centropy. That is how the more random your message is. So if your message is perfectly predictable, if the outcome of the message is known, you don't have to send any information at all. So nowadays, because I think cell phones are cheaper, but one doesn't do it anymore. But this infamous missed\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"if the outcome of the message is known, you don't have to send any information at all. So nowadays, because I think cell phones are cheaper, but one doesn't do it anymore. But this infamous missed call. So everybody knows about the missed call. So what is the missed call? Missed call is a prearranged signal, right? It's a message whose answer is known. It says, I am at the gate or I have reached or something. So when you go there, send me a missed call. When you arrive at the door, send me a missed call where the existence of the message is all that you need. You don't need to know what the message contains. There's no information to be conveyed because the answer is the message content is fixed. There's no randomness. Whereas if I'm trying to tell you the sequence of ten coin tosses, then I have to tell you the sequence. Because there is no way, if the coin tosses are truly random, that I could ever compress it into a smaller. So, this is what Shannon was trying to capture. So, here\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"I have to tell you the sequence. Because there is no way, if the coin tosses are truly random, that I could ever compress it into a smaller. So, this is what Shannon was trying to capture. So, here we are dealing with two cases, zero and one. So what we do is we split the n columns, n rows, rather as n zero and n one, right? So n is equal to n zero plus n one, the number of rows in which the class is zero. And the number of rows in which the class is one. And I interpret these ratios as probabilities. So, p zero and p one. The p zero is the probability of zero, which is a number of zero rows. Number of rows where the last column is zero. P one is the probability of one number of rows where the last column is one. And now you just have to swallow this. This is Shannon's formula for input. It's p zero log, p zero log to the base two p one log p one. And the reason you have a minus sign here is that, remember that p zero, p one are smaller than one. So log of p zero, log of p one,\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"It's p zero log, p zero log to the base two p one log p one. And the reason you have a minus sign here is that, remember that p zero, p one are smaller than one. So log of p zero, log of p one, they're going to be negative numbers because two to the zero is one. So if you're less than one, it's some one by two to the something. So it's two to the minus k.\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"So I add up this two quantities and I negate it, and you get a curve that looks like this. So this is our old linear curve, which goes up to half and then comes down, and this is this thing. Now, the only difference is that of course it's going to go up to one. And the reason it goes up to one is that when p zero and p one is half, I'm going to end up with half log half plus half, log half, and log of half is what, two to the minus one. So this is minus half, plus minus one, which is one. So what Shannon is saying is that when both outcomes are equally likely, when p zero and p one are both halves, then you have maximum randomness, and the randomness otherwise drops off like this. Now, you have a small problem, which is, what do you say for the randomness at the pure state? At the pure state, one of these probabilities is zero, because either p zero is zero or p one is zero, because I have zero rules of zero, zero rules of one. So you have to technically evaluate zero log zero, and\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"state, one of these probabilities is zero, because either p zero is zero or p one is zero, because I have zero rules of zero, zero rules of one. So you have to technically evaluate zero log zero, and zero log zero, as you probably know from calculus, is not a great thing because log of zero is not defined. You have to do some limits and all that. But for entropy calculation, zero, log zero is just defined to be zero. So log of one is going to be zero. So if I'm at an extreme case, either p zero is zero and p one is one, or p one is zero and p zero is one. So one of the two will become zero because log of one is zero. The other two will become zero because zero, log zero is declared to be zero. Anyway, the important thing for us is that this is the curve of the entropy. And independently, and quite unrelatedly, somebody called corrodo jenny, who was a genie who was an economist, was talking about inequality. So he wanted to measure inequality. How unequal? So, supposing something is\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"and quite unrelatedly, somebody called corrodo jenny, who was a genie who was an economist, was talking about inequality. So he wanted to measure inequality. How unequal? So, supposing something is distributed in society, how do you measure whether it's being distributed? So wealth. So if the wealth is distributed, so that, for example, you have, many times you have seen this, right? 50% of the wealth is with 3% of the population or something. So then the society is very unequal. So this is the kind of thing he was trying to measure. So in his case, again, he used the same thing. You have two categories, and you want to measure the inequality. If n zero is equal to n one, that means something is equal is very large compared to the other is unequal. But his quantity was expressed as one minus p zero squared plus p one squared. Again, you will notice that if this is half and this is half, then I get one fourth plus one four, and I get one minus this, which is equal to half. And you can\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content='p zero squared plus p one squared. Again, you will notice that if this is half and this is half, then I get one fourth plus one four, and I get one minus this, which is equal to half. And you can check that this is the largest value that you get. So in genie index, if the two values are half, then you get an index of half, and otherwise it will turn out to be much smaller than that. So again, you can plot the genie index curve. And now these are the curves. So this is our old friend. This is entropy. I mean, sorry, this is our misclassification, linear. This green dashed line is genie impurity, and this is entropy. But entropy goes up to one, which is out of scale. So we have the entropy, we get this red. So, notice that the entropy line is slightly steeper than the genie impurity line. And our logic earlier was that somehow nonlinear is better empirically, whatever that means. So empirically just means that somebody built these systems and found that if they use these kind of things,', metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"our logic earlier was that somehow nonlinear is better empirically, whatever that means. So empirically just means that somebody built these systems and found that if they use these kind of things, it works better. So empirically, entropy is, I mean, theoretically, entropy is slightly steeper, but entropy requires us to compute logs and all that, whereas genie index is just this computing squares of numbers. So genie index is actually easier to compute. And if you look at, as we will see, libraries in Python and all that, typically they use genie index. So although there is a very famous system built using entropy, which was one of the early decision tree systems, the ones that we will practically encounter in Python, libraries actually use the genie index. Basically. I'll stop with this right now. So what we are saying now is that we want to choose the next question to evaluate the next question to ask. So for the next question to ask, we have to say something about the quality of\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"right now. So what we are saying now is that we want to choose the next question to evaluate the next question to ask. So for the next question to ask, we have to say something about the quality of the impurity before and after. So if I just look at the raw ratio of the impurity, I have a motion of just that impurity percentage. But that gives me this linear relationship, which apparently is not that good in practice. So I want a nonlinear way of giving some impurity value to this ratio of yes or no, and then we can use either this entropy idea or this genie index, and it gives me a sharper curve, and this sharper practice turns out to be better. So that's how you build a decision tree. You build a decision tree by applying one of these two nonlinear things. If you come up to the third thing, it's also fine. It's just that these two happen to be standard things. They are both borrowed from other areas where they have a long and established history. So, as I said, genie index comes\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"thing, it's also fine. It's just that these two happen to be standard things. They are both borrowed from other areas where they have a long and established history. So, as I said, genie index comes from economics and entropy comes from information. So we'll stop here. So, any questions? It okay, if there are no questions, then we'll continue with this discussion of decision trees on Thursday when we meet next.\", metadata={'source': '/content/audio_3.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create and Store Vectors"
      ],
      "metadata": {
        "id": "monslxx3L7Ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio_store = Chroma.from_documents(texts, hf, collection_metadata={\"hnsw:space\": \"cosine\"}, persist_directory=\"audio_chroma_cosine\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T18:30:06.289937Z",
          "iopub.execute_input": "2024-04-01T18:30:06.290578Z",
          "iopub.status.idle": "2024-04-01T18:36:48.922220Z",
          "shell.execute_reply.started": "2024-04-01T18:30:06.290546Z",
          "shell.execute_reply": "2024-04-01T18:36:48.921209Z"
        },
        "trusted": true,
        "id": "162qpZRbL7Ya"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/audio_chroma_cosine.zip /content/audio_chroma_cosine"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B72EHJERXKOZ",
        "outputId": "e0ef9691-9a82-4010-b3e0-97c9c1dde991"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/audio_chroma_cosine/ (stored 0%)\n",
            "  adding: content/audio_chroma_cosine/chroma.sqlite3 (deflated 39%)\n",
            "  adding: content/audio_chroma_cosine/d5323ef8-2a67-4261-9489-2e43abf389f0/ (stored 0%)\n",
            "  adding: content/audio_chroma_cosine/d5323ef8-2a67-4261-9489-2e43abf389f0/header.bin (deflated 61%)\n",
            "  adding: content/audio_chroma_cosine/d5323ef8-2a67-4261-9489-2e43abf389f0/link_lists.bin (stored 0%)\n",
            "  adding: content/audio_chroma_cosine/d5323ef8-2a67-4261-9489-2e43abf389f0/data_level0.bin (deflated 7%)\n",
            "  adding: content/audio_chroma_cosine/d5323ef8-2a67-4261-9489-2e43abf389f0/length.bin (deflated 22%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/audio_chroma_cosine.zip"
      ],
      "metadata": {
        "id": "5P5LtR_BNtLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_audio_store = Chroma(persist_directory=\"audio_chroma_cosine\", embedding_function=hf)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T18:43:35.540129Z",
          "iopub.execute_input": "2024-04-01T18:43:35.541090Z",
          "iopub.status.idle": "2024-04-01T18:43:35.826233Z",
          "shell.execute_reply.started": "2024-04-01T18:43:35.541054Z",
          "shell.execute_reply": "2024-04-01T18:43:35.825275Z"
        },
        "trusted": true,
        "id": "Im-CleUvL7Yb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Query Retriever"
      ],
      "metadata": {
        "id": "HB7Pp2yVL7Yb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "env_vars = {\n",
        "    \"OPENAI_API_KEY\": \"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\",\n",
        "}\n",
        "\n",
        "for key, value in env_vars.items():\n",
        "    os.environ[key] = value"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T19:07:51.656282Z",
          "iopub.execute_input": "2024-04-01T19:07:51.656660Z",
          "iopub.status.idle": "2024-04-01T19:07:51.661870Z",
          "shell.execute_reply.started": "2024-04-01T19:07:51.656631Z",
          "shell.execute_reply": "2024-04-01T19:07:51.660938Z"
        },
        "trusted": true,
        "id": "J7qrWqUhL7Yb"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "        temperature=0,\n",
        "        max_tokens=800,\n",
        "        model_kwargs={\"top_p\": 0, \"frequency_penalty\": 0, \"presence_penalty\": 0},\n",
        "    )\n",
        "\n",
        "\n",
        "retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=load_audio_store.as_retriever(), llm=llm\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T19:09:12.185107Z",
          "iopub.execute_input": "2024-04-01T19:09:12.185911Z",
          "iopub.status.idle": "2024-04-01T19:09:12.831568Z",
          "shell.execute_reply.started": "2024-04-01T19:09:12.185878Z",
          "shell.execute_reply": "2024-04-01T19:09:12.830819Z"
        },
        "trusted": true,
        "id": "BwxP_uXiL7Yc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27bf5d5a-d8e4-4669-fc7f-ad7dea6acecd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set logging for the queries\n",
        "import logging\n",
        "\n",
        "logging.basicConfig()\n",
        "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T19:10:39.051165Z",
          "iopub.execute_input": "2024-04-01T19:10:39.052003Z",
          "iopub.status.idle": "2024-04-01T19:10:39.058593Z",
          "shell.execute_reply.started": "2024-04-01T19:10:39.051969Z",
          "shell.execute_reply": "2024-04-01T19:10:39.057573Z"
        },
        "trusted": true,
        "id": "H5poVYOeL7Yc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 1"
      ],
      "metadata": {
        "id": "R6DlH30oYY53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_1=\" What are some challenges associated with data collection?\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T19:11:17.655489Z",
          "iopub.execute_input": "2024-04-01T19:11:17.656177Z",
          "iopub.status.idle": "2024-04-01T19:11:17.660448Z",
          "shell.execute_reply.started": "2024-04-01T19:11:17.656147Z",
          "shell.execute_reply": "2024-04-01T19:11:17.659559Z"
        },
        "trusted": true,
        "id": "iK9jSbwNL7Yc"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_docs_1 = retriever.get_relevant_documents(query_1)\n",
        "len(unique_docs_1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T19:11:29.934441Z",
          "iopub.execute_input": "2024-04-01T19:11:29.935183Z",
          "iopub.status.idle": "2024-04-01T19:11:34.301446Z",
          "shell.execute_reply.started": "2024-04-01T19:11:29.935151Z",
          "shell.execute_reply": "2024-04-01T19:11:34.299794Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "id": "K6ByQ758L7Yd",
        "outputId": "61c6d86a-529e-4db9-e34e-2a433332c6a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What obstacles are commonly faced during the process of data collection?', '2. What difficulties can arise when gathering data?', '3. What are the main issues linked to data collection?']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_docs_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSkcrtGCOiUX",
        "outputId": "a44ac906-75af-402a-d64f-520de1c728eb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"is the fact that this data has to be collected to begin with. So historically, a lot of data was collected manually through forms and so on. And one part of the problem would be that these forms were manually entered by somebody and then converted to electronic forms. So that would be, there would be two levels of potential sources for errors. The person writing down the information and then the person typing in the information. Now, gradually these kind of electronic forms are spilled in directly, so at least the source of the error is reduced to one step. But still, people mistype things. I mean, there are any number of situations where people type their email address wrong and so notifications don't reach them and so on. So there is this data collection. How do you collect the data and how do you clean it? And the third thing is, how do you make it uniform? So when data is being collected by different people, they may collect different things. And for instance, if you look at the\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"some unknown data. So, one other strategy where you don't want to, I mean, one of the other disadvantages of doing this is that maybe that there are some minority anomalies in this data, and the choice of test data that you have made might have hidden all these anomalies, so you never see them, and maybe it's important to see them. So there may be many situations, or maybe you just don't have enough data as a whole to build a good model by only looking at 80%. So another strategy is to systematically do this with different subsets.\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content='how do you clean it? And the third thing is, how do you make it uniform? So when data is being collected by different people, they may collect different things. And for instance, if you look at the government, typically the government collects data in different forms. For instance, there is a public distribution system which the ration shops, so they collect some information about who is collecting ration and who is income levels and so on. Then on the other side, you have, of course, things like electoral information. Then you have property information. Who owns property, who pays electricity bills, who owns vehicles, vehicle registration, driving licenses. Not necessarily the same thing. Not everybody who has a driving license owns a vehicle. So you have all these different sources of information. Now, is it possible really to find out same person in all these? Well, a lot depends on how the information is collected. You have a common format in which you record information about', metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"information. Now, is it possible really to find out same person in all these? Well, a lot depends on how the information is collected. You have a common format in which you record information about people. Even names in India tend to be spelt in different ways, written in different ways. Sometimes we expand initials, sometimes we suppress them, sometimes we write middle names, sometimes we don't. Sometimes we invert the order, sometimes we don't. Addresses, of course, are written in a million different ways. So there are all kinds of issues with just getting the data to a format where you can work on it. So this is an entirely different ballgame. It's much more kind of. There are lots of tools and techniques to deal with this, but that's not really going to be the focus of the course. So, in a sense, if you want to think about it, this data mining aspect will be almost missing from this course, even though it's part of the title. So what we are really going to look at is the machine\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"the data that you are given to build a model, and you keep a random subset of 20% to 30% away from the model building process. So it is unknown as far as the model or unknown data. It's the same as getting new data from the real world in some sense, and you can use it to validate. So, of course, this is a little bit of a disadvantage, because one of the features I mentioned right at the beginning is that typically machine learning expects that the model improves in quality with the amount of training data you give it. So you are in some sense artificially, or because you have no choice, more or less. You are building a suboptimal model, you have 100% data, but you are forcing yourself to use only 70% to 80% of the data available to build the model, because you need to validate it against some unknown data. So, one other strategy where you don't want to, I mean, one of the other disadvantages of doing this is that maybe that there are some minority anomalies in this data, and the\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"So clearly there is something going on in the background, and that background in our case is revealed through a test. Even tests have errors and all that. So it's really a problem that we have to grapple with in real life. And we will just assume for now that when we reach such a situation, we just take a majority decision. But realistically, if we get too many of these situations, and it's very difficult to decide, we have to ask whether the data has actually been collected correctly. So as we said, the main issue is really how to ask the questions and what this means. Sorry, sir, I have a question. Yeah. Sir, once we know that the data is young, we are making a prediction and it come out that young. So like other attribute is not relevant other than it has job or not, and then we are not going to look at other attributes. Yeah, but remember that which attributes to look like vary from one path to another. So maybe in this case, right, so what we are saying is that if they are young,\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"some way, right? For instance, if it is some data, what I don't, let us say today's everybody's favorite data set is COVID, right? So it might be collected statewide. So it might be that though the data is anonymous and randomized and all that it's actually the first. So many tens of thousands of things are from a particular state, and then next state, and so on and so on will collect it and collate it into one large state. So now if I say take the first 80% and build a model and leave the last 20% for training, for testing, it might be that the first 80% all corresponds to one part of the country and the last 20% corresponds to a different part of the country. And then we have this problem that if there is a regional variation in the distribution of this particular classification, it will get badly affected. So we need to do some random sampling. So this is a general problem. But here specifically, we want to randomize. It's out of the scope of this course to do random sampling, but\", metadata={'source': '/content/audio_4.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 2"
      ],
      "metadata": {
        "id": "JSs9zEEqYcCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_2=\" What are the advantages of the Apriori algorithm?\"\n",
        "\n",
        "unique_docs_2 = retriever.get_relevant_documents(query_2)\n",
        "len(unique_docs_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCiqMMdaXrEK",
        "outputId": "662fcf9d-197b-4db5-b957-9b4bc4720df4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. How does the Apriori algorithm offer benefits in data mining tasks?', '2. In what ways can the Apriori algorithm be advantageous for association rule mining?', '3. What are the strengths and benefits of using the Apriori algorithm for frequent itemset generation?']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_docs_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c0A-2KHYUno",
        "outputId": "fd28c430-613b-4265-c77a-f30abb54fda2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"these rules corresponding to the first k minus one columns, implying the last column. And this now becomes something where these rules that you get out of this give you a kind of classifier. So this is a kind of a simplistic way of doing supervised learning. So that kind of brings me to the end of this basic treatment of classes, this market basket analysis. So what you're really looking for is correlations between sets of items across transactions, which we formalize in terms of these association groups. X implies y. And what we saw is that as such, combinatorially, this is a very complicated thing to calculate, given the sizes of the sets involved. But we can use this appropriate principle both to identify the sets which are interesting, those which are frequent enough to be considered, and also to decompose them into rules. So you use the approoriate twice, once to find the frequent z, and second to decompose each frequent z into possible x implies y. And using this, we can build\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content='So we were looking at this market basket analysis, and towards the end of the discussion on association rule rules, if everybody who buys X is also likely to buy y. We said that one application of this association rule idea is to have these so called class association rules. So the right hand side was a category. We looked at an example with documents and we said, if a document has these words, it is also likely to belong to this category. So that brings us to the topic that is one of the central ones in machine learning, which is the idea of supervised learning. So in supervised learning, we have a set of items, and each item belongs to some category or class, and we distinguish these items in terms of some attributes. So you can think about this, as we will see shortly, you can think about these attributes as columns, right? So the attributes will be things like name, age, date of birth, whatever, income, stuff like that. So each row in the table will be one item, and there will be a', metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"data. So in this case it's literally the shape, because you have many possible trees you could construct, and the actual tree you construct depends on the data. And because you don't have any preconceptions about the data, you have to make sure that if there are these kind of patterns which disturb your algorithm, you will be able to counteract. So that's the purpose of this information gain ratio. It's not just the information gain alone, which is what we had initially. It's information gain divided by the entropy of the attribute. So is this clear what role this is playing? We are just saying that if you have a scattered attribute, then you discount the gain that you get from it by the scattering value. So more scattered attributes will, the net effect will be less because of the denominator. So in terms of our heuristics, what we said is that nonlinear functions work better in practice. And the genie index is in fact the one that is used in most libraries. But we should not blindly\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"itself would be a huge improvement, not even necessarily a very precise quantification, but just some indication. But all these are very speculative. So that's a huge challenge. So there are some theoretical things you can say which we may not, may or may not get to in this course. But it's a big problem. This is the so called generalization problem. So that's what basically, if you build association rules, is it going to hold or not? I don't know.\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"I'm not going to formalize this anymore, but it's very similar in idea to the prior year. So you can formalize this algorithm for yourself, but it's essentially saying the same thing, except here the thing is constraint, because we know that z is fixed. So once z is fixed, then we just have to start with each individual element in z as a possible right hand side, and then work forwards to that. We don't have to look at all. So for each z, we do this calculation within that set. So that's how you generate association rules once you have a frequent set. So within that set, you play this a priority game with respect to the size of the right hand side, and then you generate all the rules. Is this clear? Yes. So it's exactly the same as the previous one, just a slightly different presentation of the idea. Okay, so now, as I said, this whole thing so far has been largely a kind of algorithmic problem, right? In some sense, if I fix the set of items and the set of transactions and the set of\", metadata={'source': '/content/audio_2.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 3"
      ],
      "metadata": {
        "id": "Zm7HuEYkYf-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_3=\"What distinguishes training data in supervised learning, and what is its purpose?\"\n",
        "\n",
        "unique_docs_3 = retriever.get_relevant_documents(query_3)\n",
        "len(unique_docs_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2m1z2WXY3sX",
        "outputId": "58d2f0b5-7bbb-4c5a-86ac-a389415b51f9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What are the key differences between training data used in supervised learning and what role does it play in the learning process?', '2. How does training data specifically for supervised learning differ from other types of data, and what function does it serve within the learning model?', '3. Can you explain the unique characteristics of training data in supervised learning and elaborate on its significance in the training phase of machine learning models?']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_docs_3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42RAArzDZI4Q",
        "outputId": "73b0b8df-fc48-4ccb-f6ab-916243846f13"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='these attributes as columns, right? So the attributes will be things like name, age, date of birth, whatever, income, stuff like that. So each row in the table will be one item, and there will be a special column which indicates what category, depending on the classification problem that we are dealing with, what category it is. And the goal of supervised learning is to now construct, given a new row, a new item, a new combination of these attributes, which is not seen before, try to predict what would be the most appropriate category to assign. So these items which are given with the labels. So these are called labels. So these items are called training data. So we have labeled training data, which consists of some information which has been collected in the past, some historical information about items and their classification. And we want to find a model that generalizes the training data. So since normally this categorization corresponds to some kind of classification, this is', metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"it as code. Here it's different. So here you are given some kind of a format. You're saying you want to fit a line, and then you're given data, and then you're supposed to now take this line fitting thing and make the best line out of it. So it's not quite generating a program from a specification, but generating a concrete program from a generic program you can think of a model template as. So I just give you a line here which says a Y equal to Mx plus c. And here I want a specific m zero and C zero, which fits the given data in the best possible way, which is also part of the description of the algorithm. What does it mean for one line to be better than another line? So that's part of it. So the other side of this picture, as I mentioned, is when you don't have this training data, you don't have any examples that people have already labeled, but you are looking for some kind of patterns. So there is no guidance. So the thing about supervised learning is somebody should have gone\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content='hopefully not unfamiliar to you. Right? At the beginning, when we talked about supervised learning, we said that we are trying to predict something, and those predictions can be in two types. So one type is what we have seen in the decision tree, which is a classification. So the predicted value is one from a small range of possibilities. So we have been looking at a binary case, but even at an iris data set, there was a three way split. But we are choosing a class from a finite set. The other type of problem, which is very common, is to predict a numerical value. So we were talking in terms of predicting the marks of a student based on their board exam prelims and so on. So here is an example of that. So, supposing you want to predict the price of a house based on its square foot area. So you have, in this case, the training data will consist of the input is the house price for some given houses where you know the living area and you know the price. So these, now you would plot them.', metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"Our training data comes to us with labels. So for each input in our training data, we know the manually or however classified correct answer. So we are assuming those are the correct answers because otherwise our model building process will not make sense. So assuming that those are correct answers, we are building our model. So that's the only source of inputs and outputs where we know the answer. So the only solution really is to use that. So we have to take that training data and keep some of it aside because we are optimizing our model, as we saw in the decision tree, we are optimizing it to give correct answers on the training data. So therefore it doesn't make sense to evaluate it back on the same inputs which are used to construct the tree. But if I withhold some data, so I basically take my training data as a whole and then I kind of split this and say, do not use this part, only use this part to build my model. Then the model that I built from here, I can ask, how does it do\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"the data that you are given to build a model, and you keep a random subset of 20% to 30% away from the model building process. So it is unknown as far as the model or unknown data. It's the same as getting new data from the real world in some sense, and you can use it to validate. So, of course, this is a little bit of a disadvantage, because one of the features I mentioned right at the beginning is that typically machine learning expects that the model improves in quality with the amount of training data you give it. So you are in some sense artificially, or because you have no choice, more or less. You are building a suboptimal model, you have 100% data, but you are forcing yourself to use only 70% to 80% of the data available to build the model, because you need to validate it against some unknown data. So, one other strategy where you don't want to, I mean, one of the other disadvantages of doing this is that maybe that there are some minority anomalies in this data, and the\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"don't have any examples that people have already labeled, but you are looking for some kind of patterns. So there is no guidance. So the thing about supervised learning is somebody should have gone through and given you this information before. Now, this information could have been collected over time. Maybe it's not manually collected, like the school marks. The school marks over time. Everybody has. I mean, the school keeps a record of what happens in the model exams, what happens in the board exam. So it comes automatically. The bank will be keeping records of all the loans that it bought and what happened to the customers who bought loans. So this historical information is accumulated over time. On the other hand, in some situations, like the example I mentioned about classifying paintings, now, when you go into a museum or you go into some art gallery or something, the painting doesn't have a label on it, saying, this is a painting of this type and this is a painting of that\", metadata={'source': '/content/audio_1.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 4"
      ],
      "metadata": {
        "id": "tD6kTzq3Yip7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_4=\"What is cross-validation?\"\n",
        "\n",
        "unique_docs_4 = retriever.get_relevant_documents(query_4)\n",
        "len(unique_docs_4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOcBPsiEZOvy",
        "outputId": "70873c55-38bb-45ee-c5e3-85f9910c774a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. Can you explain the concept of cross-validation?', '2. How does cross-validation work?', '3. What are the benefits of using cross-validation in machine learning?']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_docs_4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvd8MmarZjuU",
        "outputId": "9c1610ad-5677-4ba4-de35-ccd9fc6ac615"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"cross validation, cross validation. What I'm saying is that you do not make a hard and fast choice once and for all to split your data this way. So you're taking your data and you're saying, hey, first let me leave out this 20% and build one model. So I am building multiple models. I'm building m one, m two, m three, m four, and m five. Each of these models is built using 80% of the test data, but a different 80%. So each of them leaves out some 20%. So I'm building five different models and comparing what I see in all these five. So the first thing is that if this approach is good, then in general, these five models should behave in a similar way with respect to the outcome. So if they all behave similarly, then I can decide what to do. So there are two strategies. I can, I can somehow combine these by saying, build a composite model, or I can say, okay, this strategy is working for me. So this approach that I have chosen seems to be a good way to build a model for this data. So now\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"is this method actually giving me something reasonable? And is it giving me better something than something else? So, for this one strategy is so called cross validation. So what you do is you take some fraction, say, in this case, it is one fifth. So 20%. So you take your data and you leave out. So you randomize it into five groups, which preserve, in some sense, the random behavior, so they partition it five ways. So if I leave out any one of these groups, it's as though I have taken the remaining 80% for training and this 20% for test. But because I don't want to actually think of it this way, I will actually keep doing this for each one. So I will leave out the first 20%, then I will leave out the second 20%. So I will do this. If I have like k chunks, I will do this k times in each chunk. Each model I will build using all but one of these chunks. So this then will solve this problem of many problems. It will solve the problem where you don't have enough data to begin with to\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"chunk. Each model I will build using all but one of these chunks. So this then will solve this problem of many problems. It will solve the problem where you don't have enough data to begin with to afford to not look at some of it, because the interesting things may be in that sum of it. Now, what will you do with this whole thing? Well, you have five. Typically, each of these models are going to be different because we saw with the decision tree, it's kind of the questions that you ask depend on the distribution of the different roles. And if the distribution changes slightly, one attribute might get better entropy than another attribute. And you will choose that question first. So the shape of the tree might vary a lot between these, so you may not get the same. Okay, so the question about cross validation, cross validation. What I'm saying is that you do not make a hard and fast choice once and for all to split your data this way. So you're taking your data and you're saying, hey,\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"So what you're really asking at some higher level is that machine learning approach. So, remember, we have seen only decision trees, but we are going to see many models. So there are clearly many ways to build models. And the reason, whenever you see that there are many ways to do something, it's only because there is no guarantee that a given way is the best one. So more or less what you want to validate is, I have, say, seven different strategies to solve this problem using machine learning. And I want to check which one of them is good. So I need to compare. So once I compare, then I'm stuck with whatever I have. It may be that this gives me only 70% accuracy or some such metric, but the other models give me even less. So it's more really a comparative statement to start with, which is, is this method actually giving me something reasonable? And is it giving me better something than something else? So, for this one strategy is so called cross validation. So what you do is you take\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"instead of removing the widest blue, you can try to remove the narrowest green or the two narrowest greens or something and see what happens. So you just have to copy paste this code and just change the condition and all the visualization. You can just run it as it is and see, I have not done it, but I think you might get something different. So the predict outcome is not predictable. I cannot guarantee you that one or the other will produce a different output, but it can happen. And this is generally something which one has to be mindful of when you're dealing with decision. One reason that this is important is remember that we talked about this cross validation. So what do we do in cross validation or even in normal validation? In normal validation or in cross validation. In either case, you pull out some random data, set data points from your input and build a tree on the rest. Now the question is, the model that you built from the rest, is it a stable structure in the sense that\", metadata={'source': '/content/audio_5.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 5"
      ],
      "metadata": {
        "id": "ucRsYtKDYlWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_5=\"What is the process of building the decision tree classifier, and how is it trained on the dataset?\"\n",
        "\n",
        "unique_docs_5 = retriever.get_relevant_documents(query_5)\n",
        "len(unique_docs_5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFJZmlmdZwtr",
        "outputId": "248c9cd8-bc6b-48d7-f7e9-631f230a61f0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. How is the decision tree classifier constructed, and what is the training process on the dataset like?', '2. Can you explain the steps involved in building a decision tree classifier and how it is trained using a dataset?', '3. What are the procedures for creating a decision tree classifier and how does the training process with the dataset work?']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_docs_5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zh8ems__aAwX",
        "outputId": "269dcf15-837d-478b-b21c-45d5ede0377c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"better. So here, priority, they are saying, let us not construct deep trees. So whatever tree we construct, we are going to stop it when it reaches. So it's a two step process, right? So what you do is you first say what are the parameters for the decision tree classifier? So he says, I want a decision tree classifier to be set up with this random state 42 and which will not grow to more than depth two. And then I have to actually construct the classifier for a particular data set. So that's the next thing. So I use this fit function, right? In some sense this creates a decision tree object with certain operating parameters, and then you pass it the training data in the form of the input and the output x and y, and say fit. So the important thing to see is that this is all you need to do to build a decision tree in python. If you have the data, what you need to do manually is to probably decide. So the question is, can we specify a certain threshold? A certain threshold of what? Yes\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content='of irises that we saw, so it will be essentially a number, zero, one. And now to get this decision tree classifier, we have to take this function that we have imported. So we say take a decision tree classifier. We have to provide it with, again some random state because it uses some randomization inside it. And now this is something which we have not talked about. So we discussed that we will build a decision tree normally until we either achieve some pure leaf nodes where the classification is uniform, or we run out of attributes. Now this is a numerical case, so we are not going to run out of attributes because we are going to run out eventually when we run out of data points. But we will end up with these very long slicing, the width between 1.5 and 1.71.5 and 1.6 and so on. So what you can do is you can tell the decision C classifier not to expand beyond a certain depth. And here it has fixed it to be depth too. So we will come back to this at a later lecture, which is, this is', metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"predictions which were learned. So that is the training data, and it produced a model which we visualized there. Now, when I want to actually run this model, what I have to do is I have to give it an input and ask it what do you think is the output? So things, effectively, the output will be ambiguous because there will always be some degree of mixing between the classes. So what it will do is it'll first compute according to what. In this decision tree, it will basically follow the input to the leaf. And in that leaf it will look at the distribution of nodes and it will assign a number to each one so that they are normalized to add up to one. And after I normalize and add up to one, I take the highest probability one as the answer. So if you now insist that this tree should tell you one of the three classes is obviously going to predict the class with the highest probability. So it gives me one. The reason it gives me one is that the three classes are encoded as zero, one, two. So\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"is related to that also where to save. So this is some setup which is not relevant to decision. This is more to do with organizing how and visualize things and save those figures for later. Okay, so now this is the real part. So, so Scikitlearn has some built in data sets and one of them is this iris library. So the way to get the iris library into your data set is to import this function called load iris. And Scikitlearn has a bunch of models predefined. So what we have been looking at is decision trees as classifiers. So we are going to import from the subset of models of form tree. We are going to import this model called decision tree classifier. So the first thing that we do is we actually load this data set. So we call this load iris function, and then we are going to remember, the iris data set had two measures. It had the petal and the sepal, length and width. So it had four quantities. But we decided we will only use two quantities. So this is what this is doing. It's taking\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"So last time we started with supervised learning in earnest, and we were looking at this decision tree model. So, in a decision tree model, you have essentially a table in which the columns are the. So each item, a row in the table and one special column denotes the class that you are trying to classify. And we assumed it's a binary classification. So the class is zero, one, or yes, no, or whatever you want to interpret. The two categories are. So the algorithm was very simple. At a high level, what you want to do is choose an attribute based on the answer to the question that you're asking about that attribute. The data splits, and then you take each partition, ask another question. So that's the thing. And you want to come down and keep asking questions until either you have reached, like these situations here, a partition in which all the values of the class variable are the same. So you have a uniform class level or you have run out of questions, because we can only ask in any path\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"will work. So you have to have a little bit of an idea about what's going on behind the algorithm in order to make best use of it when it is not producing an answer which is acceptable to you. But as far as using it per se, all you need to do is set up the appropriate. So here it's a decision tree, but pretty much the same. So these two steps that you see here, these two steps. So this is a pretty canonical style of programming. In this scikitler, you import the right type of classifier, you build a classifier of that type with the parameters that you think are reasonable for your application, and then you pass it the training data and say fit. Now here, one thing we have not done, for instance, is to segregate out, we talked about training and test data. We have not done that. So that's also something that you can do. Before you set it up, you might get all the iris data and then you might want to keep part of it aside. So we will see as we go along how to do all these things. But\", metadata={'source': '/content/audio_5.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 6"
      ],
      "metadata": {
        "id": "Mwa_G1elYoxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_6=\" What is the challenge associated with supervised learning? \"\n",
        "\n",
        "unique_docs_6 = retriever.get_relevant_documents(query_6)\n",
        "len(unique_docs_6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2vNjV-DaJ3h",
        "outputId": "23d4ffb6-b96a-4b52-8778-7b4d62ff5947"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What are the difficulties linked to supervised learning?', '2. What obstacles are commonly faced in supervised learning?', '3. What are the main challenges of implementing supervised learning algorithms?']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_docs_6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycUG6Imhazim",
        "outputId": "2a43e314-5d3d-4a03-f229-76078dcf0754"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"I can start with something which actually, in the original thing, is separated by a simple plane, right? So I have this plane here which separates the green from the yellow. But then, believing that it'll be better to work in two dimensions, I actually unroll it. And then I find I have to deal with these multiple bands. So now it's not so obvious anymore because I do have lines, but I have many of them, and I have to decide between which bands it is, green, and which bands it is, this yellow color. So the peril of supervised unsupervised learning is that you don't know that the step that you are taking in advance, you don't know is going to be good or bad because it's unsupervised. You're just taking a guess in some sense, but then you have to have some way of evaluating whether the guess was good or not. And then going back and doing it again may be a different. So this is kind of a quick overview, a summary of the main things that we will be looking at. So, supervised learning and\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"don't have any examples that people have already labeled, but you are looking for some kind of patterns. So there is no guidance. So the thing about supervised learning is somebody should have gone through and given you this information before. Now, this information could have been collected over time. Maybe it's not manually collected, like the school marks. The school marks over time. Everybody has. I mean, the school keeps a record of what happens in the model exams, what happens in the board exam. So it comes automatically. The bank will be keeping records of all the loans that it bought and what happened to the customers who bought loans. So this historical information is accumulated over time. On the other hand, in some situations, like the example I mentioned about classifying paintings, now, when you go into a museum or you go into some art gallery or something, the painting doesn't have a label on it, saying, this is a painting of this type and this is a painting of that\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"validate at the end somehow that what we are doing is right. But in practice what you're saying makes sense that really to do supervised learning, it's much more realistic to do it in collaboration with somebody who understands a domain rather than just doing it blindly. Because also when you do it blindly, you might come up with solutions which make no sense. You might actually identify features which contribute to a decision purely by statistical coincidence, and no human would actually do that. So there's another issue which comes up of interpretability or explainability of your answers. So there are various factors which get into this. It's only not only accuracy which matters, but also whether your model is sensible in something, so that you can justify it. So these will all come as we go along. So let's proceed with this. So the first thing is that because the choice of attribute to question will determine the structure of the tree. So here are two different trees. So this is\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"So supervised learning is by far the more attractive and the more, should I say, attractive, I guess it's the more attractive aspect of machine learning. This is what people are really looking for. But unfortunately, as I said, to do effective supervised learning, you need to actually get a lot of training data. And this is sometimes expensive. Very often it is expensive. So unsupervised learning is often the starting. A few. Couple of years back, there were three turing awards given for deep learning. So one of them was given to Jan Lakun. And his observation is that if you think of this whole thing as a cake, then actually most of the cake is actually unsupervised learning. So supervised learning is just a thin layer on the top, but it's like the icing on the cake. So when we go to a bakery and we see an attractive cake, what we're attracted to is the surface, the icing surface. But if the inside is made of sodas, then the cake will not taste good. So you have to be careful not to\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"and we see an attractive cake, what we're attracted to is the surface, the icing surface. But if the inside is made of sodas, then the cake will not taste good. So you have to be careful not to be fooled by the icing. So the unsupervised learning is also important. But supervised learning is where all the, in some sense, the jazz is. That's what people spend up spending a lot more time on. And we will also probably spend, in terms of lecture hours, more time on supervised learning than on unsupervised learning. But don't forget that the bread and butter work of machine learning actually rests on a strong foundation of unsupervised learning. Okay, so I'm going to start with something after this. So if there are any questions at this point, I'll just take a minute to pause and ask, then continue. Okay, you. Fine. So now let's start our first topic. So this is a topic which is actually of historical interest because it is how this whole idea about learning from data and data driven\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"the idea. Okay, so now, as I said, this whole thing so far has been largely a kind of algorithmic problem, right? In some sense, if I fix the set of items and the set of transactions and the set of the two thresholds, the support and the confidence threshold, then for any x, comma y, either it is true or it is not true. So there is a fixed answer. So the answer is known. So this is something where I can actually validate that you have either enumerated all the association rules or you have missed out something. So technically there is no learning involved. It's more an algorithmic question. So how does one connect this to learning? Well, in specific context, you can interpret these rules as classification rules. So, remember we said supervised learning says, if this is there, then it is a fraud. If this thing is not there, then it is not a fraud, and so on. So, let's look at a typical example. So, supposing we have some question which is about labeling the topic of a document. So our\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"difficult question. All you can say is that given what we are doing now, this is what, then it becomes really an experimental issue. So this is the hardest question in machine learning, which is, how do I improve the quality of the classifier? So there is no answer to that question, which is obvious. So you have to start experimenting. You have to do something. So there is something called feature engineering. You might want to come out with some combination of features. So some of you may have heard of this thing, for example, which is used in medicine called body mass index. So, body mass index is some combination of some formula involving height and weight. So you might have a data set in which you have something like height and weight usually classify using height and weight. It doesn't work. But if you combine it in this strange way using body mass index, then you actually get a better class. So this is an example of what is called feature engineering. You make new features out\", metadata={'source': '/content/audio_4.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 7"
      ],
      "metadata": {
        "id": "U-8OIfE_Yqau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_7=\"What considerations should businesses keep in mind when applying market basket analysis to understand customer behavior?\"\n",
        "\n",
        "unique_docs_7 = retriever.get_relevant_documents(query_7)\n",
        "len(unique_docs_7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgMG5nt8a_Jq",
        "outputId": "0f4ab0a1-364a-42e6-c256-9ca83fbb5bb6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. How can businesses effectively utilize market basket analysis to gain insights into customer behavior?', '2. What factors should businesses take into account when implementing market basket analysis for understanding customer behavior?', '3. What are the key considerations for businesses looking to leverage market basket analysis in order to comprehend customer behavior better?']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_docs_7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxfUdeTGbN6l",
        "outputId": "8d817a57-2d9e-4580-d995-74148c2d738b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"maybe you cannot do. So that's another thing about this whole model building thing is that the same model may or may not be capable of tackling every different question that you ask. So this model, some of these things maybe you can segregate and answer. But if, as you're saying, you want to compare behaviors across customers of different types, you cannot, as you said, sensibly aggregate them into a single market basket model because they all have different profiles. So you will have to, in the example that you gave, you would have to first separate out the data for these, say, the people who buy one month groceries at a time from those who buy at more frequent intervals and then analyze them separately, because together the data doesn't actually make sense. So last time, if you remember, I talked about this unsupervised. So maybe one thing that you need to do when you have something large is to basically look at the transactions and categorize them according to some criteria, maybe\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"or there are a bunch of transactions which have come together, and then you want to look at each of these groups as a whole and do some kind of. So not just one consumer, but across consumers, you want to say, as a rule, consumers who do this also do that and so on. So these are all refinements. And there are many situations where these are important and interesting, but this model as such, this kind of market basket analysis approach is very naive. And so that's why these things will come up in different contexts. But we are not going to explore it anymore in this context, because in this context, the kind of model that you build, this association rule model is rather simplistic. So we are going to look at more sophisticated models as we go along. And there you can ask the same question, but it'll take a slightly different format. Does this also include tendencies like, if there is one person who tends to buy all his groceries once a month, then they will have a bigger basket size.\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"Maybe it is true that at certain times, people who buy diapers also buy beer, but it at least gave some confidence to the idea that you can use this kind of information to make decisions. So from that perspective, it possibly had a role to play. But anyway, so these are the origins of this problem, and it's called market basket analysis because it's really talking in this context about shopping baskets. I have a shopping basket when I reach the counter to pay. So question is, what is in that basket? If it has x, does it also have y? So this is a very literal interpretation of markets and baskets. But the notion of markets and baskets can be a little bit more abstract. So, for instance, if you are thinking about, especially nowadays, given the number of elearning platforms, there's a lot of emphasis on trying to structure topics in such a way that students who find difficulty, you can identify why they are facing difficulties. So you want to think of these concepts that you're teaching\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"these rules corresponding to the first k minus one columns, implying the last column. And this now becomes something where these rules that you get out of this give you a kind of classifier. So this is a kind of a simplistic way of doing supervised learning. So that kind of brings me to the end of this basic treatment of classes, this market basket analysis. So what you're really looking for is correlations between sets of items across transactions, which we formalize in terms of these association groups. X implies y. And what we saw is that as such, combinatorially, this is a very complicated thing to calculate, given the sizes of the sets involved. But we can use this appropriate principle both to identify the sets which are interesting, those which are frequent enough to be considered, and also to decompose them into rules. So you use the approoriate twice, once to find the frequent z, and second to decompose each frequent z into possible x implies y. And using this, we can build\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"I talked about this unsupervised. So maybe one thing that you need to do when you have something large is to basically look at the transactions and categorize them according to some criteria, maybe by the size of the basket. And then you will find that there are maybe a lot of people who buy five items at a time. There are people who buy ten to 20 items at a time. There are people who buy 40 items at a time. And maybe you need to then apply the model separately to each of these clusters. So all of these things are not a one size fits all solution. And also, it's not that one approach on its own is going to solve the problem at any given time. So you need to have a kind of think of it as a toolbox, and then you need to apply some combination of tools for a given problem based on what you can do about analyzing the problem. So what we are looking at is the question of what these tools are. But they will not work out of the box in some sense, in every situation, like you said. So that's\", metadata={'source': '/content/audio_2.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 8"
      ],
      "metadata": {
        "id": "Os5HM-TJYroq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_8=\"How does the genie index contribute to decision tree construction, and why is it favored in practice, particularly in Python libraries?\"\n",
        "\n",
        "unique_docs_8 = retriever.get_relevant_documents(query_8)\n",
        "len(unique_docs_8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pa9UY62bSaD",
        "outputId": "aaf9a767-bb9b-473a-cc8c-ce30e5332259"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What role does the genie index play in the construction of decision trees, and what makes it a popular choice in practical applications, especially within Python libraries?', '2. How does the genie index impact the process of building decision trees, and what factors make it a preferred option in real-world scenarios, specifically within Python libraries?', '3. In what ways does the genie index influence decision tree construction, and what are the reasons behind its popularity in practical use, particularly within Python libraries?']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_docs_8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4OfqyWMbirg",
        "outputId": "0ff65a91-8128-487d-91a3-055b0dd54806"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"our logic earlier was that somehow nonlinear is better empirically, whatever that means. So empirically just means that somebody built these systems and found that if they use these kind of things, it works better. So empirically, entropy is, I mean, theoretically, entropy is slightly steeper, but entropy requires us to compute logs and all that, whereas genie index is just this computing squares of numbers. So genie index is actually easier to compute. And if you look at, as we will see, libraries in Python and all that, typically they use genie index. So although there is a very famous system built using entropy, which was one of the early decision tree systems, the ones that we will practically encounter in Python, libraries actually use the genie index. Basically. I'll stop with this right now. So what we are saying now is that we want to choose the next question to evaluate the next question to ask. So for the next question to ask, we have to say something about the quality of\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"tree for it. So this tree, which we will see later, this python library for decision trees, actually constructs from the iris data set, this decision tree. So what you do is you start with these 150 samples, and initially there are three categories, so there are 50 in each category. And remember that if there are at any point when you have an impure sample, you have to decide on something. So this particular algorithm, they're equal. It happens to just choose one of them more or less at random as the category to predict nation. So this last line is the prediction, this is the distribution of values. This is how many samples there are. And this is the actual genie index calculation. If you think about it, it is the three samples are each with one third. So you have one third squared plus one third, one third squared, one nine. So one minus one 9th plus one nine plus one nine, which is one minus one thirds. So that's why you get this in the index of zero, 67. So this is the initial.\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"right now. So what we are saying now is that we want to choose the next question to evaluate the next question to ask. So for the next question to ask, we have to say something about the quality of the impurity before and after. So if I just look at the raw ratio of the impurity, I have a motion of just that impurity percentage. But that gives me this linear relationship, which apparently is not that good in practice. So I want a nonlinear way of giving some impurity value to this ratio of yes or no, and then we can use either this entropy idea or this genie index, and it gives me a sharper curve, and this sharper practice turns out to be better. So that's how you build a decision tree. You build a decision tree by applying one of these two nonlinear things. If you come up to the third thing, it's also fine. It's just that these two happen to be standard things. They are both borrowed from other areas where they have a long and established history. So, as I said, genie index comes\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"data. So in this case it's literally the shape, because you have many possible trees you could construct, and the actual tree you construct depends on the data. And because you don't have any preconceptions about the data, you have to make sure that if there are these kind of patterns which disturb your algorithm, you will be able to counteract. So that's the purpose of this information gain ratio. It's not just the information gain alone, which is what we had initially. It's information gain divided by the entropy of the attribute. So is this clear what role this is playing? We are just saying that if you have a scattered attribute, then you discount the gain that you get from it by the scattering value. So more scattered attributes will, the net effect will be less because of the denominator. So in terms of our heuristics, what we said is that nonlinear functions work better in practice. And the genie index is in fact the one that is used in most libraries. But we should not blindly\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"about this. Okay, so now one of the problems with decision trees, which you will see later on, is also that a lot of the construction of the tree depends on these statistics. Because we said that we look at which attribute has the maximum discrepancy, whether it's entropy or genie index or whatever, which one of them will improve it the most. And that improvement, or lack of improvement really depends on the distribution of the different inputs at that point in my table. So if there is a borderline case, it could be that by shifting one value here or there, the attribute which has the highest improvement might shift from one to the other. And then I ask a different question. The moment I ask a different question, the tree becomes radically different because the question pattern changes and the split will change and so on. So here is to illustrate that what they have done is they have first, so y equal to one. Remember that the categories are zero, one and two. So they are picked out\", metadata={'source': '/content/audio_5.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 9"
      ],
      "metadata": {
        "id": "LcwacsALYtXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_9=\"What is the harmonic mean, and how does it enable the combination of precision and recall into a single metric?\"\n",
        "\n",
        "unique_docs_9 = retriever.get_relevant_documents(query_9)\n",
        "len(unique_docs_9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MGa7NiFb0tw",
        "outputId": "132cd104-273b-48e9-c435-6ac0581f9963"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. How does the harmonic mean facilitate the integration of precision and recall into a unified metric?', '2. Can you explain the concept of the harmonic mean and its role in combining precision and recall into a single measurement?', '3. What is the significance of using the harmonic mean to merge precision and recall into a singular metric?']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_docs_9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVPUuyzQb_Ok",
        "outputId": "207914f5-7d25-4788-ccf2-28daa5d3d442"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"separately and a recall separately and let me choose, I want a single number. There is this thing called harmonic mean. So the harmonic mean is one upon p plus one upon R. So remember, p is precision one and it's one upon that. So it's the reciprocal of the mean of the reciprocals. So if you work this out, you get two PR on two p plus. So this is sometimes called the f score. Right? So this is a way of combining the precision score and the recall score into a single number. But as I said, in most cases, it really depends on the application. So either you want something based on the context which has higher precision or higher recall, and you choose. So these are some of the ways in which we can actually. So, remember, our starting point was that we wanted to evaluate classifiers. So we said first we have a strategy for evaluating classifiers, so we can test them by using this test set and so on. But what is it that you want to measure on the test set? So that we have this wide variety\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"If you want the other one go up, the first one will come down. So now I just mentioned that these are not the only things so people also talk about. So there is, for instance, the corresponding thing to precision for the negative part is specificity. Right? So what is the ratio of correct negative results compared to all negative results as a true negative? So that's with respect to this column, then accuracy, of course, is just saying how many of the correct things are there with respect to the whole thing. So that also can be measured easily from this. And then there's something called thread score and so on. So I'm not going to talk about it. And just for completeness, I will mention that sometimes people want to say that we want a single number. It's not good to give me a precision separately and a recall separately and let me choose, I want a single number. There is this thing called harmonic mean. So the harmonic mean is one upon p plus one upon R. So remember, p is precision\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"first we have a strategy for evaluating classifiers, so we can test them by using this test set and so on. But what is it that you want to measure on the test set? So that we have this wide variety of things, but most commonly this precision and recall are the ones that people are interested in, and they will choose which one to emphasize. Depending on the context. Depending on the context, you may want precision predominant. You may want recall to be predominant. So there, it really becomes a domain specific. So I'll stop there. Any questions, sir? Like, given a precision or recall value, what we need to change in the algorithm so that we could come to the certain value of precision and recall. So that is a harder question. Right. So what is it that you need to change? So that's a very difficult question. All you can say is that given what we are doing now, this is what, then it becomes really an experimental issue. So this is the hardest question in machine learning, which is, how\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"pancreatic cancer, and blurt it out to the patient. You'll make them both. So in many situations, it's not clear, but what is generally clear is that these two things are usually not aligned. You cannot normally do both together. You cannot have normally. If you have to trade off either you want recall or you want precision. And one will go up.\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"data. So in this case it's literally the shape, because you have many possible trees you could construct, and the actual tree you construct depends on the data. And because you don't have any preconceptions about the data, you have to make sure that if there are these kind of patterns which disturb your algorithm, you will be able to counteract. So that's the purpose of this information gain ratio. It's not just the information gain alone, which is what we had initially. It's information gain divided by the entropy of the attribute. So is this clear what role this is playing? We are just saying that if you have a scattered attribute, then you discount the gain that you get from it by the scattering value. So more scattered attributes will, the net effect will be less because of the denominator. So in terms of our heuristics, what we said is that nonlinear functions work better in practice. And the genie index is in fact the one that is used in most libraries. But we should not blindly\", metadata={'source': '/content/audio_4.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 10"
      ],
      "metadata": {
        "id": "7-e-75MWYuxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_10=\"What are the implications of choosing a large or small learning rate?\"\n",
        "\n",
        "unique_docs_10 = retriever.get_relevant_documents(query_10)\n",
        "len(unique_docs_10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5ConL9BcETR",
        "outputId": "05faad02-5a11-4634-d3c4-820939f2c307"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. How does the choice between a large and small learning rate impact outcomes?', '2. What effects can be expected from selecting a large versus small learning rate?', '3. In what ways do the implications differ when opting for a large or small learning rate?']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_docs_10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuUIFaWYcRCv",
        "outputId": "e9bdabb1-4ef2-484e-e62a-d341e6a58e76"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"sample. So you're not saying that the examples have every possible situation that you are likely to use for judgment, but you're really talking at the level of probabilities. You're saying that the probability distribution of the training is somehow identical to that of the unseen data on which you are going to make the prediction. So this is going to be your training and this is going to be your prediction. So this is a fundamental assumption. If this assumption does not hold, then whatever you try to do doesn't really have any basis. So this is something that we have to assume, and we will talk about later on what this assumption means. Maybe not in this course, but in later courses. But for now, let us assume this. So the next question is, what does it mean to learn? Right? So, as we saw in this loan data set, 60% of the loans were granted, so nine by 15. So if I were to just say yes, supposing I don't have an interesting model and I just say yes all the time, every answer that\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"saw in this loan data set, 60% of the loans were granted, so nine by 15. So if I were to just say yes, supposing I don't have an interesting model and I just say yes all the time, every answer that every person who comes, I just say yes, give this person. And if the previous assumption is actually valid, that the distribution of training examples and the distribution of unseen data are actually the same, then if I used to give 60% of the training people loans, then giving 60% of the unseen data loans is something that I would expect to do. So if I just keep on saying yes, then 60% of the time I would be correct, which might suggest that you're doing better than half, but actually the answer is biased, so you're not doing any better than random guessing. It's like if I toss a fair coin and I keep saying heads all the time, then I should expect to call it right 50% of the time. So doing 50% of the time in a fair situation is no better than random guessing in this kind of situation,\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"I was trying to make is that as a gets more scattered, there are more values and the ratio of each value is smaller, then that denominator will become larger. So whatever gain you get on the top. So remember the problem was that these give us high gain on the top, but we are dividing that gain by the scattering of that attribute. So if we have high gain divided by high scattering, then the net normalized effect is lower. So you might choose an attribute which actually gives you a lower information gain overall, because the attribute itself is less static. And that's the thing that we want to achieve. Just to emphasize that when you have a machine learning algorithm, see generally, what is a machine learning algorithm doing? It's trying to describe the shape of the model based on the training data. So in this case it's literally the shape, because you have many possible trees you could construct, and the actual tree you construct depends on the data. And because you don't have any\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"these situations here, a partition in which all the values of the class variable are the same. So you have a uniform class level or you have run out of questions, because we can only ask in any path one question per attribute. And if in case we have the second case where we run out of questions, but we have not reached a so called uniform node, then we just use the majority as a prediction. So we will see an example of this later as we go. Yeah, so what we also said was that, as we can see here, we have two different trees, and the trees are not the same shape. And we argued that a smaller tree is probably a more desirable one. It's more explainable. And we also claim, without at the moment justifying, that it also has a better generalization property. That is, it will move from the specific training data to the unknown data in a better way. So how do we build small trees? Well, computationally, we said, I claimed at least that it was infeasible to build the smallest tree. So you\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"turn out that small trees also solve the other problem that we mentioned earlier about generalization. So, question is whether or not this model is better for the data you have not seen compared to the data that you are training on. So is it too specific for the training data that you are building. So if you ask more questions, then you are really focusing on more individual eto synchronicity or complications in the data that you see. That might be an isolated case, and it may not appear in general, so maybe you should ignore it. I will show you later on an example of this. But the problem is that as a computational task, so I have not yet told you how to do this, but if you wanted to make this your goal, build the smallest tree, which, among other things, gives you this kind of a nice boundary of uniform answers. The problem is that this is a task which is what is known in computer science as NP complete. Now, you don't need to know what this is, but if you are interested, I will put\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"the range that I would normally expect, and they typically indicate something. And therefore, I believe you have. So you narrow down the suspects. So it's the same as saying that you have a complex explanation which says, test whatever. Number one is this and number two is this, and number three is this, and number 15 is this, and therefore. Or you say, number two is this and number seven is that. And therefore, in any context, generally, a smaller model is usually better, but everything is. And this is a common feature of anything we do, machine learning. Nothing is. You cannot say it universally, as I said, like newtonian mechanics and quantum mechanics, you could always have a situation where the more complicated model is required. So, yes, certainly small trees are more explainable. It'll turn out that small trees also solve the other problem that we mentioned earlier about generalization. So, question is whether or not this model is better for the data you have not seen compared\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"the data that you are given to build a model, and you keep a random subset of 20% to 30% away from the model building process. So it is unknown as far as the model or unknown data. It's the same as getting new data from the real world in some sense, and you can use it to validate. So, of course, this is a little bit of a disadvantage, because one of the features I mentioned right at the beginning is that typically machine learning expects that the model improves in quality with the amount of training data you give it. So you are in some sense artificially, or because you have no choice, more or less. You are building a suboptimal model, you have 100% data, but you are forcing yourself to use only 70% to 80% of the data available to build the model, because you need to validate it against some unknown data. So, one other strategy where you don't want to, I mean, one of the other disadvantages of doing this is that maybe that there are some minority anomalies in this data, and the\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"that one input that you get might be one, which is kind of very extreme, and so it might give you a wild swing, and then you might oscillate a lot more. So it might take you longer to converge, but faster to compute. So there's a trade off between doing each iteration. You are making a lot of adjustments fast, but those adjustments are less controlled. The other way in between is take small batches. If you have 1 million things, you break it up into 1000 batches. So you do 1000 points. So you get some averaging out of the behavior of the function, make an adjustment, then you do another 1000, make an adjustment and so on. So all these things are actually used, we will see later in neural networks and all that. But in this linear prediction also, that's where they arise. I mean, that's the origin of this. So this is, of course, as you have all probably read. So this strategy is conventionally called linear regression, but it's basically this iterative linear prediction, which you can\", metadata={'source': '/content/audio_5.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Co-here Re-ranking"
      ],
      "metadata": {
        "id": "LaxBkgs0cYY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install cohere"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JroYVq1cxrR",
        "outputId": "7010afc2-9b2c-43e9-9f66-89c891f83b09"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cohere\n",
            "  Downloading cohere-5.2.2-py3-none-any.whl (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.3/144.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
            "  Downloading fastavro-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.27.0)\n",
            "Requirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.6.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.16.0,>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.15.2)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
            "  Downloading types_requests-2.31.0.20240406-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (4.10.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.2->cohere) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.2->cohere) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (2.0.7)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<0.16.0,>=0.15.2->cohere) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<0.16.0,>=0.15.2->cohere) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<0.16.0,>=0.15.2->cohere) (2023.6.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<0.16.0,>=0.15.2->cohere) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<0.16.0,>=0.15.2->cohere) (6.0.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<0.16.0,>=0.15.2->cohere) (23.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.2.0)\n",
            "Installing collected packages: types-requests, fastavro, cohere\n",
            "Successfully installed cohere-5.2.2 fastavro-1.9.4 types-requests-2.31.0.20240406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cohere\n",
        "co = cohere.Client('w8CnnlzVol2aZEiirZNLUs0onAqXUUYBZCw2Oj7g')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T14:57:02.730508Z",
          "iopub.execute_input": "2024-04-03T14:57:02.731273Z",
          "iopub.status.idle": "2024-04-03T14:57:03.104304Z",
          "shell.execute_reply.started": "2024-04-03T14:57:02.731241Z",
          "shell.execute_reply": "2024-04-03T14:57:03.103506Z"
        },
        "trusted": true,
        "id": "fCgmNQTGL7Yx"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 1"
      ],
      "metadata": {
        "id": "ZPXV4reDdWvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_query1=[]\n",
        "for i in range(len(unique_docs_1)):\n",
        "  retrieved_docs_query1.append(unique_docs_1[i].page_content)\n",
        "\n",
        "print(query_1,\"\\n\")\n",
        "results = co.rerank(query=query_1, documents=retrieved_docs_query1, top_n=5, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
        "for idx, r in enumerate(results.results):\n",
        "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
        "  print(f\"Source Document: {unique_docs_1[r.index].metadata}\")\n",
        "  print(f\"Document: {retrieved_docs_query1[r.index]}\")\n",
        "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzOgFYRCct35",
        "outputId": "eac883d2-a18d-4cd0-d041-ea85517c7162"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " What are some challenges associated with data collection? \n",
            "\n",
            "Document Rank: 1, Document Index: 0\n",
            "Source Document: {'source': '/content/audio_1.txt'}\n",
            "Document: is the fact that this data has to be collected to begin with. So historically, a lot of data was collected manually through forms and so on. And one part of the problem would be that these forms were manually entered by somebody and then converted to electronic forms. So that would be, there would be two levels of potential sources for errors. The person writing down the information and then the person typing in the information. Now, gradually these kind of electronic forms are spilled in directly, so at least the source of the error is reduced to one step. But still, people mistype things. I mean, there are any number of situations where people type their email address wrong and so notifications don't reach them and so on. So there is this data collection. How do you collect the data and how do you clean it? And the third thing is, how do you make it uniform? So when data is being collected by different people, they may collect different things. And for instance, if you look at the\n",
            "Relevance Score: 0.98\n",
            "\n",
            "\n",
            "Document Rank: 2, Document Index: 2\n",
            "Source Document: {'source': '/content/audio_1.txt'}\n",
            "Document: how do you clean it? And the third thing is, how do you make it uniform? So when data is being collected by different people, they may collect different things. And for instance, if you look at the government, typically the government collects data in different forms. For instance, there is a public distribution system which the ration shops, so they collect some information about who is collecting ration and who is income levels and so on. Then on the other side, you have, of course, things like electoral information. Then you have property information. Who owns property, who pays electricity bills, who owns vehicles, vehicle registration, driving licenses. Not necessarily the same thing. Not everybody who has a driving license owns a vehicle. So you have all these different sources of information. Now, is it possible really to find out same person in all these? Well, a lot depends on how the information is collected. You have a common format in which you record information about\n",
            "Relevance Score: 0.78\n",
            "\n",
            "\n",
            "Document Rank: 3, Document Index: 3\n",
            "Source Document: {'source': '/content/audio_1.txt'}\n",
            "Document: information. Now, is it possible really to find out same person in all these? Well, a lot depends on how the information is collected. You have a common format in which you record information about people. Even names in India tend to be spelt in different ways, written in different ways. Sometimes we expand initials, sometimes we suppress them, sometimes we write middle names, sometimes we don't. Sometimes we invert the order, sometimes we don't. Addresses, of course, are written in a million different ways. So there are all kinds of issues with just getting the data to a format where you can work on it. So this is an entirely different ballgame. It's much more kind of. There are lots of tools and techniques to deal with this, but that's not really going to be the focus of the course. So, in a sense, if you want to think about it, this data mining aspect will be almost missing from this course, even though it's part of the title. So what we are really going to look at is the machine\n",
            "Relevance Score: 0.59\n",
            "\n",
            "\n",
            "Document Rank: 4, Document Index: 5\n",
            "Source Document: {'source': '/content/audio_3.txt'}\n",
            "Document: So clearly there is something going on in the background, and that background in our case is revealed through a test. Even tests have errors and all that. So it's really a problem that we have to grapple with in real life. And we will just assume for now that when we reach such a situation, we just take a majority decision. But realistically, if we get too many of these situations, and it's very difficult to decide, we have to ask whether the data has actually been collected correctly. So as we said, the main issue is really how to ask the questions and what this means. Sorry, sir, I have a question. Yeah. Sir, once we know that the data is young, we are making a prediction and it come out that young. So like other attribute is not relevant other than it has job or not, and then we are not going to look at other attributes. Yeah, but remember that which attributes to look like vary from one path to another. So maybe in this case, right, so what we are saying is that if they are young,\n",
            "Relevance Score: 0.33\n",
            "\n",
            "\n",
            "Document Rank: 5, Document Index: 4\n",
            "Source Document: {'source': '/content/audio_4.txt'}\n",
            "Document: the data that you are given to build a model, and you keep a random subset of 20% to 30% away from the model building process. So it is unknown as far as the model or unknown data. It's the same as getting new data from the real world in some sense, and you can use it to validate. So, of course, this is a little bit of a disadvantage, because one of the features I mentioned right at the beginning is that typically machine learning expects that the model improves in quality with the amount of training data you give it. So you are in some sense artificially, or because you have no choice, more or less. You are building a suboptimal model, you have 100% data, but you are forcing yourself to use only 70% to 80% of the data available to build the model, because you need to validate it against some unknown data. So, one other strategy where you don't want to, I mean, one of the other disadvantages of doing this is that maybe that there are some minority anomalies in this data, and the\n",
            "Relevance Score: 0.19\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 2"
      ],
      "metadata": {
        "id": "8w72aScWdg4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_query2=[]\n",
        "for i in range(len(unique_docs_2)):\n",
        "  retrieved_docs_query2.append(unique_docs_2[i].page_content)\n",
        "\n",
        "print(query_2,\"\\n\")\n",
        "results = co.rerank(query=query_1, documents=retrieved_docs_query2, top_n=5, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
        "for idx, r in enumerate(results.results):\n",
        "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
        "  print(f\"Source Document: {unique_docs_2[r.index].metadata}\")\n",
        "  print(f\"Document: {retrieved_docs_query2[r.index]}\")\n",
        "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7aUh9u4doXI",
        "outputId": "60336737-1538-4ecb-9201-e19b61ea3f8c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " What are the advantages of the Apriori algorithm? \n",
            "\n",
            "Document Rank: 1, Document Index: 3\n",
            "Source Document: {'source': '/content/audio_2.txt'}\n",
            "Document: itself would be a huge improvement, not even necessarily a very precise quantification, but just some indication. But all these are very speculative. So that's a huge challenge. So there are some theoretical things you can say which we may not, may or may not get to in this course. But it's a big problem. This is the so called generalization problem. So that's what basically, if you build association rules, is it going to hold or not? I don't know.\n",
            "Relevance Score: 0.10\n",
            "\n",
            "\n",
            "Document Rank: 2, Document Index: 1\n",
            "Source Document: {'source': '/content/audio_3.txt'}\n",
            "Document: So we were looking at this market basket analysis, and towards the end of the discussion on association rule rules, if everybody who buys X is also likely to buy y. We said that one application of this association rule idea is to have these so called class association rules. So the right hand side was a category. We looked at an example with documents and we said, if a document has these words, it is also likely to belong to this category. So that brings us to the topic that is one of the central ones in machine learning, which is the idea of supervised learning. So in supervised learning, we have a set of items, and each item belongs to some category or class, and we distinguish these items in terms of some attributes. So you can think about this, as we will see shortly, you can think about these attributes as columns, right? So the attributes will be things like name, age, date of birth, whatever, income, stuff like that. So each row in the table will be one item, and there will be a\n",
            "Relevance Score: 0.02\n",
            "\n",
            "\n",
            "Document Rank: 3, Document Index: 2\n",
            "Source Document: {'source': '/content/audio_4.txt'}\n",
            "Document: data. So in this case it's literally the shape, because you have many possible trees you could construct, and the actual tree you construct depends on the data. And because you don't have any preconceptions about the data, you have to make sure that if there are these kind of patterns which disturb your algorithm, you will be able to counteract. So that's the purpose of this information gain ratio. It's not just the information gain alone, which is what we had initially. It's information gain divided by the entropy of the attribute. So is this clear what role this is playing? We are just saying that if you have a scattered attribute, then you discount the gain that you get from it by the scattering value. So more scattered attributes will, the net effect will be less because of the denominator. So in terms of our heuristics, what we said is that nonlinear functions work better in practice. And the genie index is in fact the one that is used in most libraries. But we should not blindly\n",
            "Relevance Score: 0.02\n",
            "\n",
            "\n",
            "Document Rank: 4, Document Index: 0\n",
            "Source Document: {'source': '/content/audio_2.txt'}\n",
            "Document: these rules corresponding to the first k minus one columns, implying the last column. And this now becomes something where these rules that you get out of this give you a kind of classifier. So this is a kind of a simplistic way of doing supervised learning. So that kind of brings me to the end of this basic treatment of classes, this market basket analysis. So what you're really looking for is correlations between sets of items across transactions, which we formalize in terms of these association groups. X implies y. And what we saw is that as such, combinatorially, this is a very complicated thing to calculate, given the sizes of the sets involved. But we can use this appropriate principle both to identify the sets which are interesting, those which are frequent enough to be considered, and also to decompose them into rules. So you use the approoriate twice, once to find the frequent z, and second to decompose each frequent z into possible x implies y. And using this, we can build\n",
            "Relevance Score: 0.01\n",
            "\n",
            "\n",
            "Document Rank: 5, Document Index: 4\n",
            "Source Document: {'source': '/content/audio_2.txt'}\n",
            "Document: I'm not going to formalize this anymore, but it's very similar in idea to the prior year. So you can formalize this algorithm for yourself, but it's essentially saying the same thing, except here the thing is constraint, because we know that z is fixed. So once z is fixed, then we just have to start with each individual element in z as a possible right hand side, and then work forwards to that. We don't have to look at all. So for each z, we do this calculation within that set. So that's how you generate association rules once you have a frequent set. So within that set, you play this a priority game with respect to the size of the right hand side, and then you generate all the rules. Is this clear? Yes. So it's exactly the same as the previous one, just a slightly different presentation of the idea. Okay, so now, as I said, this whole thing so far has been largely a kind of algorithmic problem, right? In some sense, if I fix the set of items and the set of transactions and the set of\n",
            "Relevance Score: 0.01\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 3"
      ],
      "metadata": {
        "id": "tYmoo1iAdonn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_query3=[]\n",
        "for i in range(len(unique_docs_3)):\n",
        "  retrieved_docs_query3.append(unique_docs_3[i].page_content)\n",
        "\n",
        "print(query_3,\"\\n\")\n",
        "results = co.rerank(query=query_1, documents=retrieved_docs_query3, top_n=5, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
        "for idx, r in enumerate(results.results):\n",
        "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
        "  print(f\"Source Document: {unique_docs_3[r.index].metadata}\")\n",
        "  print(f\"Document: {retrieved_docs_query3[r.index]}\")\n",
        "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJXCQEnndqg2",
        "outputId": "21fe96b1-6adf-4c79-a6d7-05270a95df9c"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What distinguishes training data in supervised learning, and what is its purpose? \n",
            "\n",
            "Document Rank: 1, Document Index: 4\n",
            "Source Document: {'source': '/content/audio_4.txt'}\n",
            "Document: the data that you are given to build a model, and you keep a random subset of 20% to 30% away from the model building process. So it is unknown as far as the model or unknown data. It's the same as getting new data from the real world in some sense, and you can use it to validate. So, of course, this is a little bit of a disadvantage, because one of the features I mentioned right at the beginning is that typically machine learning expects that the model improves in quality with the amount of training data you give it. So you are in some sense artificially, or because you have no choice, more or less. You are building a suboptimal model, you have 100% data, but you are forcing yourself to use only 70% to 80% of the data available to build the model, because you need to validate it against some unknown data. So, one other strategy where you don't want to, I mean, one of the other disadvantages of doing this is that maybe that there are some minority anomalies in this data, and the\n",
            "Relevance Score: 0.19\n",
            "\n",
            "\n",
            "Document Rank: 2, Document Index: 5\n",
            "Source Document: {'source': '/content/audio_1.txt'}\n",
            "Document: don't have any examples that people have already labeled, but you are looking for some kind of patterns. So there is no guidance. So the thing about supervised learning is somebody should have gone through and given you this information before. Now, this information could have been collected over time. Maybe it's not manually collected, like the school marks. The school marks over time. Everybody has. I mean, the school keeps a record of what happens in the model exams, what happens in the board exam. So it comes automatically. The bank will be keeping records of all the loans that it bought and what happened to the customers who bought loans. So this historical information is accumulated over time. On the other hand, in some situations, like the example I mentioned about classifying paintings, now, when you go into a museum or you go into some art gallery or something, the painting doesn't have a label on it, saying, this is a painting of this type and this is a painting of that\n",
            "Relevance Score: 0.18\n",
            "\n",
            "\n",
            "Document Rank: 3, Document Index: 2\n",
            "Source Document: {'source': '/content/audio_5.txt'}\n",
            "Document: hopefully not unfamiliar to you. Right? At the beginning, when we talked about supervised learning, we said that we are trying to predict something, and those predictions can be in two types. So one type is what we have seen in the decision tree, which is a classification. So the predicted value is one from a small range of possibilities. So we have been looking at a binary case, but even at an iris data set, there was a three way split. But we are choosing a class from a finite set. The other type of problem, which is very common, is to predict a numerical value. So we were talking in terms of predicting the marks of a student based on their board exam prelims and so on. So here is an example of that. So, supposing you want to predict the price of a house based on its square foot area. So you have, in this case, the training data will consist of the input is the house price for some given houses where you know the living area and you know the price. So these, now you would plot them.\n",
            "Relevance Score: 0.12\n",
            "\n",
            "\n",
            "Document Rank: 4, Document Index: 3\n",
            "Source Document: {'source': '/content/audio_4.txt'}\n",
            "Document: Our training data comes to us with labels. So for each input in our training data, we know the manually or however classified correct answer. So we are assuming those are the correct answers because otherwise our model building process will not make sense. So assuming that those are correct answers, we are building our model. So that's the only source of inputs and outputs where we know the answer. So the only solution really is to use that. So we have to take that training data and keep some of it aside because we are optimizing our model, as we saw in the decision tree, we are optimizing it to give correct answers on the training data. So therefore it doesn't make sense to evaluate it back on the same inputs which are used to construct the tree. But if I withhold some data, so I basically take my training data as a whole and then I kind of split this and say, do not use this part, only use this part to build my model. Then the model that I built from here, I can ask, how does it do\n",
            "Relevance Score: 0.09\n",
            "\n",
            "\n",
            "Document Rank: 5, Document Index: 0\n",
            "Source Document: {'source': '/content/audio_3.txt'}\n",
            "Document: these attributes as columns, right? So the attributes will be things like name, age, date of birth, whatever, income, stuff like that. So each row in the table will be one item, and there will be a special column which indicates what category, depending on the classification problem that we are dealing with, what category it is. And the goal of supervised learning is to now construct, given a new row, a new item, a new combination of these attributes, which is not seen before, try to predict what would be the most appropriate category to assign. So these items which are given with the labels. So these are called labels. So these items are called training data. So we have labeled training data, which consists of some information which has been collected in the past, some historical information about items and their classification. And we want to find a model that generalizes the training data. So since normally this categorization corresponds to some kind of classification, this is\n",
            "Relevance Score: 0.03\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 4"
      ],
      "metadata": {
        "id": "im3VieVhdqrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_query4=[]\n",
        "for i in range(len(unique_docs_4)):\n",
        "  retrieved_docs_query4.append(unique_docs_4[i].page_content)\n",
        "\n",
        "print(query_4,\"\\n\")\n",
        "results = co.rerank(query=query_1, documents=retrieved_docs_query4, top_n=5, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
        "for idx, r in enumerate(results.results):\n",
        "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
        "  print(f\"Source Document: {unique_docs_4[r.index].metadata}\")\n",
        "  print(f\"Document: {retrieved_docs_query4[r.index]}\")\n",
        "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnX24TGAdx1b",
        "outputId": "09a2eaa4-b451-49e0-a1b9-47faffd18784"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is cross-validation? \n",
            "\n",
            "Document Rank: 1, Document Index: 0\n",
            "Source Document: {'source': '/content/audio_4.txt'}\n",
            "Document: cross validation, cross validation. What I'm saying is that you do not make a hard and fast choice once and for all to split your data this way. So you're taking your data and you're saying, hey, first let me leave out this 20% and build one model. So I am building multiple models. I'm building m one, m two, m three, m four, and m five. Each of these models is built using 80% of the test data, but a different 80%. So each of them leaves out some 20%. So I'm building five different models and comparing what I see in all these five. So the first thing is that if this approach is good, then in general, these five models should behave in a similar way with respect to the outcome. So if they all behave similarly, then I can decide what to do. So there are two strategies. I can, I can somehow combine these by saying, build a composite model, or I can say, okay, this strategy is working for me. So this approach that I have chosen seems to be a good way to build a model for this data. So now\n",
            "Relevance Score: 0.07\n",
            "\n",
            "\n",
            "Document Rank: 2, Document Index: 2\n",
            "Source Document: {'source': '/content/audio_4.txt'}\n",
            "Document: chunk. Each model I will build using all but one of these chunks. So this then will solve this problem of many problems. It will solve the problem where you don't have enough data to begin with to afford to not look at some of it, because the interesting things may be in that sum of it. Now, what will you do with this whole thing? Well, you have five. Typically, each of these models are going to be different because we saw with the decision tree, it's kind of the questions that you ask depend on the distribution of the different roles. And if the distribution changes slightly, one attribute might get better entropy than another attribute. And you will choose that question first. So the shape of the tree might vary a lot between these, so you may not get the same. Okay, so the question about cross validation, cross validation. What I'm saying is that you do not make a hard and fast choice once and for all to split your data this way. So you're taking your data and you're saying, hey,\n",
            "Relevance Score: 0.03\n",
            "\n",
            "\n",
            "Document Rank: 3, Document Index: 4\n",
            "Source Document: {'source': '/content/audio_5.txt'}\n",
            "Document: instead of removing the widest blue, you can try to remove the narrowest green or the two narrowest greens or something and see what happens. So you just have to copy paste this code and just change the condition and all the visualization. You can just run it as it is and see, I have not done it, but I think you might get something different. So the predict outcome is not predictable. I cannot guarantee you that one or the other will produce a different output, but it can happen. And this is generally something which one has to be mindful of when you're dealing with decision. One reason that this is important is remember that we talked about this cross validation. So what do we do in cross validation or even in normal validation? In normal validation or in cross validation. In either case, you pull out some random data, set data points from your input and build a tree on the rest. Now the question is, the model that you built from the rest, is it a stable structure in the sense that\n",
            "Relevance Score: 0.02\n",
            "\n",
            "\n",
            "Document Rank: 4, Document Index: 3\n",
            "Source Document: {'source': '/content/audio_4.txt'}\n",
            "Document: So what you're really asking at some higher level is that machine learning approach. So, remember, we have seen only decision trees, but we are going to see many models. So there are clearly many ways to build models. And the reason, whenever you see that there are many ways to do something, it's only because there is no guarantee that a given way is the best one. So more or less what you want to validate is, I have, say, seven different strategies to solve this problem using machine learning. And I want to check which one of them is good. So I need to compare. So once I compare, then I'm stuck with whatever I have. It may be that this gives me only 70% accuracy or some such metric, but the other models give me even less. So it's more really a comparative statement to start with, which is, is this method actually giving me something reasonable? And is it giving me better something than something else? So, for this one strategy is so called cross validation. So what you do is you take\n",
            "Relevance Score: 0.02\n",
            "\n",
            "\n",
            "Document Rank: 5, Document Index: 1\n",
            "Source Document: {'source': '/content/audio_4.txt'}\n",
            "Document: is this method actually giving me something reasonable? And is it giving me better something than something else? So, for this one strategy is so called cross validation. So what you do is you take some fraction, say, in this case, it is one fifth. So 20%. So you take your data and you leave out. So you randomize it into five groups, which preserve, in some sense, the random behavior, so they partition it five ways. So if I leave out any one of these groups, it's as though I have taken the remaining 80% for training and this 20% for test. But because I don't want to actually think of it this way, I will actually keep doing this for each one. So I will leave out the first 20%, then I will leave out the second 20%. So I will do this. If I have like k chunks, I will do this k times in each chunk. Each model I will build using all but one of these chunks. So this then will solve this problem of many problems. It will solve the problem where you don't have enough data to begin with to\n",
            "Relevance Score: 0.01\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 5"
      ],
      "metadata": {
        "id": "N-VaHR0Udx_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_query5=[]\n",
        "for i in range(len(unique_docs_5)):\n",
        "  retrieved_docs_query5.append(unique_docs_5[i].page_content)\n",
        "\n",
        "print(query_5,\"\\n\")\n",
        "results = co.rerank(query=query_1, documents=retrieved_docs_query5, top_n=5, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
        "for idx, r in enumerate(results.results):\n",
        "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
        "  print(f\"Source Document: {unique_docs_5[r.index].metadata}\")\n",
        "  print(f\"Document: {retrieved_docs_query5[r.index]}\")\n",
        "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWbJdgHyepuF",
        "outputId": "0aa51013-ff79-457d-c172-49374a952ea5"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the process of building the decision tree classifier, and how is it trained on the dataset? \n",
            "\n",
            "Document Rank: 1, Document Index: 2\n",
            "Source Document: {'source': '/content/audio_5.txt'}\n",
            "Document: predictions which were learned. So that is the training data, and it produced a model which we visualized there. Now, when I want to actually run this model, what I have to do is I have to give it an input and ask it what do you think is the output? So things, effectively, the output will be ambiguous because there will always be some degree of mixing between the classes. So what it will do is it'll first compute according to what. In this decision tree, it will basically follow the input to the leaf. And in that leaf it will look at the distribution of nodes and it will assign a number to each one so that they are normalized to add up to one. And after I normalize and add up to one, I take the highest probability one as the answer. So if you now insist that this tree should tell you one of the three classes is obviously going to predict the class with the highest probability. So it gives me one. The reason it gives me one is that the three classes are encoded as zero, one, two. So\n",
            "Relevance Score: 0.07\n",
            "\n",
            "\n",
            "Document Rank: 2, Document Index: 4\n",
            "Source Document: {'source': '/content/audio_4.txt'}\n",
            "Document: So last time we started with supervised learning in earnest, and we were looking at this decision tree model. So, in a decision tree model, you have essentially a table in which the columns are the. So each item, a row in the table and one special column denotes the class that you are trying to classify. And we assumed it's a binary classification. So the class is zero, one, or yes, no, or whatever you want to interpret. The two categories are. So the algorithm was very simple. At a high level, what you want to do is choose an attribute based on the answer to the question that you're asking about that attribute. The data splits, and then you take each partition, ask another question. So that's the thing. And you want to come down and keep asking questions until either you have reached, like these situations here, a partition in which all the values of the class variable are the same. So you have a uniform class level or you have run out of questions, because we can only ask in any path\n",
            "Relevance Score: 0.04\n",
            "\n",
            "\n",
            "Document Rank: 3, Document Index: 1\n",
            "Source Document: {'source': '/content/audio_5.txt'}\n",
            "Document: of irises that we saw, so it will be essentially a number, zero, one. And now to get this decision tree classifier, we have to take this function that we have imported. So we say take a decision tree classifier. We have to provide it with, again some random state because it uses some randomization inside it. And now this is something which we have not talked about. So we discussed that we will build a decision tree normally until we either achieve some pure leaf nodes where the classification is uniform, or we run out of attributes. Now this is a numerical case, so we are not going to run out of attributes because we are going to run out eventually when we run out of data points. But we will end up with these very long slicing, the width between 1.5 and 1.71.5 and 1.6 and so on. So what you can do is you can tell the decision C classifier not to expand beyond a certain depth. And here it has fixed it to be depth too. So we will come back to this at a later lecture, which is, this is\n",
            "Relevance Score: 0.02\n",
            "\n",
            "\n",
            "Document Rank: 4, Document Index: 3\n",
            "Source Document: {'source': '/content/audio_5.txt'}\n",
            "Document: is related to that also where to save. So this is some setup which is not relevant to decision. This is more to do with organizing how and visualize things and save those figures for later. Okay, so now this is the real part. So, so Scikitlearn has some built in data sets and one of them is this iris library. So the way to get the iris library into your data set is to import this function called load iris. And Scikitlearn has a bunch of models predefined. So what we have been looking at is decision trees as classifiers. So we are going to import from the subset of models of form tree. We are going to import this model called decision tree classifier. So the first thing that we do is we actually load this data set. So we call this load iris function, and then we are going to remember, the iris data set had two measures. It had the petal and the sepal, length and width. So it had four quantities. But we decided we will only use two quantities. So this is what this is doing. It's taking\n",
            "Relevance Score: 0.01\n",
            "\n",
            "\n",
            "Document Rank: 5, Document Index: 0\n",
            "Source Document: {'source': '/content/audio_5.txt'}\n",
            "Document: better. So here, priority, they are saying, let us not construct deep trees. So whatever tree we construct, we are going to stop it when it reaches. So it's a two step process, right? So what you do is you first say what are the parameters for the decision tree classifier? So he says, I want a decision tree classifier to be set up with this random state 42 and which will not grow to more than depth two. And then I have to actually construct the classifier for a particular data set. So that's the next thing. So I use this fit function, right? In some sense this creates a decision tree object with certain operating parameters, and then you pass it the training data in the form of the input and the output x and y, and say fit. So the important thing to see is that this is all you need to do to build a decision tree in python. If you have the data, what you need to do manually is to probably decide. So the question is, can we specify a certain threshold? A certain threshold of what? Yes\n",
            "Relevance Score: 0.01\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 6"
      ],
      "metadata": {
        "id": "AR7v8XpOdx_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_query6=[]\n",
        "for i in range(len(unique_docs_6)):\n",
        "  retrieved_docs_query6.append(unique_docs_6[i].page_content)\n",
        "\n",
        "print(query_6,\"\\n\")\n",
        "results = co.rerank(query=query_1, documents=retrieved_docs_query6, top_n=5, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
        "for idx, r in enumerate(results.results):\n",
        "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
        "  print(f\"Source Document: {unique_docs_6[r.index].metadata}\")\n",
        "  print(f\"Document: {retrieved_docs_query6[r.index]}\")\n",
        "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrBsgt6pe3wz",
        "outputId": "dfcb8fff-01e4-44d0-9116-171ae4a02cf6"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " What is the challenge associated with supervised learning?  \n",
            "\n",
            "Document Rank: 1, Document Index: 4\n",
            "Source Document: {'source': '/content/audio_1.txt'}\n",
            "Document: and we see an attractive cake, what we're attracted to is the surface, the icing surface. But if the inside is made of sodas, then the cake will not taste good. So you have to be careful not to be fooled by the icing. So the unsupervised learning is also important. But supervised learning is where all the, in some sense, the jazz is. That's what people spend up spending a lot more time on. And we will also probably spend, in terms of lecture hours, more time on supervised learning than on unsupervised learning. But don't forget that the bread and butter work of machine learning actually rests on a strong foundation of unsupervised learning. Okay, so I'm going to start with something after this. So if there are any questions at this point, I'll just take a minute to pause and ask, then continue. Okay, you. Fine. So now let's start our first topic. So this is a topic which is actually of historical interest because it is how this whole idea about learning from data and data driven\n",
            "Relevance Score: 0.26\n",
            "\n",
            "\n",
            "Document Rank: 2, Document Index: 1\n",
            "Source Document: {'source': '/content/audio_1.txt'}\n",
            "Document: don't have any examples that people have already labeled, but you are looking for some kind of patterns. So there is no guidance. So the thing about supervised learning is somebody should have gone through and given you this information before. Now, this information could have been collected over time. Maybe it's not manually collected, like the school marks. The school marks over time. Everybody has. I mean, the school keeps a record of what happens in the model exams, what happens in the board exam. So it comes automatically. The bank will be keeping records of all the loans that it bought and what happened to the customers who bought loans. So this historical information is accumulated over time. On the other hand, in some situations, like the example I mentioned about classifying paintings, now, when you go into a museum or you go into some art gallery or something, the painting doesn't have a label on it, saying, this is a painting of this type and this is a painting of that\n",
            "Relevance Score: 0.18\n",
            "\n",
            "\n",
            "Document Rank: 3, Document Index: 2\n",
            "Source Document: {'source': '/content/audio_3.txt'}\n",
            "Document: validate at the end somehow that what we are doing is right. But in practice what you're saying makes sense that really to do supervised learning, it's much more realistic to do it in collaboration with somebody who understands a domain rather than just doing it blindly. Because also when you do it blindly, you might come up with solutions which make no sense. You might actually identify features which contribute to a decision purely by statistical coincidence, and no human would actually do that. So there's another issue which comes up of interpretability or explainability of your answers. So there are various factors which get into this. It's only not only accuracy which matters, but also whether your model is sensible in something, so that you can justify it. So these will all come as we go along. So let's proceed with this. So the first thing is that because the choice of attribute to question will determine the structure of the tree. So here are two different trees. So this is\n",
            "Relevance Score: 0.16\n",
            "\n",
            "\n",
            "Document Rank: 4, Document Index: 0\n",
            "Source Document: {'source': '/content/audio_1.txt'}\n",
            "Document: I can start with something which actually, in the original thing, is separated by a simple plane, right? So I have this plane here which separates the green from the yellow. But then, believing that it'll be better to work in two dimensions, I actually unroll it. And then I find I have to deal with these multiple bands. So now it's not so obvious anymore because I do have lines, but I have many of them, and I have to decide between which bands it is, green, and which bands it is, this yellow color. So the peril of supervised unsupervised learning is that you don't know that the step that you are taking in advance, you don't know is going to be good or bad because it's unsupervised. You're just taking a guess in some sense, but then you have to have some way of evaluating whether the guess was good or not. And then going back and doing it again may be a different. So this is kind of a quick overview, a summary of the main things that we will be looking at. So, supervised learning and\n",
            "Relevance Score: 0.14\n",
            "\n",
            "\n",
            "Document Rank: 5, Document Index: 3\n",
            "Source Document: {'source': '/content/audio_1.txt'}\n",
            "Document: So supervised learning is by far the more attractive and the more, should I say, attractive, I guess it's the more attractive aspect of machine learning. This is what people are really looking for. But unfortunately, as I said, to do effective supervised learning, you need to actually get a lot of training data. And this is sometimes expensive. Very often it is expensive. So unsupervised learning is often the starting. A few. Couple of years back, there were three turing awards given for deep learning. So one of them was given to Jan Lakun. And his observation is that if you think of this whole thing as a cake, then actually most of the cake is actually unsupervised learning. So supervised learning is just a thin layer on the top, but it's like the icing on the cake. So when we go to a bakery and we see an attractive cake, what we're attracted to is the surface, the icing surface. But if the inside is made of sodas, then the cake will not taste good. So you have to be careful not to\n",
            "Relevance Score: 0.05\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 7"
      ],
      "metadata": {
        "id": "7Mvansgmdx_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_query7=[]\n",
        "for i in range(len(unique_docs_7)):\n",
        "  retrieved_docs_query7.append(unique_docs_7[i].page_content)\n",
        "\n",
        "print(query_7,\"\\n\")\n",
        "results = co.rerank(query=query_1, documents=retrieved_docs_query7, top_n=5, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
        "for idx, r in enumerate(results.results):\n",
        "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
        "  print(f\"Source Document: {unique_docs_7[r.index].metadata}\")\n",
        "  print(f\"Document: {retrieved_docs_query7[r.index]}\")\n",
        "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xlkoHx-fCbR",
        "outputId": "20d674d4-0a0d-415f-ae4a-9add6b66c96e"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What considerations should businesses keep in mind when applying market basket analysis to understand customer behavior? \n",
            "\n",
            "Document Rank: 1, Document Index: 4\n",
            "Source Document: {'source': '/content/audio_2.txt'}\n",
            "Document: I talked about this unsupervised. So maybe one thing that you need to do when you have something large is to basically look at the transactions and categorize them according to some criteria, maybe by the size of the basket. And then you will find that there are maybe a lot of people who buy five items at a time. There are people who buy ten to 20 items at a time. There are people who buy 40 items at a time. And maybe you need to then apply the model separately to each of these clusters. So all of these things are not a one size fits all solution. And also, it's not that one approach on its own is going to solve the problem at any given time. So you need to have a kind of think of it as a toolbox, and then you need to apply some combination of tools for a given problem based on what you can do about analyzing the problem. So what we are looking at is the question of what these tools are. But they will not work out of the box in some sense, in every situation, like you said. So that's\n",
            "Relevance Score: 0.29\n",
            "\n",
            "\n",
            "Document Rank: 2, Document Index: 2\n",
            "Source Document: {'source': '/content/audio_1.txt'}\n",
            "Document: Maybe it is true that at certain times, people who buy diapers also buy beer, but it at least gave some confidence to the idea that you can use this kind of information to make decisions. So from that perspective, it possibly had a role to play. But anyway, so these are the origins of this problem, and it's called market basket analysis because it's really talking in this context about shopping baskets. I have a shopping basket when I reach the counter to pay. So question is, what is in that basket? If it has x, does it also have y? So this is a very literal interpretation of markets and baskets. But the notion of markets and baskets can be a little bit more abstract. So, for instance, if you are thinking about, especially nowadays, given the number of elearning platforms, there's a lot of emphasis on trying to structure topics in such a way that students who find difficulty, you can identify why they are facing difficulties. So you want to think of these concepts that you're teaching\n",
            "Relevance Score: 0.10\n",
            "\n",
            "\n",
            "Document Rank: 3, Document Index: 1\n",
            "Source Document: {'source': '/content/audio_2.txt'}\n",
            "Document: or there are a bunch of transactions which have come together, and then you want to look at each of these groups as a whole and do some kind of. So not just one consumer, but across consumers, you want to say, as a rule, consumers who do this also do that and so on. So these are all refinements. And there are many situations where these are important and interesting, but this model as such, this kind of market basket analysis approach is very naive. And so that's why these things will come up in different contexts. But we are not going to explore it anymore in this context, because in this context, the kind of model that you build, this association rule model is rather simplistic. So we are going to look at more sophisticated models as we go along. And there you can ask the same question, but it'll take a slightly different format. Does this also include tendencies like, if there is one person who tends to buy all his groceries once a month, then they will have a bigger basket size.\n",
            "Relevance Score: 0.05\n",
            "\n",
            "\n",
            "Document Rank: 4, Document Index: 0\n",
            "Source Document: {'source': '/content/audio_2.txt'}\n",
            "Document: maybe you cannot do. So that's another thing about this whole model building thing is that the same model may or may not be capable of tackling every different question that you ask. So this model, some of these things maybe you can segregate and answer. But if, as you're saying, you want to compare behaviors across customers of different types, you cannot, as you said, sensibly aggregate them into a single market basket model because they all have different profiles. So you will have to, in the example that you gave, you would have to first separate out the data for these, say, the people who buy one month groceries at a time from those who buy at more frequent intervals and then analyze them separately, because together the data doesn't actually make sense. So last time, if you remember, I talked about this unsupervised. So maybe one thing that you need to do when you have something large is to basically look at the transactions and categorize them according to some criteria, maybe\n",
            "Relevance Score: 0.02\n",
            "\n",
            "\n",
            "Document Rank: 5, Document Index: 3\n",
            "Source Document: {'source': '/content/audio_2.txt'}\n",
            "Document: these rules corresponding to the first k minus one columns, implying the last column. And this now becomes something where these rules that you get out of this give you a kind of classifier. So this is a kind of a simplistic way of doing supervised learning. So that kind of brings me to the end of this basic treatment of classes, this market basket analysis. So what you're really looking for is correlations between sets of items across transactions, which we formalize in terms of these association groups. X implies y. And what we saw is that as such, combinatorially, this is a very complicated thing to calculate, given the sizes of the sets involved. But we can use this appropriate principle both to identify the sets which are interesting, those which are frequent enough to be considered, and also to decompose them into rules. So you use the approoriate twice, once to find the frequent z, and second to decompose each frequent z into possible x implies y. And using this, we can build\n",
            "Relevance Score: 0.01\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 8"
      ],
      "metadata": {
        "id": "BAr86bXCdx_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_query8=[]\n",
        "for i in range(len(unique_docs_8)):\n",
        "  retrieved_docs_query8.append(unique_docs_8[i].page_content)\n",
        "\n",
        "print(query_8,\"\\n\")\n",
        "results = co.rerank(query=query_1, documents=retrieved_docs_query8, top_n=5, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
        "for idx, r in enumerate(results.results):\n",
        "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
        "  print(f\"Source Document: {unique_docs_8[r.index].metadata}\")\n",
        "  print(f\"Document: {retrieved_docs_query8[r.index]}\")\n",
        "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wR56rCzGfMAi",
        "outputId": "0e0e3a38-f0eb-44f4-9b1d-1c1e16907ecf"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How does the genie index contribute to decision tree construction, and why is it favored in practice, particularly in Python libraries? \n",
            "\n",
            "Document Rank: 1, Document Index: 2\n",
            "Source Document: {'source': '/content/audio_3.txt'}\n",
            "Document: right now. So what we are saying now is that we want to choose the next question to evaluate the next question to ask. So for the next question to ask, we have to say something about the quality of the impurity before and after. So if I just look at the raw ratio of the impurity, I have a motion of just that impurity percentage. But that gives me this linear relationship, which apparently is not that good in practice. So I want a nonlinear way of giving some impurity value to this ratio of yes or no, and then we can use either this entropy idea or this genie index, and it gives me a sharper curve, and this sharper practice turns out to be better. So that's how you build a decision tree. You build a decision tree by applying one of these two nonlinear things. If you come up to the third thing, it's also fine. It's just that these two happen to be standard things. They are both borrowed from other areas where they have a long and established history. So, as I said, genie index comes\n",
            "Relevance Score: 0.04\n",
            "\n",
            "\n",
            "Document Rank: 2, Document Index: 1\n",
            "Source Document: {'source': '/content/audio_4.txt'}\n",
            "Document: tree for it. So this tree, which we will see later, this python library for decision trees, actually constructs from the iris data set, this decision tree. So what you do is you start with these 150 samples, and initially there are three categories, so there are 50 in each category. And remember that if there are at any point when you have an impure sample, you have to decide on something. So this particular algorithm, they're equal. It happens to just choose one of them more or less at random as the category to predict nation. So this last line is the prediction, this is the distribution of values. This is how many samples there are. And this is the actual genie index calculation. If you think about it, it is the three samples are each with one third. So you have one third squared plus one third, one third squared, one nine. So one minus one 9th plus one nine plus one nine, which is one minus one thirds. So that's why you get this in the index of zero, 67. So this is the initial.\n",
            "Relevance Score: 0.03\n",
            "\n",
            "\n",
            "Document Rank: 3, Document Index: 4\n",
            "Source Document: {'source': '/content/audio_5.txt'}\n",
            "Document: about this. Okay, so now one of the problems with decision trees, which you will see later on, is also that a lot of the construction of the tree depends on these statistics. Because we said that we look at which attribute has the maximum discrepancy, whether it's entropy or genie index or whatever, which one of them will improve it the most. And that improvement, or lack of improvement really depends on the distribution of the different inputs at that point in my table. So if there is a borderline case, it could be that by shifting one value here or there, the attribute which has the highest improvement might shift from one to the other. And then I ask a different question. The moment I ask a different question, the tree becomes radically different because the question pattern changes and the split will change and so on. So here is to illustrate that what they have done is they have first, so y equal to one. Remember that the categories are zero, one and two. So they are picked out\n",
            "Relevance Score: 0.03\n",
            "\n",
            "\n",
            "Document Rank: 4, Document Index: 0\n",
            "Source Document: {'source': '/content/audio_3.txt'}\n",
            "Document: our logic earlier was that somehow nonlinear is better empirically, whatever that means. So empirically just means that somebody built these systems and found that if they use these kind of things, it works better. So empirically, entropy is, I mean, theoretically, entropy is slightly steeper, but entropy requires us to compute logs and all that, whereas genie index is just this computing squares of numbers. So genie index is actually easier to compute. And if you look at, as we will see, libraries in Python and all that, typically they use genie index. So although there is a very famous system built using entropy, which was one of the early decision tree systems, the ones that we will practically encounter in Python, libraries actually use the genie index. Basically. I'll stop with this right now. So what we are saying now is that we want to choose the next question to evaluate the next question to ask. So for the next question to ask, we have to say something about the quality of\n",
            "Relevance Score: 0.03\n",
            "\n",
            "\n",
            "Document Rank: 5, Document Index: 3\n",
            "Source Document: {'source': '/content/audio_4.txt'}\n",
            "Document: data. So in this case it's literally the shape, because you have many possible trees you could construct, and the actual tree you construct depends on the data. And because you don't have any preconceptions about the data, you have to make sure that if there are these kind of patterns which disturb your algorithm, you will be able to counteract. So that's the purpose of this information gain ratio. It's not just the information gain alone, which is what we had initially. It's information gain divided by the entropy of the attribute. So is this clear what role this is playing? We are just saying that if you have a scattered attribute, then you discount the gain that you get from it by the scattering value. So more scattered attributes will, the net effect will be less because of the denominator. So in terms of our heuristics, what we said is that nonlinear functions work better in practice. And the genie index is in fact the one that is used in most libraries. But we should not blindly\n",
            "Relevance Score: 0.02\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 9"
      ],
      "metadata": {
        "id": "eFciKNb6dx_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_query9=[]\n",
        "for i in range(len(unique_docs_9)):\n",
        "  retrieved_docs_query9.append(unique_docs_9[i].page_content)\n",
        "\n",
        "print(query_9,\"\\n\")\n",
        "results = co.rerank(query=query_1, documents=retrieved_docs_query9, top_n=5, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
        "for idx, r in enumerate(results.results):\n",
        "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
        "  print(f\"Source Document: {unique_docs_9[r.index].metadata}\")\n",
        "  print(f\"Document: {retrieved_docs_query9[r.index]}\")\n",
        "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGAg5QL1fUGh",
        "outputId": "b004d5cc-a992-4e99-db5d-0e904c990dbb"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the harmonic mean, and how does it enable the combination of precision and recall into a single metric? \n",
            "\n",
            "Document Rank: 1, Document Index: 3\n",
            "Source Document: {'source': '/content/audio_4.txt'}\n",
            "Document: pancreatic cancer, and blurt it out to the patient. You'll make them both. So in many situations, it's not clear, but what is generally clear is that these two things are usually not aligned. You cannot normally do both together. You cannot have normally. If you have to trade off either you want recall or you want precision. And one will go up.\n",
            "Relevance Score: 0.36\n",
            "\n",
            "\n",
            "Document Rank: 2, Document Index: 1\n",
            "Source Document: {'source': '/content/audio_4.txt'}\n",
            "Document: If you want the other one go up, the first one will come down. So now I just mentioned that these are not the only things so people also talk about. So there is, for instance, the corresponding thing to precision for the negative part is specificity. Right? So what is the ratio of correct negative results compared to all negative results as a true negative? So that's with respect to this column, then accuracy, of course, is just saying how many of the correct things are there with respect to the whole thing. So that also can be measured easily from this. And then there's something called thread score and so on. So I'm not going to talk about it. And just for completeness, I will mention that sometimes people want to say that we want a single number. It's not good to give me a precision separately and a recall separately and let me choose, I want a single number. There is this thing called harmonic mean. So the harmonic mean is one upon p plus one upon R. So remember, p is precision\n",
            "Relevance Score: 0.03\n",
            "\n",
            "\n",
            "Document Rank: 3, Document Index: 0\n",
            "Source Document: {'source': '/content/audio_4.txt'}\n",
            "Document: separately and a recall separately and let me choose, I want a single number. There is this thing called harmonic mean. So the harmonic mean is one upon p plus one upon R. So remember, p is precision one and it's one upon that. So it's the reciprocal of the mean of the reciprocals. So if you work this out, you get two PR on two p plus. So this is sometimes called the f score. Right? So this is a way of combining the precision score and the recall score into a single number. But as I said, in most cases, it really depends on the application. So either you want something based on the context which has higher precision or higher recall, and you choose. So these are some of the ways in which we can actually. So, remember, our starting point was that we wanted to evaluate classifiers. So we said first we have a strategy for evaluating classifiers, so we can test them by using this test set and so on. But what is it that you want to measure on the test set? So that we have this wide variety\n",
            "Relevance Score: 0.02\n",
            "\n",
            "\n",
            "Document Rank: 4, Document Index: 4\n",
            "Source Document: {'source': '/content/audio_4.txt'}\n",
            "Document: data. So in this case it's literally the shape, because you have many possible trees you could construct, and the actual tree you construct depends on the data. And because you don't have any preconceptions about the data, you have to make sure that if there are these kind of patterns which disturb your algorithm, you will be able to counteract. So that's the purpose of this information gain ratio. It's not just the information gain alone, which is what we had initially. It's information gain divided by the entropy of the attribute. So is this clear what role this is playing? We are just saying that if you have a scattered attribute, then you discount the gain that you get from it by the scattering value. So more scattered attributes will, the net effect will be less because of the denominator. So in terms of our heuristics, what we said is that nonlinear functions work better in practice. And the genie index is in fact the one that is used in most libraries. But we should not blindly\n",
            "Relevance Score: 0.02\n",
            "\n",
            "\n",
            "Document Rank: 5, Document Index: 2\n",
            "Source Document: {'source': '/content/audio_4.txt'}\n",
            "Document: first we have a strategy for evaluating classifiers, so we can test them by using this test set and so on. But what is it that you want to measure on the test set? So that we have this wide variety of things, but most commonly this precision and recall are the ones that people are interested in, and they will choose which one to emphasize. Depending on the context. Depending on the context, you may want precision predominant. You may want recall to be predominant. So there, it really becomes a domain specific. So I'll stop there. Any questions, sir? Like, given a precision or recall value, what we need to change in the algorithm so that we could come to the certain value of precision and recall. So that is a harder question. Right. So what is it that you need to change? So that's a very difficult question. All you can say is that given what we are doing now, this is what, then it becomes really an experimental issue. So this is the hardest question in machine learning, which is, how\n",
            "Relevance Score: 0.01\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 10"
      ],
      "metadata": {
        "id": "JbY8AhIkdx_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_query10=[]\n",
        "for i in range(len(unique_docs_10)):\n",
        "  retrieved_docs_query10.append(unique_docs_10[i].page_content)\n",
        "\n",
        "print(query_10,\"\\n\")\n",
        "results = co.rerank(query=query_1, documents=retrieved_docs_query10, top_n=5, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
        "for idx, r in enumerate(results.results):\n",
        "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
        "  print(f\"Source Document: {unique_docs_10[r.index].metadata}\")\n",
        "  print(f\"Document: {retrieved_docs_query10[r.index]}\")\n",
        "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUrauYnNfelt",
        "outputId": "be0e60c1-086b-41c2-dde5-dd7c37e17d8f"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What are the implications of choosing a large or small learning rate? \n",
            "\n",
            "Document Rank: 1, Document Index: 6\n",
            "Source Document: {'source': '/content/audio_4.txt'}\n",
            "Document: the data that you are given to build a model, and you keep a random subset of 20% to 30% away from the model building process. So it is unknown as far as the model or unknown data. It's the same as getting new data from the real world in some sense, and you can use it to validate. So, of course, this is a little bit of a disadvantage, because one of the features I mentioned right at the beginning is that typically machine learning expects that the model improves in quality with the amount of training data you give it. So you are in some sense artificially, or because you have no choice, more or less. You are building a suboptimal model, you have 100% data, but you are forcing yourself to use only 70% to 80% of the data available to build the model, because you need to validate it against some unknown data. So, one other strategy where you don't want to, I mean, one of the other disadvantages of doing this is that maybe that there are some minority anomalies in this data, and the\n",
            "Relevance Score: 0.19\n",
            "\n",
            "\n",
            "Document Rank: 2, Document Index: 7\n",
            "Source Document: {'source': '/content/audio_5.txt'}\n",
            "Document: that one input that you get might be one, which is kind of very extreme, and so it might give you a wild swing, and then you might oscillate a lot more. So it might take you longer to converge, but faster to compute. So there's a trade off between doing each iteration. You are making a lot of adjustments fast, but those adjustments are less controlled. The other way in between is take small batches. If you have 1 million things, you break it up into 1000 batches. So you do 1000 points. So you get some averaging out of the behavior of the function, make an adjustment, then you do another 1000, make an adjustment and so on. So all these things are actually used, we will see later in neural networks and all that. But in this linear prediction also, that's where they arise. I mean, that's the origin of this. So this is, of course, as you have all probably read. So this strategy is conventionally called linear regression, but it's basically this iterative linear prediction, which you can\n",
            "Relevance Score: 0.18\n",
            "\n",
            "\n",
            "Document Rank: 3, Document Index: 3\n",
            "Source Document: {'source': '/content/audio_4.txt'}\n",
            "Document: these situations here, a partition in which all the values of the class variable are the same. So you have a uniform class level or you have run out of questions, because we can only ask in any path one question per attribute. And if in case we have the second case where we run out of questions, but we have not reached a so called uniform node, then we just use the majority as a prediction. So we will see an example of this later as we go. Yeah, so what we also said was that, as we can see here, we have two different trees, and the trees are not the same shape. And we argued that a smaller tree is probably a more desirable one. It's more explainable. And we also claim, without at the moment justifying, that it also has a better generalization property. That is, it will move from the specific training data to the unknown data in a better way. So how do we build small trees? Well, computationally, we said, I claimed at least that it was infeasible to build the smallest tree. So you\n",
            "Relevance Score: 0.13\n",
            "\n",
            "\n",
            "Document Rank: 4, Document Index: 1\n",
            "Source Document: {'source': '/content/audio_3.txt'}\n",
            "Document: saw in this loan data set, 60% of the loans were granted, so nine by 15. So if I were to just say yes, supposing I don't have an interesting model and I just say yes all the time, every answer that every person who comes, I just say yes, give this person. And if the previous assumption is actually valid, that the distribution of training examples and the distribution of unseen data are actually the same, then if I used to give 60% of the training people loans, then giving 60% of the unseen data loans is something that I would expect to do. So if I just keep on saying yes, then 60% of the time I would be correct, which might suggest that you're doing better than half, but actually the answer is biased, so you're not doing any better than random guessing. It's like if I toss a fair coin and I keep saying heads all the time, then I should expect to call it right 50% of the time. So doing 50% of the time in a fair situation is no better than random guessing in this kind of situation,\n",
            "Relevance Score: 0.03\n",
            "\n",
            "\n",
            "Document Rank: 5, Document Index: 2\n",
            "Source Document: {'source': '/content/audio_4.txt'}\n",
            "Document: I was trying to make is that as a gets more scattered, there are more values and the ratio of each value is smaller, then that denominator will become larger. So whatever gain you get on the top. So remember the problem was that these give us high gain on the top, but we are dividing that gain by the scattering of that attribute. So if we have high gain divided by high scattering, then the net normalized effect is lower. So you might choose an attribute which actually gives you a lower information gain overall, because the attribute itself is less static. And that's the thing that we want to achieve. Just to emphasize that when you have a machine learning algorithm, see generally, what is a machine learning algorithm doing? It's trying to describe the shape of the model based on the training data. So in this case it's literally the shape, because you have many possible trees you could construct, and the actual tree you construct depends on the data. And because you don't have any\n",
            "Relevance Score: 0.03\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}