{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install chromadb"
      ],
      "metadata": {
        "id": "MsuXFNMk1kD7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5561d12a-feed-4d2d-fb9e-0b712fe8c6f0"
      },
      "id": "MsuXFNMk1kD7",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb\n",
            "  Downloading chromadb-0.4.24-py3-none-any.whl (525 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.1)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.6.4)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.110.1-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.25.2)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.10.0)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.17.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.24.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.24.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.45b0-py3-none-any.whl (11 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.24.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.2)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.2)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.0)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.62.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (698 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.4)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.2.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson>=3.9.12 (from chromadb)\n",
            "  Downloading orjson-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (24.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting importlib-metadata<=7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading importlib_metadata-7.0.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.24.0-py3-none-any.whl (17 kB)\n",
            "Collecting opentelemetry-proto==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.24.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-instrumentation-asgi==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.45b0-py3-none-any.whl (14 kB)\n",
            "Collecting opentelemetry-instrumentation==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.45b0-py3-none-any.whl (28 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl (36 kB)\n",
            "Collecting opentelemetry-util-http==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.45b0-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.7.2)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.6)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.20.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.18.1)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.38.0,>=0.37.2->fastapi>=0.95.2->chromadb) (3.7.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi>=0.95.2->chromadb) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi>=0.95.2->chromadb) (1.2.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53724 sha256=b384ca9e5508d23dbf0dcfbff2f7f5278db7e2d2df22ebc6c4d34f322b0dd060\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, mmh3, websockets, uvloop, python-dotenv, pulsar-client, overrides, orjson, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, importlib-metadata, humanfriendly, httptools, h11, deprecated, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, uvicorn, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, kubernetes, fastapi, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 7.1.0\n",
            "    Uninstalling importlib_metadata-7.1.0:\n",
            "      Successfully uninstalled importlib_metadata-7.1.0\n",
            "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.1.2 chroma-hnswlib-0.7.3 chromadb-0.4.24 coloredlogs-15.0.1 deprecated-1.2.14 fastapi-0.110.1 h11-0.14.0 httptools-0.6.1 humanfriendly-10.0 importlib-metadata-7.0.0 kubernetes-29.0.0 mmh3-4.1.0 monotonic-1.6 onnxruntime-1.17.1 opentelemetry-api-1.24.0 opentelemetry-exporter-otlp-proto-common-1.24.0 opentelemetry-exporter-otlp-proto-grpc-1.24.0 opentelemetry-instrumentation-0.45b0 opentelemetry-instrumentation-asgi-0.45b0 opentelemetry-instrumentation-fastapi-0.45b0 opentelemetry-proto-1.24.0 opentelemetry-sdk-1.24.0 opentelemetry-semantic-conventions-0.45b0 opentelemetry-util-http-0.45b0 orjson-3.10.0 overrides-7.7.0 posthog-3.5.0 pulsar-client-3.4.0 pypika-0.48.9 python-dotenv-1.0.1 starlette-0.37.2 uvicorn-0.29.0 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain"
      ],
      "metadata": {
        "id": "vOb3AgPt2A0S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b15e912-1abf-4d70-fdba-73363a594273"
      },
      "id": "vOb3AgPt2A0S",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.14-py3-none-any.whl (812 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.8/812.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.30 (from langchain)\n",
            "  Downloading langchain_community-0.0.31-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.37 (from langchain)\n",
            "  Downloading langchain_core-0.1.40-py3-none-any.whl (276 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.8/276.8 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.40-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.37->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: packaging, mypy-extensions, jsonpointer, typing-inspect, marshmallow, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed dataclasses-json-0.6.4 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.14 langchain-community-0.0.31 langchain-core-0.1.40 langchain-text-splitters-0.0.1 langsmith-0.1.40 marshmallow-3.21.1 mypy-extensions-1.0.0 packaging-23.2 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence_transformers"
      ],
      "metadata": {
        "id": "OUivyC6T2PIN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "670590f3-f905-4803-a50b-44564e323d55"
      },
      "id": "OUivyC6T2PIN",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence_transformers\n",
            "  Downloading sentence_transformers-2.6.1-py3-none-any.whl (163 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/163.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m153.6/163.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.3/163.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.38.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m590.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.4.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence_transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 sentence_transformers-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install patool"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRYUQQY6rqtI",
        "outputId": "80e750d0-6901-4581-9239-9e6c3cd56522"
      },
      "id": "QRYUQQY6rqtI",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting patool\n",
            "  Downloading patool-2.2.0-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/96.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m92.2/96.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.0/96.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: patool\n",
            "Successfully installed patool-2.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ef663270-9c8f-4d22-afe9-c6ebd38dc027",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-23T08:19:18.576133Z",
          "start_time": "2023-10-23T08:19:18.572018Z"
        },
        "tags": [],
        "id": "ef663270-9c8f-4d22-afe9-c6ebd38dc027"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import chromadb\n",
        "from langchain.retrievers.merger_retriever import MergerRetriever\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.document_transformers import (\n",
        "    EmbeddingsRedundantFilter,\n",
        "    EmbeddingsClusteringFilter,\n",
        ")\n",
        "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19c1c81e",
      "metadata": {
        "id": "19c1c81e"
      },
      "source": [
        "## Get the Embedding Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "e7b2b839",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-23T08:16:33.768026Z",
          "start_time": "2023-10-23T08:15:58.421696Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490,
          "referenced_widgets": [
            "9b3b86e2e178403dbea480033f0c3170",
            "3b4aff0d02ce4fba9abe27dc24eaccb4",
            "64167dffcaaf4a27b90f28d4d7d2deab",
            "5392b5f56bee461b80113e4e8b6d55ba",
            "33478a3d277841f39e0a2ae43f9f01de",
            "b977658d6d044f5dba826005f1703683",
            "d45b716f201f4ab29e20097ee15db082",
            "909cc46b5c22478dbc8550906b1deb97",
            "8f595f940820473387490fcded5db6a1",
            "ca62535d6be64037a31aba858cc8eb3d",
            "2c1fc2a4951643d5836f313453a3035c",
            "302350358d7e45fab74d808c8e2c7c01",
            "a392fb83a608434386f8137516a9af5a",
            "d2a766064a854c51849d33abb21b3681",
            "10907a0927c34ec49552c5ecbc0ee188",
            "bc9522147c454b019bba31e7466722c0",
            "e2889aa978c243849ae5481ffa6abd69",
            "1d80e0b61db640e283e745303a78c8d2",
            "2543684c940c4bf780e5f57911c4f8e5",
            "eb2c6f996d0344889f0ed13061ec108e",
            "149d8c9b65094cc3b9cd27a9fb1538b7",
            "ac48af462a56446ba379c6bc6f34df0c",
            "e4344f55939f4de0bed4a55c46e487c8",
            "6c5f4cbb4bb1499dbfda05d60a03a462",
            "83632d9a01044bcd954795d5e3612ee2",
            "16bae1026ff14d738731f4769f1fe9fb",
            "fa94f3425b604f62b551c42fe1cfc198",
            "6012ffbc02aa4bcdb586f89dfed9b703",
            "152d90fb8ec84939b8aa18287116671c",
            "f3c59c09f7b14c69b638f9faf42b9971",
            "b6887eae1851411d810175782cef953c",
            "6f4e349511bc4cb0bdbe5734887d2023",
            "01ee9d68184d4d479ce5855ad772843f",
            "ef27a27e6a5844e4816e95d627f02ae3",
            "b3fe4febc305400292d0c8c0625c99b3",
            "629066a3730d44c69cdcc83f803b2146",
            "23fe753a41984870a53ea5d38ae5542b",
            "f8072534ac0b411ca7345b31e348c99a",
            "9459f22b75604f1c9f3c6436ba47df64",
            "9c7fd57e39fb4f0d8869ddf80267d618",
            "74f0fe9f395c4d779f5baeef9bcde5c0",
            "1e953a2c1b154d64913cda1e34287cc1",
            "0f2f655b19a3429196f46bf96721f9bd",
            "945fb22b9c254c0b9e059bd189790f67",
            "8cb49bbdbf7b4964bb28e3557dde7bf8",
            "59d8ac2d3e374ac8a34761e2f17569a4",
            "9c442f62ec5e4808b1230f586293458d",
            "622f06d416304186b769f58c0ee462bf",
            "46a640a2540044c3a50edb9f38b31c11",
            "a82438beb9494546913b1426242bc19e",
            "32c1423b9a31494db7ca9863796708c3",
            "39e542c560bc46f9923b306b4a8840f1",
            "ed0979c9224743fca17bd72672ed879b",
            "8b5ab048aef64fc1af016ffb03489277",
            "e96b5cbd3b1e4229b45d92d4f97b4a35",
            "37410199c1e4440486d50cd8115761ba",
            "b4ab592d59dc4f45abf6bd561a0d9c3d",
            "6968815fab0a487b961a2ca5034f6f90",
            "7db492829a5a430cb369926102b515c7",
            "a68a15a97f45410d83cc14079c669465",
            "3eb26b143f23459c82833b34d589a10f",
            "09f55e43ecda4204bdc14a32444a919c",
            "7d280dd551d24262aaeb845c808665f0",
            "e8133ee097714d91b725f97095af75e1",
            "8e60a1bc798b4e7cbdbbbaf9454117e2",
            "3bddd5fabdf845108aba1ceb16e1b00e",
            "ff41e1d5bfc24e5db318d728bf8c9c40",
            "55cb97c8ccec42f595339547e0402117",
            "9a501ac2c2ac41c2acd9b242bc4ec4ec",
            "567cfdfa6d46426b9acd2b96a5fdc93a",
            "5133b22dc2c14aaaa4b067c7c06ea405",
            "ef14d2d34c7347cfb605ed95ea0549c3",
            "e6bcd3860048405ba3f08973ef1277bd",
            "6cf365e4e05e445b909c12b06c229f40",
            "60a198237cdf42c58d5eb2360756d569",
            "df3f00a2e4574693a3fc07dbbe59f8ee",
            "452ab265b0f742728e49086345cbeb35",
            "adf7f663c97948ae8cbcc14682a170b7",
            "70dae3a6d95a4e84a864b5f3571f77ed",
            "91f9952d01e74481a5f830651cb50c42",
            "9c66817193fb46efa973d8d7f94b8eda",
            "3577bb86756d4948b760df4e01eff30b",
            "2e0d51954464465e91cac40a907cb1fa",
            "afc93aed15c24dda91fe8c872b8f9bd0",
            "b0991f7732cf48d59cee65669584b9f1",
            "6bec3258325c4feb96fb4dbded66baa3",
            "49afe4205f274dc0bd8b1d2ab302ffa7",
            "5300962f8b784c72a8496c02295ee02f",
            "cf6df2e32d7944d3aec0ca7b363a6b47",
            "9b9a1abde3f54975bf347dd9ae01e5ea",
            "f0c006cc88e2463cbb1358047c93e3e9",
            "0671b863e09a4c5f8988e28193aca827",
            "d3a9cc90ac5646d9ab49a6550ba18e74",
            "51c2457c43a947d28301a524a20ae88c",
            "b2c963233cba46788835278b19a9de62",
            "f1ec30624ead42dc970d73603eafa47b",
            "18b020f9c36b495fb59c927811ee2559",
            "dd09b62cdff34b47bac7ff21f4503d41",
            "49890c51d54b431a82e94521c4e352a4",
            "507a79ec4ff2436191ea2e35631dd313",
            "a70a414b6ede431ab305be0947ad2a2c",
            "6ca1d786e596433a8a1f49ab9f2ed3f6",
            "ea9075c84fa34988b93fd9a2c7c82302",
            "ca3605253ec445878852ab46caba56ba",
            "ad87ac7f020f42fe8bcc555946fbd0de",
            "bf5d50d92c6b4f3e85d86d1ab738b331",
            "ae950e561b624c44b6ede9bf884042a6",
            "87a438c9a2204e948810d8d7261442c6",
            "dbde7f1709124ce4a613d23df14754a8",
            "a489928602e64c97aac017b2a3f2c8a4",
            "e744b9c9788649d79192a4621d0953aa",
            "d4bdae2bf8904389bcb2e84a000a1d33",
            "91cc54b84f9244b29acfb2adc86676a4",
            "bd44082ae39842faa918d0ecdc62c740",
            "dfff187735b94b23a2e3e52fa24fccd4",
            "86eb2f932f3b4f70b6d7a19f3613e787",
            "ed346407aa5d46fb9ea1a912786b476f",
            "0ffa1a7230d74b6bbde7895506fd0210",
            "bc8e62b33fef4c18a41873aee4468b84",
            "15c5a5db4b9141c8b3fc3b9126e16b33",
            "a9df433da035460197c12b8e328332d3"
          ]
        },
        "id": "e7b2b839",
        "outputId": "924ca234-95cc-456c-dd5f-6b564a5cae41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b3b86e2e178403dbea480033f0c3170"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "302350358d7e45fab74d808c8e2c7c01"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/90.3k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e4344f55939f4de0bed4a55c46e487c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef27a27e6a5844e4816e95d627f02ae3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/720 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8cb49bbdbf7b4964bb28e3557dde7bf8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37410199c1e4440486d50cd8115761ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff41e1d5bfc24e5db318d728bf8c9c40"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "adf7f663c97948ae8cbcc14682a170b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf6df2e32d7944d3aec0ca7b363a6b47"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "507a79ec4ff2436191ea2e35631dd313"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e744b9c9788649d79192a4621d0953aa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding Model Loaded..........\n"
          ]
        }
      ],
      "source": [
        "model_name = \"BAAI/bge-large-en\"\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "hf = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n",
        "print(\"Embedding Model Loaded..........\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cd3fdac",
      "metadata": {
        "id": "5cd3fdac"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "def process_audio_text(audio_filename):\n",
        "    # Load text from the audio file\n",
        "    loader = TextLoader(audio_filename)\n",
        "    text_audio = loader.load()\n",
        "\n",
        "    # Split text using RecursiveCharacterTextSplitter\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "    text_content_audio = text_splitter.split_documents(text_audio)\n",
        "\n",
        "    return text_content_audio"
      ],
      "metadata": {
        "id": "WhwlNESUQmVX"
      },
      "id": "WhwlNESUQmVX",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_files=[\"audio_1.txt\",\"audio_2.txt\",\"audio_3.txt\",\"audio_4.txt\",\"audio_5.txt\"]"
      ],
      "metadata": {
        "id": "ss4c8jkYRfQQ"
      },
      "id": "ss4c8jkYRfQQ",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_contents={}\n",
        "for i,audio_file in enumerate(audio_files):\n",
        "  content=process_audio_text(audio_file)\n",
        "  file_name=f\"text_content_audio_{i+1}\"\n",
        "  text_contents[file_name]=content"
      ],
      "metadata": {
        "id": "_PM-sCUdRxU1"
      },
      "id": "_PM-sCUdRxU1",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_contents['text_content_audio_1'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1y3FH3WeTCpb",
        "outputId": "3a729f98-fa7b-4ab5-98cd-d1e3dcb00d1d"
      },
      "id": "1y3FH3WeTCpb",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content=\"So welcome to this first lecture on this course, DMML. So today I will try to set some context for the course and also tell you a little bit about start the first topic. So I have set up some information just a few minutes back on the moodle page to give you information about the list of topics, roughly that we plan to cover and also about the assessment and all that. So if you have any questions on that, you can look it up and maybe next time we can discuss that. So I won't spend too much time right now on the administrative aspects of the course. We'll just start looking at what we are going to do in this course. So if you look at the title of the course, it clearly says two things. It says data mining and machine learning. In a sense, this is a kind of historical thing. That's how this course was initially created some many years back. So if you want to look at the two parts of the title in some detail. So data mining is a loose term which talks about identifying some hidden\", metadata={'source': 'audio_1.txt'})"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6420577a",
      "metadata": {
        "id": "6420577a"
      },
      "source": [
        "## Create and Store Vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ef86740",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-23T08:36:02.439715Z",
          "start_time": "2023-10-23T08:32:26.262422Z"
        },
        "id": "5ef86740"
      },
      "outputs": [],
      "source": [
        "audio1_store = Chroma.from_documents(text_contents['text_content_audio_1'], hf, collection_metadata={\"hnsw:space\": \"cosine\"}, persist_directory=\"audio1_chroma_cosine\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " !zip -r /content/audio1_chroma_cosine.zip /content/audio1_chroma_cosine"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-3-JVHVaQ4Q",
        "outputId": "9dbbb504-0b97-471c-dc90-45ce0fcdf035"
      },
      "id": "9-3-JVHVaQ4Q",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/audio1_chroma_cosine/ (stored 0%)\n",
            "  adding: content/audio1_chroma_cosine/23e64686-b861-4f1e-8411-c279494758b4/ (stored 0%)\n",
            "  adding: content/audio1_chroma_cosine/23e64686-b861-4f1e-8411-c279494758b4/header.bin (deflated 61%)\n",
            "  adding: content/audio1_chroma_cosine/23e64686-b861-4f1e-8411-c279494758b4/length.bin (deflated 37%)\n",
            "  adding: content/audio1_chroma_cosine/23e64686-b861-4f1e-8411-c279494758b4/link_lists.bin (stored 0%)\n",
            "  adding: content/audio1_chroma_cosine/23e64686-b861-4f1e-8411-c279494758b4/data_level0.bin (deflated 100%)\n",
            "  adding: content/audio1_chroma_cosine/chroma.sqlite3 (deflated 53%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f31a053",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-23T08:49:54.220901Z",
          "start_time": "2023-10-23T08:49:11.975656Z"
        },
        "id": "7f31a053"
      },
      "outputs": [],
      "source": [
        "audio2_store = Chroma.from_documents(text_contents['text_content_audio_2'], hf, collection_metadata={\"hnsw:space\": \"cosine\"}, persist_directory=\"audio2_chroma_cosine\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " !zip -r /content/audio2_chroma_cosine.zip /content/audio2_chroma_cosine"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEFYquTYcOuV",
        "outputId": "2c131ced-05e8-43fa-bb5b-157b18370ec2"
      },
      "id": "BEFYquTYcOuV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/audio2_chroma_cosine/ (stored 0%)\n",
            "  adding: content/audio2_chroma_cosine/39fe4e3b-be9c-44a2-84d4-5c8f189c301d/ (stored 0%)\n",
            "  adding: content/audio2_chroma_cosine/39fe4e3b-be9c-44a2-84d4-5c8f189c301d/header.bin (deflated 61%)\n",
            "  adding: content/audio2_chroma_cosine/39fe4e3b-be9c-44a2-84d4-5c8f189c301d/length.bin (deflated 30%)\n",
            "  adding: content/audio2_chroma_cosine/39fe4e3b-be9c-44a2-84d4-5c8f189c301d/link_lists.bin (stored 0%)\n",
            "  adding: content/audio2_chroma_cosine/39fe4e3b-be9c-44a2-84d4-5c8f189c301d/data_level0.bin (deflated 7%)\n",
            "  adding: content/audio2_chroma_cosine/chroma.sqlite3 (deflated 50%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audio3_store = Chroma.from_documents(text_contents['text_content_audio_3'], hf, collection_metadata={\"hnsw:space\": \"cosine\"}, persist_directory=\"audio3_chroma_cosine\")"
      ],
      "metadata": {
        "id": "fuNALvlgTlCv"
      },
      "id": "fuNALvlgTlCv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " !zip -r /content/audio3_chroma_cosine.zip /content/audio3_chroma_cosine"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGhxdNzlcbsW",
        "outputId": "3818929e-866c-4574-ccd5-af2a4ea678b0"
      },
      "id": "eGhxdNzlcbsW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/audio3_chroma_cosine/ (stored 0%)\n",
            "  adding: content/audio3_chroma_cosine/2cd5c8b3-2d84-4649-8d4a-0448b568a700/ (stored 0%)\n",
            "  adding: content/audio3_chroma_cosine/2cd5c8b3-2d84-4649-8d4a-0448b568a700/header.bin (deflated 61%)\n",
            "  adding: content/audio3_chroma_cosine/2cd5c8b3-2d84-4649-8d4a-0448b568a700/length.bin (deflated 98%)\n",
            "  adding: content/audio3_chroma_cosine/2cd5c8b3-2d84-4649-8d4a-0448b568a700/link_lists.bin (stored 0%)\n",
            "  adding: content/audio3_chroma_cosine/2cd5c8b3-2d84-4649-8d4a-0448b568a700/data_level0.bin (deflated 8%)\n",
            "  adding: content/audio3_chroma_cosine/chroma.sqlite3 (deflated 53%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audio4_store = Chroma.from_documents(text_contents['text_content_audio_4'], hf, collection_metadata={\"hnsw:space\": \"cosine\"}, persist_directory=\"audio4_chroma_cosine\")"
      ],
      "metadata": {
        "id": "fD2CVCTQTrmy"
      },
      "id": "fD2CVCTQTrmy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " !zip -r /content/audio4_chroma_cosine.zip /content/audio4_chroma_cosine"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ib0STE_Qcqc9",
        "outputId": "8d17f19b-8444-40f3-f3ae-5c03dbc6fb82"
      },
      "id": "ib0STE_Qcqc9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/audio4_chroma_cosine/ (stored 0%)\n",
            "  adding: content/audio4_chroma_cosine/9af56ffe-cdde-4811-853d-d366cdcb2eee/ (stored 0%)\n",
            "  adding: content/audio4_chroma_cosine/9af56ffe-cdde-4811-853d-d366cdcb2eee/header.bin (deflated 61%)\n",
            "  adding: content/audio4_chroma_cosine/9af56ffe-cdde-4811-853d-d366cdcb2eee/length.bin (deflated 32%)\n",
            "  adding: content/audio4_chroma_cosine/9af56ffe-cdde-4811-853d-d366cdcb2eee/link_lists.bin (stored 0%)\n",
            "  adding: content/audio4_chroma_cosine/9af56ffe-cdde-4811-853d-d366cdcb2eee/data_level0.bin (deflated 94%)\n",
            "  adding: content/audio4_chroma_cosine/chroma.sqlite3 (deflated 50%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audio5_store = Chroma.from_documents(text_contents['text_content_audio_5'], hf, collection_metadata={\"hnsw:space\": \"cosine\"}, persist_directory=\"audio5_chroma_cosine\")"
      ],
      "metadata": {
        "id": "6CVpZO-xTsf3"
      },
      "id": "6CVpZO-xTsf3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " !zip -r /content/audio5_chroma_cosine.zip /content/audio5_chroma_cosine"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAi7sGXwcvxn",
        "outputId": "bbc84bb2-ecbc-4706-d83b-e08f46331a1b"
      },
      "id": "AAi7sGXwcvxn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/audio5_chroma_cosine/ (stored 0%)\n",
            "  adding: content/audio5_chroma_cosine/c048879c-0cc0-4984-8f6b-74bc56de0af7/ (stored 0%)\n",
            "  adding: content/audio5_chroma_cosine/c048879c-0cc0-4984-8f6b-74bc56de0af7/header.bin (deflated 61%)\n",
            "  adding: content/audio5_chroma_cosine/c048879c-0cc0-4984-8f6b-74bc56de0af7/length.bin (deflated 37%)\n",
            "  adding: content/audio5_chroma_cosine/c048879c-0cc0-4984-8f6b-74bc56de0af7/link_lists.bin (stored 0%)\n",
            "  adding: content/audio5_chroma_cosine/c048879c-0cc0-4984-8f6b-74bc56de0af7/data_level0.bin (deflated 17%)\n",
            "  adding: content/audio5_chroma_cosine/chroma.sqlite3 (deflated 54%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import patoolib"
      ],
      "metadata": {
        "id": "GxYK_C6kr78W"
      },
      "id": "GxYK_C6kr78W",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patoolib.extract_archive('audio1_chroma_cosine.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "yHfhhmuesGiV",
        "outputId": "2988d9f0-ef5d-4621-84d0-5d714bac329a"
      },
      "id": "yHfhhmuesGiV",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO patool: Extracting audio1_chroma_cosine.zip ...\n",
            "INFO:patool:Extracting audio1_chroma_cosine.zip ...\n",
            "INFO patool: running /usr/bin/7z x -o./Unpack_m2p3tab8 -- audio1_chroma_cosine.zip\n",
            "INFO:patool:running /usr/bin/7z x -o./Unpack_m2p3tab8 -- audio1_chroma_cosine.zip\n",
            "INFO patool:     with input=''\n",
            "INFO:patool:    with input=''\n",
            "INFO patool: ... audio1_chroma_cosine.zip extracted to `audio1_chroma_cosine' (local file exists).\n",
            "INFO:patool:... audio1_chroma_cosine.zip extracted to `audio1_chroma_cosine' (local file exists).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'audio1_chroma_cosine'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "patoolib.extract_archive('audio2_chroma_cosine.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "IsCtmHpq03Tk",
        "outputId": "925e2859-e0db-44c2-8cc2-20fe8ce5680e"
      },
      "id": "IsCtmHpq03Tk",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO patool: Extracting audio2_chroma_cosine.zip ...\n",
            "INFO:patool:Extracting audio2_chroma_cosine.zip ...\n",
            "INFO patool: running /usr/bin/7z x -o./Unpack_hddqui83 -- audio2_chroma_cosine.zip\n",
            "INFO:patool:running /usr/bin/7z x -o./Unpack_hddqui83 -- audio2_chroma_cosine.zip\n",
            "INFO patool:     with input=''\n",
            "INFO:patool:    with input=''\n",
            "INFO patool: ... audio2_chroma_cosine.zip extracted to `audio2_chroma_cosine' (local file exists).\n",
            "INFO:patool:... audio2_chroma_cosine.zip extracted to `audio2_chroma_cosine' (local file exists).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'audio2_chroma_cosine'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "patoolib.extract_archive('audio3_chroma_cosine.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "26T_Ayq01E2Q",
        "outputId": "dd5fd490-92d5-442f-bd66-9cc49a4bfc49"
      },
      "id": "26T_Ayq01E2Q",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO patool: Extracting audio3_chroma_cosine.zip ...\n",
            "INFO:patool:Extracting audio3_chroma_cosine.zip ...\n",
            "INFO patool: running /usr/bin/7z x -o./Unpack_0zkn8lc8 -- audio3_chroma_cosine.zip\n",
            "INFO:patool:running /usr/bin/7z x -o./Unpack_0zkn8lc8 -- audio3_chroma_cosine.zip\n",
            "INFO patool:     with input=''\n",
            "INFO:patool:    with input=''\n",
            "INFO patool: ... audio3_chroma_cosine.zip extracted to `audio3_chroma_cosine' (local file exists).\n",
            "INFO:patool:... audio3_chroma_cosine.zip extracted to `audio3_chroma_cosine' (local file exists).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'audio3_chroma_cosine'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "patoolib.extract_archive('audio4_chroma_cosine.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "WOsw4inK1HzA",
        "outputId": "5f7aa9ef-f524-428e-cbbd-b6bdcf86f233"
      },
      "id": "WOsw4inK1HzA",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO patool: Extracting audio4_chroma_cosine.zip ...\n",
            "INFO:patool:Extracting audio4_chroma_cosine.zip ...\n",
            "INFO patool: running /usr/bin/7z x -o./Unpack_n5p2b1b0 -- audio4_chroma_cosine.zip\n",
            "INFO:patool:running /usr/bin/7z x -o./Unpack_n5p2b1b0 -- audio4_chroma_cosine.zip\n",
            "INFO patool:     with input=''\n",
            "INFO:patool:    with input=''\n",
            "INFO patool: ... audio4_chroma_cosine.zip extracted to `audio4_chroma_cosine' (local file exists).\n",
            "INFO:patool:... audio4_chroma_cosine.zip extracted to `audio4_chroma_cosine' (local file exists).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'audio4_chroma_cosine'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "patoolib.extract_archive('audio5_chroma_cosine.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "oW7NWB4E1QuY",
        "outputId": "b61bc6e4-4ac3-4d0f-ede2-fc096f251e5c"
      },
      "id": "oW7NWB4E1QuY",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO patool: Extracting audio5_chroma_cosine.zip ...\n",
            "INFO:patool:Extracting audio5_chroma_cosine.zip ...\n",
            "INFO patool: running /usr/bin/7z x -o./Unpack_si1kvksi -- audio5_chroma_cosine.zip\n",
            "INFO:patool:running /usr/bin/7z x -o./Unpack_si1kvksi -- audio5_chroma_cosine.zip\n",
            "INFO patool:     with input=''\n",
            "INFO:patool:    with input=''\n",
            "INFO patool: ... audio5_chroma_cosine.zip extracted to `audio5_chroma_cosine' (local file exists).\n",
            "INFO:patool:... audio5_chroma_cosine.zip extracted to `audio5_chroma_cosine' (local file exists).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'audio5_chroma_cosine'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa1dfda5",
      "metadata": {
        "id": "aa1dfda5"
      },
      "source": [
        "## Load Vector Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "9faea31c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-23T09:01:38.800968Z",
          "start_time": "2023-10-23T09:01:38.791486Z"
        },
        "id": "9faea31c"
      },
      "outputs": [],
      "source": [
        "load_audio1_store = Chroma(persist_directory=\"/content/audio1_chroma_cosine/content/audio1_chroma_cosine\", embedding_function=hf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "4767de7a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-23T09:02:04.719254Z",
          "start_time": "2023-10-23T09:02:04.709131Z"
        },
        "id": "4767de7a"
      },
      "outputs": [],
      "source": [
        "load_audio2_store = Chroma(persist_directory=\"/content/audio2_chroma_cosine/content/audio2_chroma_cosine\", embedding_function=hf)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load_audio3_store = Chroma(persist_directory=\"/content/audio3_chroma_cosine/content/audio3_chroma_cosine\", embedding_function=hf)"
      ],
      "metadata": {
        "id": "sdSJBQZxc-a1"
      },
      "id": "sdSJBQZxc-a1",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_audio4_store = Chroma(persist_directory=\"/content/audio4_chroma_cosine/content/audio4_chroma_cosine\", embedding_function=hf)"
      ],
      "metadata": {
        "id": "QCpK3TVPdDR3"
      },
      "id": "QCpK3TVPdDR3",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_audio5_store = Chroma(persist_directory=\"/content/audio5_chroma_cosine/content/\", embedding_function=hf)"
      ],
      "metadata": {
        "id": "2RDQealbdGF3"
      },
      "id": "2RDQealbdGF3",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "bbe6d5bf",
      "metadata": {
        "id": "bbe6d5bf"
      },
      "source": [
        "## Init Merge Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "dca28b13",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-23T09:02:52.862205Z",
          "start_time": "2023-10-23T09:02:52.857127Z"
        },
        "id": "dca28b13"
      },
      "outputs": [],
      "source": [
        "retriever_audio1 = load_audio1_store.as_retriever(search_type = \"similarity\", search_kwargs = {\"k\":3})\n",
        "\n",
        "retriever_audio2 = load_audio2_store.as_retriever(search_type = \"similarity\", search_kwargs = {\"k\":3})\n",
        "\n",
        "retriever_audio3 = load_audio3_store.as_retriever(search_type = \"similarity\", search_kwargs = {\"k\":3})\n",
        "\n",
        "retriever_audio4 = load_audio4_store.as_retriever(search_type = \"similarity\", search_kwargs = {\"k\":3})\n",
        "\n",
        "retriever_audio5 = load_audio5_store.as_retriever(search_type = \"similarity\", search_kwargs = {\"k\":3})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "7cf4780e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-23T09:02:54.002581Z",
          "start_time": "2023-10-23T09:02:53.999837Z"
        },
        "id": "7cf4780e"
      },
      "outputs": [],
      "source": [
        "lotr = MergerRetriever(retrievers=[retriever_audio1, retriever_audio2,retriever_audio3,retriever_audio4,retriever_audio5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "fb9ccdc9",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-23T09:02:54.636861Z",
          "start_time": "2023-10-23T09:02:54.631590Z"
        },
        "id": "fb9ccdc9",
        "outputId": "240a42dd-9392-4a1d-9cfd-0405089aa137",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MergerRetriever(retrievers=[VectorStoreRetriever(tags=['Chroma', 'HuggingFaceBgeEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7f6fb37d3a90>, search_kwargs={'k': 3}), VectorStoreRetriever(tags=['Chroma', 'HuggingFaceBgeEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7f6fb37d0e20>, search_kwargs={'k': 3}), VectorStoreRetriever(tags=['Chroma', 'HuggingFaceBgeEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7f6fb37d05b0>, search_kwargs={'k': 3}), VectorStoreRetriever(tags=['Chroma', 'HuggingFaceBgeEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7f6fb37d3460>, search_kwargs={'k': 3}), VectorStoreRetriever(tags=['Chroma', 'HuggingFaceBgeEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7f6fb37d30a0>, search_kwargs={'k': 3})])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "lotr"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28a46e7e",
      "metadata": {
        "id": "28a46e7e"
      },
      "source": [
        "## Perform Semantic Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d61909f7",
      "metadata": {
        "id": "d61909f7"
      },
      "outputs": [],
      "source": [
        "query=\" What are some challenges associated with data collection?\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs = lotr.get_relevant_documents(query)\n",
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5_YbZuz6tUJ",
        "outputId": "62e66ee6-b3af-4079-c28d-daa726ef19ac"
      },
      "id": "J5_YbZuz6tUJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"entered by somebody and then converted to electronic forms. So that would be, there would be two levels of potential sources for errors. The person writing down the information and then the person typing in the information. Now, gradually these kind of electronic forms are spilled in directly, so at least the source of the error is reduced to one step. But still, people mistype things. I mean, there are any number of situations where people type their email address wrong and so notifications don't reach them and so on. So there is this data collection. How do you collect the data and how do you clean it? And the third thing is, how do you make it uniform? So when data is being collected by different people, they may collect different things. And for instance, if you look at the government, typically the government collects data in different forms. For instance, there is a public distribution system which the ration shops, so they collect some information about who is collecting ration\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"have to, in the example that you gave, you would have to first separate out the data for these, say, the people who buy one month groceries at a time from those who buy at more frequent intervals and then analyze them separately, because together the data doesn't actually make sense. So last time, if you remember, I talked about this unsupervised. So maybe one thing that you need to do when you have something large is to basically look at the transactions and categorize them according to some criteria, maybe by the size of the basket. And then you will find that there are maybe a lot of people who buy five items at a time. There are people who buy ten to 20 items at a time. There are people who buy 40 items at a time. And maybe you need to then apply the model separately to each of these clusters. So all of these things are not a one size fits all solution. And also, it's not that one approach on its own is going to solve the problem at any given time. So you need to have a kind of\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content='been collected correctly. So as we said, the main issue is really how to ask the questions and what this means. Sorry, sir, I have a question. Yeah. Sir, once we know that the data is young, we are making a prediction and it come out that young. So like other attribute is not relevant other than it has job or not, and then we are not going to look at other attributes. Yeah, but remember that which attributes to look like vary from one path to another. So maybe in this case, right, so what we are saying is that if they are young, we only need the job, right? But if they are middle aged, we need to know the house. So for different combinations, so they may not occur together. I agree, in any decision, but each of these plays a role somewhere. Now, if there is another attribute which never plays a role, as we will see the next tree as that example. Right? So if you look at this, this is a different tree for the same data set. So instead of first asking the age, we ask whether you own a', metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"picked up is because we have not interpreted. So you might ask, okay, why passport number? But it could be something indirect, like, it could be some kind of an encoded timestamp. It could be some actions which are recorded in some system, and then one of the columns records somehow the timestamp, and no two events happen with the same timestamp. So there could be any number of hidden reasons why something is actually unique. And either you have to clean up the data and check it, or you have to build a safeguard against this happening. It may or may not happen, but I just wanted to bring this up because this is a byproduct of the algorithm that we have now. It's a shortcoming. It's a shortcoming in the sense that if we apply the algorithm that we have now, right now, in such a situation, blindly, then we will end up with this kind of curious choice right up front. It will tell us that the only question you need to ask is, say, the passport number of the patient, and then you will be\", metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content='It asks one vertical question and one horizontal question and isolates that. Similarly, there is a blue point in the middle here, right there on the right hand side, you can see where my cursor is, that blue part. So here it basically create just to pull that blue point out. So this is, again, kind of an experimental or empirical justification of the argument that smaller trees will give you more manageable and more understandable marks. The tree on the right sort of makes mistakes, but you can kind of figure out why it is drawing the lines where they are. And there are going to be a few anomalies with it. The left hand side is arguably got fewer anomalies, but it has created these strange islands of yellow at the bottom and the spike of green in the middle and all that, which may not make much sense. So this is as far as constraining the thing. Now, if I take this. So now we go back to our, this iris data set. So iris data set had capital X as the input. That was our original input.', metadata={'source': 'audio_5.txt'}),\n",
              " Document(page_content=\"of elearning platforms, there's a lot of emphasis on trying to structure topics in such a way that students who find difficulty, you can identify why they are facing difficulties. So you want to think of these concepts that you're teaching as these items. And basket is now a set of concepts. And if you look at a concept which is difficult to learn, maybe there is a connection to another concept which is also difficult to learn. So you might want to group this. So people who misunderstand a also misunderstand b. And therefore maybe the problems has to start by fixing b. And another place where this is used is in kind of similarities of documents. So if two documents share words, then you can think they are similar. So you can think of documents. Or if two words appear rather in many documents, you can think they are similar. So if you say that wherever x appears, y also appears, then it may mean that x and y are actually related concepts. So this is the context. So the real question is\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"can measure it, but you cannot. In some sense, if you could validate it, then this whole problem becomes much simpler. But the whole difficulty with machine learning is exactly this, that there is no sensible way to predict the behavior or even when it will fail. Even if you could say that this is likely to work well here and not likely to work well there that itself would be a huge improvement, not even necessarily a very precise quantification, but just some indication. But all these are very speculative. So that's a huge challenge. So there are some theoretical things you can say which we may not, may or may not get to in this course. But it's a big problem. This is the so called generalization problem. So that's what basically, if you build association rules, is it going to hold or not? I don't know. Okay, sir, I had a question. We have been talking about items and transactions, but we haven't anywhere spoken about whether one customer, suppose if you're talking about somebody\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content=\"and these subtables be destroyed. And when you stop, you ask whether it's uniform or whether there's a majority. So this is what we were discussing. So a non uniform leaf node where I have run out of questions, but I have different classes. So basically I have identical combination of attributes, but different classes is something which indicates that our data is somehow incomplete. The attributes are not capturing all the criteria which are actually used for the classification. There are some hidden attributes which are not being captured, and this happens quite often. So we can't derive this. This might happen for a variety of reasons. It might happen in these kind of manmade scenarios, like giving loans. But even when you're trying to classify, this is happening around us. Every example can be traced to COVID, but right now we see that, right? So there are lots of situations where we have to realize that different people with the same bit different symptoms. So conversely, two\", metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"against some unknown data. So, one other strategy where you don't want to, I mean, one of the other disadvantages of doing this is that maybe that there are some minority anomalies in this data, and the choice of test data that you have made might have hidden all these anomalies, so you never see them, and maybe it's important to see them. So there may be many situations, or maybe you just don't have enough data as a whole to build a good model by only looking at 80%. So another strategy is to systematically do this with different subsets. So what you're really asking at some higher level is that machine learning approach. So, remember, we have seen only decision trees, but we are going to see many models. So there are clearly many ways to build models. And the reason, whenever you see that there are many ways to do something, it's only because there is no guarantee that a given way is the best one. So more or less what you want to validate is, I have, say, seven different strategies\", metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"the width not equal to 1.8. Among the blue flowers, we are eliminating the. We know there is one which is 1.8 and that will get eliminated. Among the green flowers. It's possible that there is something which is 1.8, but because y is equal to two, it will get retaked all the blue flowers which have 1.8, but I believe there's only one in the data set. So suppose instead of the point that we removed in this time, if we removed a different point, would we get a different decision tree? For that, it's not necessary. You will get a decision tree. It's different. But in general, the removing and modifying the data set in a very small way can produce it. So it's not guaranteed. It will produce. So you can experiment with it. So you could, for instance, in the same data set, probably that's a good experiment for you to use, which is that. Notice that we have two types of discrepancies here, right? So we have this horizontal line and we have this one blue thing which is above the horizontal.\", metadata={'source': 'audio_5.txt'}),\n",
              " Document(page_content=\"these values in that example. So unless you specify that, you cannot say that this is unlikely to be true or this is likely to be true, because it will be specific to the interpretation. But the problem now, we have now reduced it to one which is uninterpreted. We are just talking about abstract items. So there was a question in the chat about dimensionality reduction connected to PCA. So, yes, the PCA will be one part of it, but there are also other things that we will see. Sir, I have a question. Yeah. When we model this problem in this way, we didn't capture the frequency of items in a transaction. We were just looking at whether or not that item is present or not. Yeah, but I think for most of these examples, whether someone's buying from a shop or whether it's words in a document, that frequency will be a part of it. Right. Which we are not capturing here. Yeah. So usually this is how mathematical models work. You start with the most basic model, and then you add any features\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"an algorithmic problem. It's not technically a learning problem, because this is a fixed problem once we fix these quantities. So given the set of items and the transactions, and given these two thresholds, find every x y which satisfies these two constraints. This is what we are trying to do. So we will see what it means for learning at the end. Sir, I had a question. So therefore, what we said is that a rule is interesting only if this ratio is bigger than sigma. Right? The fraction of transactions where we see the target of the rule must be at least sigma. If I take m to the other side, this is same as saying there's a number of transactions with x and Y. So this is just the number of transactions. X and y exceeds sigma times the total. It's some whatever, 1% of the total, 10% of the total, and so on. So what we said last time is that this would be our first criteria. So we want to find x union Y, which is plausible. Once we have x union Y, which is plausible, we will try to break\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content=\"we have to realize that different people with the same bit different symptoms. So conversely, two people with the same symptoms don't necessarily. So there are asymptomatic COVID patients. There are people who have a sore throat and a cold and a headache who don't necessarily have COVID. So I cannot just say that if you have these symptoms, you must have COVID, and if you don't have these symptoms, you cannot have COVID. Right. So clearly there is something going on in the background, and that background in our case is revealed through a test. Even tests have errors and all that. So it's really a problem that we have to grapple with in real life. And we will just assume for now that when we reach such a situation, we just take a majority decision. But realistically, if we get too many of these situations, and it's very difficult to decide, we have to ask whether the data has actually been collected correctly. So as we said, the main issue is really how to ask the questions and what\", metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"the only question you need to ask is, say, the passport number of the patient, and then you will be able to predict whether they have a disease or not, which is totally nonsensical. Right? So the greedy algorithm will say, take it, but from an interpretation point of view, it's a totally useless choice. I'm not saying that this will happen. I'm just saying that if you provide an algorithm in any machine learning situation, you have to be aware of the fact that this algorithm is going to be applied in general blindly to the data. So it has to be driven by the data. So if the data can produce some peculiar behavior, you have to be able to tailor your algorithm to take care of this peculiar behavior. The question is, how would we adapt our algorithm to fix this? So right now, our tree building algorithm blindly picks the attribute that maximizes the information gain. So we need to somehow penalize those attributes which have a wide number of possibilities. Now this is also intuitively\", metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"natural. So this is a generic problem with this kind of tree model, that it is very sensitive to perturbations in the data. We will look at a more familiar problem, which you are generally aware of called regression. Right? So in regression, what do we do? We take a bunch of points, we try to fit a line to it. Now, if I take one point in that set and remove it, or if I shift it a little bit, the line will perhaps change, but it will change in a very minor way, right? The slope will change slightly. So small changes in input produce small changes in output. So that is a kind of stable kind of situation. So this is something called variance. So how much variance is there in your model? So, variance is basically a property which says, is it the case? So if you have low variance, then small changes in input produce small changes in output. But decision tree, by definition, is a kind of discrete kind of thing. There's no way to slightly change a tree. You change a question and the tree\", metadata={'source': 'audio_5.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_transformers import (\n",
        "    LongContextReorder,\n",
        ")\n",
        "reordering = LongContextReorder()\n",
        "reordered_docs = reordering.transform_documents(docs)\n",
        "\n",
        "# Confirm that the 4 relevant documents are at beginning and end.\n",
        "reordered_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wkO1WQ_64kI",
        "outputId": "e4b3ef5b-51c4-4363-f54a-6ea0fb462f04"
      },
      "id": "4wkO1WQ_64kI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"entered by somebody and then converted to electronic forms. So that would be, there would be two levels of potential sources for errors. The person writing down the information and then the person typing in the information. Now, gradually these kind of electronic forms are spilled in directly, so at least the source of the error is reduced to one step. But still, people mistype things. I mean, there are any number of situations where people type their email address wrong and so notifications don't reach them and so on. So there is this data collection. How do you collect the data and how do you clean it? And the third thing is, how do you make it uniform? So when data is being collected by different people, they may collect different things. And for instance, if you look at the government, typically the government collects data in different forms. For instance, there is a public distribution system which the ration shops, so they collect some information about who is collecting ration\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content='been collected correctly. So as we said, the main issue is really how to ask the questions and what this means. Sorry, sir, I have a question. Yeah. Sir, once we know that the data is young, we are making a prediction and it come out that young. So like other attribute is not relevant other than it has job or not, and then we are not going to look at other attributes. Yeah, but remember that which attributes to look like vary from one path to another. So maybe in this case, right, so what we are saying is that if they are young, we only need the job, right? But if they are middle aged, we need to know the house. So for different combinations, so they may not occur together. I agree, in any decision, but each of these plays a role somewhere. Now, if there is another attribute which never plays a role, as we will see the next tree as that example. Right? So if you look at this, this is a different tree for the same data set. So instead of first asking the age, we ask whether you own a', metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content='It asks one vertical question and one horizontal question and isolates that. Similarly, there is a blue point in the middle here, right there on the right hand side, you can see where my cursor is, that blue part. So here it basically create just to pull that blue point out. So this is, again, kind of an experimental or empirical justification of the argument that smaller trees will give you more manageable and more understandable marks. The tree on the right sort of makes mistakes, but you can kind of figure out why it is drawing the lines where they are. And there are going to be a few anomalies with it. The left hand side is arguably got fewer anomalies, but it has created these strange islands of yellow at the bottom and the spike of green in the middle and all that, which may not make much sense. So this is as far as constraining the thing. Now, if I take this. So now we go back to our, this iris data set. So iris data set had capital X as the input. That was our original input.', metadata={'source': 'audio_5.txt'}),\n",
              " Document(page_content=\"can measure it, but you cannot. In some sense, if you could validate it, then this whole problem becomes much simpler. But the whole difficulty with machine learning is exactly this, that there is no sensible way to predict the behavior or even when it will fail. Even if you could say that this is likely to work well here and not likely to work well there that itself would be a huge improvement, not even necessarily a very precise quantification, but just some indication. But all these are very speculative. So that's a huge challenge. So there are some theoretical things you can say which we may not, may or may not get to in this course. But it's a big problem. This is the so called generalization problem. So that's what basically, if you build association rules, is it going to hold or not? I don't know. Okay, sir, I had a question. We have been talking about items and transactions, but we haven't anywhere spoken about whether one customer, suppose if you're talking about somebody\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content=\"against some unknown data. So, one other strategy where you don't want to, I mean, one of the other disadvantages of doing this is that maybe that there are some minority anomalies in this data, and the choice of test data that you have made might have hidden all these anomalies, so you never see them, and maybe it's important to see them. So there may be many situations, or maybe you just don't have enough data as a whole to build a good model by only looking at 80%. So another strategy is to systematically do this with different subsets. So what you're really asking at some higher level is that machine learning approach. So, remember, we have seen only decision trees, but we are going to see many models. So there are clearly many ways to build models. And the reason, whenever you see that there are many ways to do something, it's only because there is no guarantee that a given way is the best one. So more or less what you want to validate is, I have, say, seven different strategies\", metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"these values in that example. So unless you specify that, you cannot say that this is unlikely to be true or this is likely to be true, because it will be specific to the interpretation. But the problem now, we have now reduced it to one which is uninterpreted. We are just talking about abstract items. So there was a question in the chat about dimensionality reduction connected to PCA. So, yes, the PCA will be one part of it, but there are also other things that we will see. Sir, I have a question. Yeah. When we model this problem in this way, we didn't capture the frequency of items in a transaction. We were just looking at whether or not that item is present or not. Yeah, but I think for most of these examples, whether someone's buying from a shop or whether it's words in a document, that frequency will be a part of it. Right. Which we are not capturing here. Yeah. So usually this is how mathematical models work. You start with the most basic model, and then you add any features\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"we have to realize that different people with the same bit different symptoms. So conversely, two people with the same symptoms don't necessarily. So there are asymptomatic COVID patients. There are people who have a sore throat and a cold and a headache who don't necessarily have COVID. So I cannot just say that if you have these symptoms, you must have COVID, and if you don't have these symptoms, you cannot have COVID. Right. So clearly there is something going on in the background, and that background in our case is revealed through a test. Even tests have errors and all that. So it's really a problem that we have to grapple with in real life. And we will just assume for now that when we reach such a situation, we just take a majority decision. But realistically, if we get too many of these situations, and it's very difficult to decide, we have to ask whether the data has actually been collected correctly. So as we said, the main issue is really how to ask the questions and what\", metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"natural. So this is a generic problem with this kind of tree model, that it is very sensitive to perturbations in the data. We will look at a more familiar problem, which you are generally aware of called regression. Right? So in regression, what do we do? We take a bunch of points, we try to fit a line to it. Now, if I take one point in that set and remove it, or if I shift it a little bit, the line will perhaps change, but it will change in a very minor way, right? The slope will change slightly. So small changes in input produce small changes in output. So that is a kind of stable kind of situation. So this is something called variance. So how much variance is there in your model? So, variance is basically a property which says, is it the case? So if you have low variance, then small changes in input produce small changes in output. But decision tree, by definition, is a kind of discrete kind of thing. There's no way to slightly change a tree. You change a question and the tree\", metadata={'source': 'audio_5.txt'}),\n",
              " Document(page_content=\"the only question you need to ask is, say, the passport number of the patient, and then you will be able to predict whether they have a disease or not, which is totally nonsensical. Right? So the greedy algorithm will say, take it, but from an interpretation point of view, it's a totally useless choice. I'm not saying that this will happen. I'm just saying that if you provide an algorithm in any machine learning situation, you have to be aware of the fact that this algorithm is going to be applied in general blindly to the data. So it has to be driven by the data. So if the data can produce some peculiar behavior, you have to be able to tailor your algorithm to take care of this peculiar behavior. The question is, how would we adapt our algorithm to fix this? So right now, our tree building algorithm blindly picks the attribute that maximizes the information gain. So we need to somehow penalize those attributes which have a wide number of possibilities. Now this is also intuitively\", metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"an algorithmic problem. It's not technically a learning problem, because this is a fixed problem once we fix these quantities. So given the set of items and the transactions, and given these two thresholds, find every x y which satisfies these two constraints. This is what we are trying to do. So we will see what it means for learning at the end. Sir, I had a question. So therefore, what we said is that a rule is interesting only if this ratio is bigger than sigma. Right? The fraction of transactions where we see the target of the rule must be at least sigma. If I take m to the other side, this is same as saying there's a number of transactions with x and Y. So this is just the number of transactions. X and y exceeds sigma times the total. It's some whatever, 1% of the total, 10% of the total, and so on. So what we said last time is that this would be our first criteria. So we want to find x union Y, which is plausible. Once we have x union Y, which is plausible, we will try to break\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content=\"the width not equal to 1.8. Among the blue flowers, we are eliminating the. We know there is one which is 1.8 and that will get eliminated. Among the green flowers. It's possible that there is something which is 1.8, but because y is equal to two, it will get retaked all the blue flowers which have 1.8, but I believe there's only one in the data set. So suppose instead of the point that we removed in this time, if we removed a different point, would we get a different decision tree? For that, it's not necessary. You will get a decision tree. It's different. But in general, the removing and modifying the data set in a very small way can produce it. So it's not guaranteed. It will produce. So you can experiment with it. So you could, for instance, in the same data set, probably that's a good experiment for you to use, which is that. Notice that we have two types of discrepancies here, right? So we have this horizontal line and we have this one blue thing which is above the horizontal.\", metadata={'source': 'audio_5.txt'}),\n",
              " Document(page_content=\"and these subtables be destroyed. And when you stop, you ask whether it's uniform or whether there's a majority. So this is what we were discussing. So a non uniform leaf node where I have run out of questions, but I have different classes. So basically I have identical combination of attributes, but different classes is something which indicates that our data is somehow incomplete. The attributes are not capturing all the criteria which are actually used for the classification. There are some hidden attributes which are not being captured, and this happens quite often. So we can't derive this. This might happen for a variety of reasons. It might happen in these kind of manmade scenarios, like giving loans. But even when you're trying to classify, this is happening around us. Every example can be traced to COVID, but right now we see that, right? So there are lots of situations where we have to realize that different people with the same bit different symptoms. So conversely, two\", metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"of elearning platforms, there's a lot of emphasis on trying to structure topics in such a way that students who find difficulty, you can identify why they are facing difficulties. So you want to think of these concepts that you're teaching as these items. And basket is now a set of concepts. And if you look at a concept which is difficult to learn, maybe there is a connection to another concept which is also difficult to learn. So you might want to group this. So people who misunderstand a also misunderstand b. And therefore maybe the problems has to start by fixing b. And another place where this is used is in kind of similarities of documents. So if two documents share words, then you can think they are similar. So you can think of documents. Or if two words appear rather in many documents, you can think they are similar. So if you say that wherever x appears, y also appears, then it may mean that x and y are actually related concepts. So this is the context. So the real question is\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"picked up is because we have not interpreted. So you might ask, okay, why passport number? But it could be something indirect, like, it could be some kind of an encoded timestamp. It could be some actions which are recorded in some system, and then one of the columns records somehow the timestamp, and no two events happen with the same timestamp. So there could be any number of hidden reasons why something is actually unique. And either you have to clean up the data and check it, or you have to build a safeguard against this happening. It may or may not happen, but I just wanted to bring this up because this is a byproduct of the algorithm that we have now. It's a shortcoming. It's a shortcoming in the sense that if we apply the algorithm that we have now, right now, in such a situation, blindly, then we will end up with this kind of curious choice right up front. It will tell us that the only question you need to ask is, say, the passport number of the patient, and then you will be\", metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"have to, in the example that you gave, you would have to first separate out the data for these, say, the people who buy one month groceries at a time from those who buy at more frequent intervals and then analyze them separately, because together the data doesn't actually make sense. So last time, if you remember, I talked about this unsupervised. So maybe one thing that you need to do when you have something large is to basically look at the transactions and categorize them according to some criteria, maybe by the size of the basket. And then you will find that there are maybe a lot of people who buy five items at a time. There are people who buy ten to 20 items at a time. There are people who buy 40 items at a time. And maybe you need to then apply the model separately to each of these clusters. So all of these things are not a one size fits all solution. And also, it's not that one approach on its own is going to solve the problem at any given time. So you need to have a kind of\", metadata={'source': 'audio_2.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_2=\" What are the advantages of the Apriori algorithm?\"\n"
      ],
      "metadata": {
        "id": "gVin1CH17aMB"
      },
      "id": "gVin1CH17aMB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = lotr.get_relevant_documents(query_2)\n",
        "docs"
      ],
      "metadata": {
        "id": "OxUgDKKa7mCX",
        "outputId": "1892d6ea-cbae-4704-9987-02d85ec9215e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "OxUgDKKa7mCX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"yet, first of all, a truly learning problem. It's more an algorithmic problem, but still, it's an interesting problem nonetheless, because I just want to illustrate how it affects the way in which we calculate what might be trivial with small data becomes nontrivial with large data. So this is the problem. So given a set of items, capital n, which is large, and a, given a set of transactions m, which is again large, and given these two ratios between zero and one, find every pair x and y such that x implies y is a valid association. So we can break up this thing into two steps. I mean, we want to first check whether x implies y is worth looking at at all. So we first look at the support part. We want to know whether x, sorry, yeah, we want to know whether the count divided by m is bigger than the support, which is the same as taking this m to the other side and saying whether the count is bigger than a certain fraction of the total. So the first idea is to identify those sets whose\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"frequency? These are discussed in that book where I have given you the reference for the first lecture and for this lecture, so you can read about it. It's not particularly relevant to the work that we want to focus on in this course, so I won't get into it in any detail, but there is a very accessible introduction to that, in that chapter later on. So you can look at it and see a lot of it is some kind of heuristics, but you can look at it. So I'll stop with this. Any questions? Is there any way to predict how better our association rule is? So that is a general problem that we will come across. There is no real good way to predict how good it is. You can, of course, associate with a concrete data set how reliable it is for that data set. But this is going to be one of the challenges, as we will see going along, which is that you are always building rules or any model with respect to a fixed data set, and you are going to apply it to some other data set which you have not seen. So\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content='So we were looking at this market basket analysis, and towards the end of the discussion on association rule rules, if everybody who buys X is also likely to buy y. We said that one application of this association rule idea is to have these so called class association rules. So the right hand side was a category. We looked at an example with documents and we said, if a document has these words, it is also likely to belong to this category. So that brings us to the topic that is one of the central ones in machine learning, which is the idea of supervised learning. So in supervised learning, we have a set of items, and each item belongs to some category or class, and we distinguish these items in terms of some attributes. So you can think about this, as we will see shortly, you can think about these attributes as columns, right? So the attributes will be things like name, age, date of birth, whatever, income, stuff like that. So each row in the table will be one item, and there will be a', metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"against some unknown data. So, one other strategy where you don't want to, I mean, one of the other disadvantages of doing this is that maybe that there are some minority anomalies in this data, and the choice of test data that you have made might have hidden all these anomalies, so you never see them, and maybe it's important to see them. So there may be many situations, or maybe you just don't have enough data as a whole to build a good model by only looking at 80%. So another strategy is to systematically do this with different subsets. So what you're really asking at some higher level is that machine learning approach. So, remember, we have seen only decision trees, but we are going to see many models. So there are clearly many ways to build models. And the reason, whenever you see that there are many ways to do something, it's only because there is no guarantee that a given way is the best one. So more or less what you want to validate is, I have, say, seven different strategies\", metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"okay, maybe it is not really that any question about this. Okay, so now one of the problems with decision trees, which you will see later on, is also that a lot of the construction of the tree depends on these statistics. Because we said that we look at which attribute has the maximum discrepancy, whether it's entropy or genie index or whatever, which one of them will improve it the most. And that improvement, or lack of improvement really depends on the distribution of the different inputs at that point in my table. So if there is a borderline case, it could be that by shifting one value here or there, the attribute which has the highest improvement might shift from one to the other. And then I ask a different question. The moment I ask a different question, the tree becomes radically different because the question pattern changes and the split will change and so on. So here is to illustrate that what they have done is they have first, so y equal to one. Remember that the categories\", metadata={'source': 'audio_5.txt'}),\n",
              " Document(page_content=\"the order, sometimes we don't. Addresses, of course, are written in a million different ways. So there are all kinds of issues with just getting the data to a format where you can work on it. So this is an entirely different ballgame. It's much more kind of. There are lots of tools and techniques to deal with this, but that's not really going to be the focus of the course. So, in a sense, if you want to think about it, this data mining aspect will be almost missing from this course, even though it's part of the title. So what we are really going to look at is the machine learning aspect. So learning means you are trying to understand something that you don't know before. And machine learning suggests that it's done automatically. It's done by now machine as a computer. So there is an algorithm which learns something about, again, it's always with respect to data. So it's something about the data. So what we are trying to do is learn some kind of mathematical models from data. And this\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"what you're really looking for is correlations between sets of items across transactions, which we formalize in terms of these association groups. X implies y. And what we saw is that as such, combinatorially, this is a very complicated thing to calculate, given the sizes of the sets involved. But we can use this appropriate principle both to identify the sets which are interesting, those which are frequent enough to be considered, and also to decompose them into rules. So you use the approoriate twice, once to find the frequent z, and second to decompose each frequent z into possible x implies y. And using this, we can build our first simple supervised learning model. So these rules now become a very simple way of prediction. Now, if you give me a document, I will look at what words are there in the document in this previous example. And if it contains the word student and school, I will predict education. If it contains game and team, I'll predict sports. Now, of course, if it has\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content=\"it. So you could basically just take the majority, right? So supposing you do all this and you come to a situation where you have eight customers who have similar attributes, and five of them are given a loan and three are not given, then if you get a new case which looks like this, you will say, okay, five out of eight were given. So let me predict that the new case will be given, because that's a nature. So that's really the only sensible thing you can do in the absence of any other information. So that's typically what a decision three algorithm. So it'll ask question by question. The real we have to figure out is this part, which is how do you pick the next attribute to question. But once you pick an attribute, then the next step is automatic. It will filter out the rows according to the values attribute. And then you have to ask the same algorithm applied on each of those subtables, and these subtables be destroyed. And when you stop, you ask whether it's uniform or whether\", metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content='have to do a search through all possible trees. So instead we wanted to use some heuristic. So the main question that we have to ask is, which attribute do we query next at any stage. So our goal is to achieve this partition, which is pure, where all the values are yes or all the values are no. So we try to accelerate towards that. So that is this greedy strategy. So we use a heuristic, which will reduce the impurity as much as possible. And when we move from one node to its children by asking a question, what we do is we compute a weighted average. So for each node we can compute the impurity. But then when we combine the children of a node to get the impurity at the next level, we use a weighted average. So going simple weighted average just assigns a weight one to every item. So you just take the fraction. So for node one, you take the fraction of instances in that partition, multiply that fraction by the impurity and so on, and add it up, and we choose the one which reduces. So', metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"at the time of constructing the tree. So this is something that certainly should be what is the, but the main thing I wanted to emphasize is that modulo understanding the parameters and what you can do with it, you actually don't have to do any of the encoding that we discussed in terms of finding the impurity and then maximizing it all that is actually something that is taken care of by the library. So all that discussion that we had about how a decision tree is constructed, choosing the best split for the numerical values, then comparing the impurity gain for all the things, picking the best one and so on, all that is done automatically. So you really never have to implement that. So the reason that it's good to know how all these things work is because indirectly that impacts how these parameters that you are allowed to control will work. So you have to have a little bit of an idea about what's going on behind the algorithm in order to make best use of it when it is not producing\", metadata={'source': 'audio_5.txt'}),\n",
              " Document(page_content=\"You can do it in two different ways. So what I am saying is that I'm overlapping the thing, right? So I'm saying for each p, this is what I'm doing for each transaction, for every subset. But now I can basically change this and say for each, because obviously I don't have to look at z, which is not mentioned in this particular transaction. So instead of looking at every possible subset z, I can look for only those subsets which are there in that transaction, because everything else does not get incremented. Now, what you are asking is the other way around. And this is potentially, I mean, superficially, they are the same thing. You have two loops and you are interchanging the order. But this is potentially much more expensive because there will be a lot of items, subsets which never occur. I mean, for instance, you might be looking for subsets where people are buying two items which are totally of different types of categories, and they might never buy these together. Like for\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"its topic is education. And if it has game and team, then it is. So this is a special kind of association rule, where the right hand side of the rule is always a singleton, and it corresponds to a special type of item which is a category, in this case a topic. So when you have a table like this, the rows may have different number of entries. Or if you want to think about it as a table, then some columns may be blank. So not all columns are filled, but there's one last column which is filled. And now you want to now construct these rules corresponding to the first k minus one columns, implying the last column. And this now becomes something where these rules that you get out of this give you a kind of classifier. So this is a kind of a simplistic way of doing supervised learning. So that kind of brings me to the end of this basic treatment of classes, this market basket analysis. So what you're really looking for is correlations between sets of items across transactions, which we\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content=\"one of the things, byproducts of decision tree will be a kind of implicit importance, ranking among the attributes, which ones are more significant. So not all columns are equally important. So these are all things that have come up. So maybe when we come to it later on we'll discuss it. But these questions that you're asking, sir, when we form this decision tree, is it important or in some sense beneficial if we know the data? Insight of the data. Insight of the data, meaning what? As in the relevance of each column? Yes, normally that is, I mean, I cannot say it will always help. Normally it will help. But on the other hand, you have to assume that you do not have. So what you're really asking in a more general setting is that if a person who is kind of aware of the context, who's a specialist or a domain expert in this, so say somebody who has been associated with this, whatever, this bank, this particular bank, which is giving out loans, if they build the tree, they might make\", metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"is which value are we using to make the greedy choice. So that's the point. Yeah. So I guess linear will be better, even computationally. Right? Well, this kind of computation doesn't take much effort, right? Instead of comparing two values, you're squaring it and adding it. And so arithmetic operations hardly make any. See, log is a more expensive calculation. So that is the point. Take logs of small values. So logs of small values are also painful to calculate. So that's one reason you don't want to take logs. But otherwise, if I'm just doing multiplication and addition, it doesn't really cost much. Okay, so in the case of genie index, we are achieving a stopping criteria much earlier, right? That's the case, yeah. What we are saying here is that we are actually going to, we say that we are going to go until we either use up all the attributes or we reach a uniform class. Now, this is a reasonable thing to do if the number of attributes is small. But if you have a very large number\", metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"So remember, these x I's are actually not unknowns. These x I's known. So these x I's are what you get. The theta I's are the unknowns that you are trying to figure out. But in terms of the theta I's you can predict for a given xi. If you have computed some theta I's, then the answer is summation theta I x. So therefore the target that we have is clearly to find the best line. So let's assume we have a simple case of only one variable. So we have x one, x two, and so on. So we have n points. In reality, each of these x size is a vector of k, and the y, by convention, is always the label. So in the earlier thing, when we looked at that scikitlearn decision tree thing, we used capital x to denote the input matrix and y to denote the column matrix of output. So this is a normal standard convention in supervised learning that x's denote the inputs of the training data and y denotes the output label. In this case, it's not a label, but it's an output value. So in order to compute the best\", metadata={'source': 'audio_5.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reordered_docs = reordering.transform_documents(docs)\n",
        "\n",
        "# Confirm that the 4 relevant documents are at beginning and end.\n",
        "reordered_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n32e-h4Le5_J",
        "outputId": "63d4225e-3643-440f-8ba0-a74ea7afe53c"
      },
      "id": "n32e-h4Le5_J",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"yet, first of all, a truly learning problem. It's more an algorithmic problem, but still, it's an interesting problem nonetheless, because I just want to illustrate how it affects the way in which we calculate what might be trivial with small data becomes nontrivial with large data. So this is the problem. So given a set of items, capital n, which is large, and a, given a set of transactions m, which is again large, and given these two ratios between zero and one, find every pair x and y such that x implies y is a valid association. So we can break up this thing into two steps. I mean, we want to first check whether x implies y is worth looking at at all. So we first look at the support part. We want to know whether x, sorry, yeah, we want to know whether the count divided by m is bigger than the support, which is the same as taking this m to the other side and saying whether the count is bigger than a certain fraction of the total. So the first idea is to identify those sets whose\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content='So we were looking at this market basket analysis, and towards the end of the discussion on association rule rules, if everybody who buys X is also likely to buy y. We said that one application of this association rule idea is to have these so called class association rules. So the right hand side was a category. We looked at an example with documents and we said, if a document has these words, it is also likely to belong to this category. So that brings us to the topic that is one of the central ones in machine learning, which is the idea of supervised learning. So in supervised learning, we have a set of items, and each item belongs to some category or class, and we distinguish these items in terms of some attributes. So you can think about this, as we will see shortly, you can think about these attributes as columns, right? So the attributes will be things like name, age, date of birth, whatever, income, stuff like that. So each row in the table will be one item, and there will be a', metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"okay, maybe it is not really that any question about this. Okay, so now one of the problems with decision trees, which you will see later on, is also that a lot of the construction of the tree depends on these statistics. Because we said that we look at which attribute has the maximum discrepancy, whether it's entropy or genie index or whatever, which one of them will improve it the most. And that improvement, or lack of improvement really depends on the distribution of the different inputs at that point in my table. So if there is a borderline case, it could be that by shifting one value here or there, the attribute which has the highest improvement might shift from one to the other. And then I ask a different question. The moment I ask a different question, the tree becomes radically different because the question pattern changes and the split will change and so on. So here is to illustrate that what they have done is they have first, so y equal to one. Remember that the categories\", metadata={'source': 'audio_5.txt'}),\n",
              " Document(page_content=\"what you're really looking for is correlations between sets of items across transactions, which we formalize in terms of these association groups. X implies y. And what we saw is that as such, combinatorially, this is a very complicated thing to calculate, given the sizes of the sets involved. But we can use this appropriate principle both to identify the sets which are interesting, those which are frequent enough to be considered, and also to decompose them into rules. So you use the approoriate twice, once to find the frequent z, and second to decompose each frequent z into possible x implies y. And using this, we can build our first simple supervised learning model. So these rules now become a very simple way of prediction. Now, if you give me a document, I will look at what words are there in the document in this previous example. And if it contains the word student and school, I will predict education. If it contains game and team, I'll predict sports. Now, of course, if it has\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content='have to do a search through all possible trees. So instead we wanted to use some heuristic. So the main question that we have to ask is, which attribute do we query next at any stage. So our goal is to achieve this partition, which is pure, where all the values are yes or all the values are no. So we try to accelerate towards that. So that is this greedy strategy. So we use a heuristic, which will reduce the impurity as much as possible. And when we move from one node to its children by asking a question, what we do is we compute a weighted average. So for each node we can compute the impurity. But then when we combine the children of a node to get the impurity at the next level, we use a weighted average. So going simple weighted average just assigns a weight one to every item. So you just take the fraction. So for node one, you take the fraction of instances in that partition, multiply that fraction by the impurity and so on, and add it up, and we choose the one which reduces. So', metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"You can do it in two different ways. So what I am saying is that I'm overlapping the thing, right? So I'm saying for each p, this is what I'm doing for each transaction, for every subset. But now I can basically change this and say for each, because obviously I don't have to look at z, which is not mentioned in this particular transaction. So instead of looking at every possible subset z, I can look for only those subsets which are there in that transaction, because everything else does not get incremented. Now, what you are asking is the other way around. And this is potentially, I mean, superficially, they are the same thing. You have two loops and you are interchanging the order. But this is potentially much more expensive because there will be a lot of items, subsets which never occur. I mean, for instance, you might be looking for subsets where people are buying two items which are totally of different types of categories, and they might never buy these together. Like for\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"one of the things, byproducts of decision tree will be a kind of implicit importance, ranking among the attributes, which ones are more significant. So not all columns are equally important. So these are all things that have come up. So maybe when we come to it later on we'll discuss it. But these questions that you're asking, sir, when we form this decision tree, is it important or in some sense beneficial if we know the data? Insight of the data. Insight of the data, meaning what? As in the relevance of each column? Yes, normally that is, I mean, I cannot say it will always help. Normally it will help. But on the other hand, you have to assume that you do not have. So what you're really asking in a more general setting is that if a person who is kind of aware of the context, who's a specialist or a domain expert in this, so say somebody who has been associated with this, whatever, this bank, this particular bank, which is giving out loans, if they build the tree, they might make\", metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"So remember, these x I's are actually not unknowns. These x I's known. So these x I's are what you get. The theta I's are the unknowns that you are trying to figure out. But in terms of the theta I's you can predict for a given xi. If you have computed some theta I's, then the answer is summation theta I x. So therefore the target that we have is clearly to find the best line. So let's assume we have a simple case of only one variable. So we have x one, x two, and so on. So we have n points. In reality, each of these x size is a vector of k, and the y, by convention, is always the label. So in the earlier thing, when we looked at that scikitlearn decision tree thing, we used capital x to denote the input matrix and y to denote the column matrix of output. So this is a normal standard convention in supervised learning that x's denote the inputs of the training data and y denotes the output label. In this case, it's not a label, but it's an output value. So in order to compute the best\", metadata={'source': 'audio_5.txt'}),\n",
              " Document(page_content=\"is which value are we using to make the greedy choice. So that's the point. Yeah. So I guess linear will be better, even computationally. Right? Well, this kind of computation doesn't take much effort, right? Instead of comparing two values, you're squaring it and adding it. And so arithmetic operations hardly make any. See, log is a more expensive calculation. So that is the point. Take logs of small values. So logs of small values are also painful to calculate. So that's one reason you don't want to take logs. But otherwise, if I'm just doing multiplication and addition, it doesn't really cost much. Okay, so in the case of genie index, we are achieving a stopping criteria much earlier, right? That's the case, yeah. What we are saying here is that we are actually going to, we say that we are going to go until we either use up all the attributes or we reach a uniform class. Now, this is a reasonable thing to do if the number of attributes is small. But if you have a very large number\", metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"its topic is education. And if it has game and team, then it is. So this is a special kind of association rule, where the right hand side of the rule is always a singleton, and it corresponds to a special type of item which is a category, in this case a topic. So when you have a table like this, the rows may have different number of entries. Or if you want to think about it as a table, then some columns may be blank. So not all columns are filled, but there's one last column which is filled. And now you want to now construct these rules corresponding to the first k minus one columns, implying the last column. And this now becomes something where these rules that you get out of this give you a kind of classifier. So this is a kind of a simplistic way of doing supervised learning. So that kind of brings me to the end of this basic treatment of classes, this market basket analysis. So what you're really looking for is correlations between sets of items across transactions, which we\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content=\"at the time of constructing the tree. So this is something that certainly should be what is the, but the main thing I wanted to emphasize is that modulo understanding the parameters and what you can do with it, you actually don't have to do any of the encoding that we discussed in terms of finding the impurity and then maximizing it all that is actually something that is taken care of by the library. So all that discussion that we had about how a decision tree is constructed, choosing the best split for the numerical values, then comparing the impurity gain for all the things, picking the best one and so on, all that is done automatically. So you really never have to implement that. So the reason that it's good to know how all these things work is because indirectly that impacts how these parameters that you are allowed to control will work. So you have to have a little bit of an idea about what's going on behind the algorithm in order to make best use of it when it is not producing\", metadata={'source': 'audio_5.txt'}),\n",
              " Document(page_content=\"it. So you could basically just take the majority, right? So supposing you do all this and you come to a situation where you have eight customers who have similar attributes, and five of them are given a loan and three are not given, then if you get a new case which looks like this, you will say, okay, five out of eight were given. So let me predict that the new case will be given, because that's a nature. So that's really the only sensible thing you can do in the absence of any other information. So that's typically what a decision three algorithm. So it'll ask question by question. The real we have to figure out is this part, which is how do you pick the next attribute to question. But once you pick an attribute, then the next step is automatic. It will filter out the rows according to the values attribute. And then you have to ask the same algorithm applied on each of those subtables, and these subtables be destroyed. And when you stop, you ask whether it's uniform or whether\", metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"the order, sometimes we don't. Addresses, of course, are written in a million different ways. So there are all kinds of issues with just getting the data to a format where you can work on it. So this is an entirely different ballgame. It's much more kind of. There are lots of tools and techniques to deal with this, but that's not really going to be the focus of the course. So, in a sense, if you want to think about it, this data mining aspect will be almost missing from this course, even though it's part of the title. So what we are really going to look at is the machine learning aspect. So learning means you are trying to understand something that you don't know before. And machine learning suggests that it's done automatically. It's done by now machine as a computer. So there is an algorithm which learns something about, again, it's always with respect to data. So it's something about the data. So what we are trying to do is learn some kind of mathematical models from data. And this\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"against some unknown data. So, one other strategy where you don't want to, I mean, one of the other disadvantages of doing this is that maybe that there are some minority anomalies in this data, and the choice of test data that you have made might have hidden all these anomalies, so you never see them, and maybe it's important to see them. So there may be many situations, or maybe you just don't have enough data as a whole to build a good model by only looking at 80%. So another strategy is to systematically do this with different subsets. So what you're really asking at some higher level is that machine learning approach. So, remember, we have seen only decision trees, but we are going to see many models. So there are clearly many ways to build models. And the reason, whenever you see that there are many ways to do something, it's only because there is no guarantee that a given way is the best one. So more or less what you want to validate is, I have, say, seven different strategies\", metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"frequency? These are discussed in that book where I have given you the reference for the first lecture and for this lecture, so you can read about it. It's not particularly relevant to the work that we want to focus on in this course, so I won't get into it in any detail, but there is a very accessible introduction to that, in that chapter later on. So you can look at it and see a lot of it is some kind of heuristics, but you can look at it. So I'll stop with this. Any questions? Is there any way to predict how better our association rule is? So that is a general problem that we will come across. There is no real good way to predict how good it is. You can, of course, associate with a concrete data set how reliable it is for that data set. But this is going to be one of the challenges, as we will see going along, which is that you are always building rules or any model with respect to a fixed data set, and you are going to apply it to some other data set which you have not seen. So\", metadata={'source': 'audio_2.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_3=\" What distinguishes training data in supervised learning, and what is its purpose? \""
      ],
      "metadata": {
        "id": "Ej1ZXYLvfHVT"
      },
      "id": "Ej1ZXYLvfHVT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = lotr.get_relevant_documents(query_3)\n",
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAwH5DWcfltS",
        "outputId": "f26dcec8-e680-4c2d-bb86-eb9e807e20f2"
      },
      "id": "kAwH5DWcfltS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"a specification, but generating a concrete program from a generic program you can think of a model template as. So I just give you a line here which says a Y equal to Mx plus c. And here I want a specific m zero and C zero, which fits the given data in the best possible way, which is also part of the description of the algorithm. What does it mean for one line to be better than another line? So that's part of it. So the other side of this picture, as I mentioned, is when you don't have this training data, you don't have any examples that people have already labeled, but you are looking for some kind of patterns. So there is no guidance. So the thing about supervised learning is somebody should have gone through and given you this information before. Now, this information could have been collected over time. Maybe it's not manually collected, like the school marks. The school marks over time. Everybody has. I mean, the school keeps a record of what happens in the model exams, what\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"a fixed data set, and you are going to apply it to some other data set which you have not seen. So will that generalize well or not is the biggest question in machine learning. How do you know that the training data that you have used to build your model is actually representative of the data that you see in the real. And there could be many reasons why it is not valid. It could be because of sampling. So, for instance, you might be building a model for some consumer behavior by looking at one segment of. Maybe you are looking at the behavior of people who are in their, are building a model of how people behave, but your actual market consists of people who are maybe in their 40s, who have a very different way of dealing with things. So you need to be careful that the data that you're looking at is actually representative. So this is a huge problem and there's no correct answer. You can measure it, but you cannot. In some sense, if you could validate it, then this whole problem\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content='whatever, income, stuff like that. So each row in the table will be one item, and there will be a special column which indicates what category, depending on the classification problem that we are dealing with, what category it is. And the goal of supervised learning is to now construct, given a new row, a new item, a new combination of these attributes, which is not seen before, try to predict what would be the most appropriate category to assign. So these items which are given with the labels. So these are called labels. So these items are called training data. So we have labeled training data, which consists of some information which has been collected in the past, some historical information about items and their classification. And we want to find a model that generalizes the training data. So since normally this categorization corresponds to some kind of classification, this is typically called a classification problem. So classification problem could be a binary problem. It', metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"particular distribution, I should keep a similar ratio. So this is called stratified sampling. But these are sort of different things. But the good thing for us is that in the libraries that we will use, this is an automatic step. You just have to say, split the data into train and test, and it'll do. So, training set and test set. That's it. So this is a little bit, what should I say? Confusing, because we actually call the whole thing also training, and then we split it. And then we again call this training and test. Right? So training data is a little bit of an ambiguous term. So training data is both the entire data set that you are given to start with. And then it is the specific choice that you make about how to split this data into a large part, which you will use for building the model, and a small part that you will reserve for validating the model. So depending on how much data you have, the usual practice is about 20% to 30% of it you keep aside. So you use about 70% to 80%\", metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"and that is this. So this blue flower here where you can see the cursor just at this corner here where I'm rotating the cursor. So this has width 1.8. So that's what we're identifying. We're identifying among the training data. So the training data where y is equal to one, where the label class label was one among those points, which is the one where the second column of the input, the width is maximum. Right? So now what you do is you basically remove it. So you say that I want a training set in which I don't have 1.8, so I have either class is equal to two, y is equal to two, or I don't have 1.8. So basically this has all the inputs except that one. If there were more than one, all the ones which are of class one and 1.8 get eliminated. So I'm basically effectively removing this one data point from my training set. Understand what I'm doing? So I'm taking the old training set of 150 flowers, and I'm removing this one tree by identifying which is the widest. So remember that last\", metadata={'source': 'audio_5.txt'}),\n",
              " Document(page_content=\"the data. So what we are trying to do is learn some kind of mathematical models from data. And this falls into two broad categories. So the first category is where you are given examples. So you are told, for instance, this is, for example, supposing you're trying to evaluate whether a painting is painted by a particular type of. So there are these pools of painting, like impressionists and cubists and so on. So you might be given a lot of pictures saying, this is cubist, this is not cubist, this is impressionist, and so on. And then from these examples which are given to you with manual information, you are supposed to build a model which will take an unknown painting and tell you from its features which of these categories. So this is a typical supervised learning kind of thing. Unsupervised learning is something which is closer to the data mining situation where you have data and you are looking for patterns, but you don't really have a clear idea beforehand what patterns they are.\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"a slightly different presentation of the idea. Okay, so now, as I said, this whole thing so far has been largely a kind of algorithmic problem, right? In some sense, if I fix the set of items and the set of transactions and the set of the two thresholds, the support and the confidence threshold, then for any x, comma y, either it is true or it is not true. So there is a fixed answer. So the answer is known. So this is something where I can actually validate that you have either enumerated all the association rules or you have missed out something. So technically there is no learning involved. It's more an algorithmic question. So how does one connect this to learning? Well, in specific context, you can interpret these rules as classification rules. So, remember we said supervised learning says, if this is there, then it is a fraud. If this thing is not there, then it is not a fraud, and so on. So, let's look at a typical example. So, supposing we have some question which is about\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content=\"base our model on should be somehow indicative of what we are going to predict. If we are going to take data pertaining to one group of individuals and extrapolate it to a completely different profile of individuals, then it's unlikely that this model that we built has any relevance. So there is this fundamental assumption. So we are not going to make this assumption mathematically precise in this course. So, in the advanced machine learning course, we will talk about a kind of theory of learning in which you can make this thing rigorous. But we are going to assume that there is a correlation between the distribution of training examples and the distribution of unseen data. So these are probabilistic, of course. Even if you have a similar set of data, you could draw samples which omit a few cases and have a few other cases. So it's always going to be a kind of probabilistic argument. But what you're saying is that this is a representative sample. So you're not saying that the examples\", metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"classification model is that this correct answer is not known. I mean, if we knew the answer, if we knew a better way to come up with the correct answer, we would not build this model. So the reason we are building this model is we don't have a reliable way to compute the correct answer. So in this situation, when we do not have a reference value to compare our answers with, how do we measure whether or not our model is a good one? So on what basis can we evaluate? So what we need is a comparison where we know the answers. But the only situation where we know the answers is our training data. Our training data comes to us with labels. So for each input in our training data, we know the manually or however classified correct answer. So we are assuming those are the correct answers because otherwise our model building process will not make sense. So assuming that those are correct answers, we are building our model. So that's the only source of inputs and outputs where we know the\", metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"So remember, these x I's are actually not unknowns. These x I's known. So these x I's are what you get. The theta I's are the unknowns that you are trying to figure out. But in terms of the theta I's you can predict for a given xi. If you have computed some theta I's, then the answer is summation theta I x. So therefore the target that we have is clearly to find the best line. So let's assume we have a simple case of only one variable. So we have x one, x two, and so on. So we have n points. In reality, each of these x size is a vector of k, and the y, by convention, is always the label. So in the earlier thing, when we looked at that scikitlearn decision tree thing, we used capital x to denote the input matrix and y to denote the column matrix of output. So this is a normal standard convention in supervised learning that x's denote the inputs of the training data and y denotes the output label. In this case, it's not a label, but it's an output value. So in order to compute the best\", metadata={'source': 'audio_5.txt'}),\n",
              " Document(page_content=\"time. Everybody has. I mean, the school keeps a record of what happens in the model exams, what happens in the board exam. So it comes automatically. The bank will be keeping records of all the loans that it bought and what happened to the customers who bought loans. So this historical information is accumulated over time. On the other hand, in some situations, like the example I mentioned about classifying paintings, now, when you go into a museum or you go into some art gallery or something, the painting doesn't have a label on it, saying, this is a painting of this type and this is a painting of that type. So somebody, if you want to fit this training model to it, somebody has to sit and actually label these things for the algorithm. So there is a lot of work in it. So generating a lot of valid training data is itself a kind of computationally intensive task. How do you provide a lot of well trained examples? Because the assumption is that the labels that these training samples\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"its topic is education. And if it has game and team, then it is. So this is a special kind of association rule, where the right hand side of the rule is always a singleton, and it corresponds to a special type of item which is a category, in this case a topic. So when you have a table like this, the rows may have different number of entries. Or if you want to think about it as a table, then some columns may be blank. So not all columns are filled, but there's one last column which is filled. And now you want to now construct these rules corresponding to the first k minus one columns, implying the last column. And this now becomes something where these rules that you get out of this give you a kind of classifier. So this is a kind of a simplistic way of doing supervised learning. So that kind of brings me to the end of this basic treatment of classes, this market basket analysis. So what you're really looking for is correlations between sets of items across transactions, which we\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content=\"a dog picture from a cat picture, or one which is neither of the two, it's not very easy to quantify that. And there the attributes in a picture are typically just if you think of a picture as a digital picture, it's a bunch of pixels and their colors and their arrangement, their relative arrangement. So even I'm saying, even if you have some domain knowledge, it's not very clear that the person's domain knowledge is aligned to the way the data is available. So what we are going to be doing is agnostic to the domain. So we are not going to believe anything or assume anything about the domain, except that we will want to validate at the end somehow that what we are doing is right. But in practice what you're saying makes sense that really to do supervised learning, it's much more realistic to do it in collaboration with somebody who understands a domain rather than just doing it blindly. Because also when you do it blindly, you might come up with solutions which make no sense. You\", metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"gain by the scattering of that attribute. So if we have high gain divided by high scattering, then the net normalized effect is lower. So you might choose an attribute which actually gives you a lower information gain overall, because the attribute itself is less static. And that's the thing that we want to achieve. Just to emphasize that when you have a machine learning algorithm, see generally, what is a machine learning algorithm doing? It's trying to describe the shape of the model based on the training data. So in this case it's literally the shape, because you have many possible trees you could construct, and the actual tree you construct depends on the data. And because you don't have any preconceptions about the data, you have to make sure that if there are these kind of patterns which disturb your algorithm, you will be able to counteract. So that's the purpose of this information gain ratio. It's not just the information gain alone, which is what we had initially. It's\", metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"to show you some code before we come back to theory. So, is there any further questions? So, at the moment, I'm not expecting you to do anything with this code other than to understand that a, it is relatively painless to invoke a model in scikitlearn. And if you have this kind of visualization code, either you write it yourself or somebody gives it to you. You can then interpret your models visually and then experiment with it by changing things and seeing what happens. And that gives you a fair amount of insight both into your data and into how the model is working. So it gives you some indications about the limitations of that particular. Okay, so now I'm going to do something hopefully not unfamiliar to you. Right? At the beginning, when we talked about supervised learning, we said that we are trying to predict something, and those predictions can be in two types. So one type is what we have seen in the decision tree, which is a classification. So the predicted value is one from a\", metadata={'source': 'audio_5.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reordered_docs = reordering.transform_documents(docs)\n",
        "\n",
        "# Confirm that the 4 relevant documents are at beginning and end.\n",
        "reordered_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLl3Va0DfsI_",
        "outputId": "a492f588-704b-4084-aa72-034ed0a56ba4"
      },
      "id": "YLl3Va0DfsI_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"a specification, but generating a concrete program from a generic program you can think of a model template as. So I just give you a line here which says a Y equal to Mx plus c. And here I want a specific m zero and C zero, which fits the given data in the best possible way, which is also part of the description of the algorithm. What does it mean for one line to be better than another line? So that's part of it. So the other side of this picture, as I mentioned, is when you don't have this training data, you don't have any examples that people have already labeled, but you are looking for some kind of patterns. So there is no guidance. So the thing about supervised learning is somebody should have gone through and given you this information before. Now, this information could have been collected over time. Maybe it's not manually collected, like the school marks. The school marks over time. Everybody has. I mean, the school keeps a record of what happens in the model exams, what\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content='whatever, income, stuff like that. So each row in the table will be one item, and there will be a special column which indicates what category, depending on the classification problem that we are dealing with, what category it is. And the goal of supervised learning is to now construct, given a new row, a new item, a new combination of these attributes, which is not seen before, try to predict what would be the most appropriate category to assign. So these items which are given with the labels. So these are called labels. So these items are called training data. So we have labeled training data, which consists of some information which has been collected in the past, some historical information about items and their classification. And we want to find a model that generalizes the training data. So since normally this categorization corresponds to some kind of classification, this is typically called a classification problem. So classification problem could be a binary problem. It', metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"and that is this. So this blue flower here where you can see the cursor just at this corner here where I'm rotating the cursor. So this has width 1.8. So that's what we're identifying. We're identifying among the training data. So the training data where y is equal to one, where the label class label was one among those points, which is the one where the second column of the input, the width is maximum. Right? So now what you do is you basically remove it. So you say that I want a training set in which I don't have 1.8, so I have either class is equal to two, y is equal to two, or I don't have 1.8. So basically this has all the inputs except that one. If there were more than one, all the ones which are of class one and 1.8 get eliminated. So I'm basically effectively removing this one data point from my training set. Understand what I'm doing? So I'm taking the old training set of 150 flowers, and I'm removing this one tree by identifying which is the widest. So remember that last\", metadata={'source': 'audio_5.txt'}),\n",
              " Document(page_content=\"a slightly different presentation of the idea. Okay, so now, as I said, this whole thing so far has been largely a kind of algorithmic problem, right? In some sense, if I fix the set of items and the set of transactions and the set of the two thresholds, the support and the confidence threshold, then for any x, comma y, either it is true or it is not true. So there is a fixed answer. So the answer is known. So this is something where I can actually validate that you have either enumerated all the association rules or you have missed out something. So technically there is no learning involved. It's more an algorithmic question. So how does one connect this to learning? Well, in specific context, you can interpret these rules as classification rules. So, remember we said supervised learning says, if this is there, then it is a fraud. If this thing is not there, then it is not a fraud, and so on. So, let's look at a typical example. So, supposing we have some question which is about\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content=\"classification model is that this correct answer is not known. I mean, if we knew the answer, if we knew a better way to come up with the correct answer, we would not build this model. So the reason we are building this model is we don't have a reliable way to compute the correct answer. So in this situation, when we do not have a reference value to compare our answers with, how do we measure whether or not our model is a good one? So on what basis can we evaluate? So what we need is a comparison where we know the answers. But the only situation where we know the answers is our training data. Our training data comes to us with labels. So for each input in our training data, we know the manually or however classified correct answer. So we are assuming those are the correct answers because otherwise our model building process will not make sense. So assuming that those are correct answers, we are building our model. So that's the only source of inputs and outputs where we know the\", metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"time. Everybody has. I mean, the school keeps a record of what happens in the model exams, what happens in the board exam. So it comes automatically. The bank will be keeping records of all the loans that it bought and what happened to the customers who bought loans. So this historical information is accumulated over time. On the other hand, in some situations, like the example I mentioned about classifying paintings, now, when you go into a museum or you go into some art gallery or something, the painting doesn't have a label on it, saying, this is a painting of this type and this is a painting of that type. So somebody, if you want to fit this training model to it, somebody has to sit and actually label these things for the algorithm. So there is a lot of work in it. So generating a lot of valid training data is itself a kind of computationally intensive task. How do you provide a lot of well trained examples? Because the assumption is that the labels that these training samples\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"a dog picture from a cat picture, or one which is neither of the two, it's not very easy to quantify that. And there the attributes in a picture are typically just if you think of a picture as a digital picture, it's a bunch of pixels and their colors and their arrangement, their relative arrangement. So even I'm saying, even if you have some domain knowledge, it's not very clear that the person's domain knowledge is aligned to the way the data is available. So what we are going to be doing is agnostic to the domain. So we are not going to believe anything or assume anything about the domain, except that we will want to validate at the end somehow that what we are doing is right. But in practice what you're saying makes sense that really to do supervised learning, it's much more realistic to do it in collaboration with somebody who understands a domain rather than just doing it blindly. Because also when you do it blindly, you might come up with solutions which make no sense. You\", metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"to show you some code before we come back to theory. So, is there any further questions? So, at the moment, I'm not expecting you to do anything with this code other than to understand that a, it is relatively painless to invoke a model in scikitlearn. And if you have this kind of visualization code, either you write it yourself or somebody gives it to you. You can then interpret your models visually and then experiment with it by changing things and seeing what happens. And that gives you a fair amount of insight both into your data and into how the model is working. So it gives you some indications about the limitations of that particular. Okay, so now I'm going to do something hopefully not unfamiliar to you. Right? At the beginning, when we talked about supervised learning, we said that we are trying to predict something, and those predictions can be in two types. So one type is what we have seen in the decision tree, which is a classification. So the predicted value is one from a\", metadata={'source': 'audio_5.txt'}),\n",
              " Document(page_content=\"gain by the scattering of that attribute. So if we have high gain divided by high scattering, then the net normalized effect is lower. So you might choose an attribute which actually gives you a lower information gain overall, because the attribute itself is less static. And that's the thing that we want to achieve. Just to emphasize that when you have a machine learning algorithm, see generally, what is a machine learning algorithm doing? It's trying to describe the shape of the model based on the training data. So in this case it's literally the shape, because you have many possible trees you could construct, and the actual tree you construct depends on the data. And because you don't have any preconceptions about the data, you have to make sure that if there are these kind of patterns which disturb your algorithm, you will be able to counteract. So that's the purpose of this information gain ratio. It's not just the information gain alone, which is what we had initially. It's\", metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"its topic is education. And if it has game and team, then it is. So this is a special kind of association rule, where the right hand side of the rule is always a singleton, and it corresponds to a special type of item which is a category, in this case a topic. So when you have a table like this, the rows may have different number of entries. Or if you want to think about it as a table, then some columns may be blank. So not all columns are filled, but there's one last column which is filled. And now you want to now construct these rules corresponding to the first k minus one columns, implying the last column. And this now becomes something where these rules that you get out of this give you a kind of classifier. So this is a kind of a simplistic way of doing supervised learning. So that kind of brings me to the end of this basic treatment of classes, this market basket analysis. So what you're really looking for is correlations between sets of items across transactions, which we\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content=\"So remember, these x I's are actually not unknowns. These x I's known. So these x I's are what you get. The theta I's are the unknowns that you are trying to figure out. But in terms of the theta I's you can predict for a given xi. If you have computed some theta I's, then the answer is summation theta I x. So therefore the target that we have is clearly to find the best line. So let's assume we have a simple case of only one variable. So we have x one, x two, and so on. So we have n points. In reality, each of these x size is a vector of k, and the y, by convention, is always the label. So in the earlier thing, when we looked at that scikitlearn decision tree thing, we used capital x to denote the input matrix and y to denote the column matrix of output. So this is a normal standard convention in supervised learning that x's denote the inputs of the training data and y denotes the output label. In this case, it's not a label, but it's an output value. So in order to compute the best\", metadata={'source': 'audio_5.txt'}),\n",
              " Document(page_content=\"base our model on should be somehow indicative of what we are going to predict. If we are going to take data pertaining to one group of individuals and extrapolate it to a completely different profile of individuals, then it's unlikely that this model that we built has any relevance. So there is this fundamental assumption. So we are not going to make this assumption mathematically precise in this course. So, in the advanced machine learning course, we will talk about a kind of theory of learning in which you can make this thing rigorous. But we are going to assume that there is a correlation between the distribution of training examples and the distribution of unseen data. So these are probabilistic, of course. Even if you have a similar set of data, you could draw samples which omit a few cases and have a few other cases. So it's always going to be a kind of probabilistic argument. But what you're saying is that this is a representative sample. So you're not saying that the examples\", metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"the data. So what we are trying to do is learn some kind of mathematical models from data. And this falls into two broad categories. So the first category is where you are given examples. So you are told, for instance, this is, for example, supposing you're trying to evaluate whether a painting is painted by a particular type of. So there are these pools of painting, like impressionists and cubists and so on. So you might be given a lot of pictures saying, this is cubist, this is not cubist, this is impressionist, and so on. And then from these examples which are given to you with manual information, you are supposed to build a model which will take an unknown painting and tell you from its features which of these categories. So this is a typical supervised learning kind of thing. Unsupervised learning is something which is closer to the data mining situation where you have data and you are looking for patterns, but you don't really have a clear idea beforehand what patterns they are.\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"particular distribution, I should keep a similar ratio. So this is called stratified sampling. But these are sort of different things. But the good thing for us is that in the libraries that we will use, this is an automatic step. You just have to say, split the data into train and test, and it'll do. So, training set and test set. That's it. So this is a little bit, what should I say? Confusing, because we actually call the whole thing also training, and then we split it. And then we again call this training and test. Right? So training data is a little bit of an ambiguous term. So training data is both the entire data set that you are given to start with. And then it is the specific choice that you make about how to split this data into a large part, which you will use for building the model, and a small part that you will reserve for validating the model. So depending on how much data you have, the usual practice is about 20% to 30% of it you keep aside. So you use about 70% to 80%\", metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"a fixed data set, and you are going to apply it to some other data set which you have not seen. So will that generalize well or not is the biggest question in machine learning. How do you know that the training data that you have used to build your model is actually representative of the data that you see in the real. And there could be many reasons why it is not valid. It could be because of sampling. So, for instance, you might be building a model for some consumer behavior by looking at one segment of. Maybe you are looking at the behavior of people who are in their, are building a model of how people behave, but your actual market consists of people who are maybe in their 40s, who have a very different way of dealing with things. So you need to be careful that the data that you're looking at is actually representative. So this is a huge problem and there's no correct answer. You can measure it, but you cannot. In some sense, if you could validate it, then this whole problem\", metadata={'source': 'audio_2.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_4=\" What is cross-validation?\""
      ],
      "metadata": {
        "id": "VVYvl3-cf_gg"
      },
      "id": "VVYvl3-cf_gg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = lotr.get_relevant_documents(query_4)\n",
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_0g7VEogTz4",
        "outputId": "cb9b07c5-5b2b-4d1e-a20d-9ef97a6a6b31"
      },
      "id": "6_0g7VEogTz4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"But there may be more subtle things. Somebody might be talking about events, some kind of pop music events, and another person may be talking about some spiritual gatherings. And one person may be interested in pop music events, and another person may not be. And another person may be interested in spiritual gatherings, and the first person may not be. So then you can train these things. So in that sense, which words signal junk and which words don't signal junk is also a parameter of the model. Just like in this linear fit, the shape of the line is a parameter. So this is the learning part. So what we are going to do is look at different types of models, as I said, and then we will look at this parameter adjustment. How does the data actually determine the model, the concrete model? How do we build the best model that we can get for the given data? That's the algorithm. So we have a kind of model template. So we have a model template on this side, we have training data, and what we\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"a fixed data set, and you are going to apply it to some other data set which you have not seen. So will that generalize well or not is the biggest question in machine learning. How do you know that the training data that you have used to build your model is actually representative of the data that you see in the real. And there could be many reasons why it is not valid. It could be because of sampling. So, for instance, you might be building a model for some consumer behavior by looking at one segment of. Maybe you are looking at the behavior of people who are in their, are building a model of how people behave, but your actual market consists of people who are maybe in their 40s, who have a very different way of dealing with things. So you need to be careful that the data that you're looking at is actually representative. So this is a huge problem and there's no correct answer. You can measure it, but you cannot. In some sense, if you could validate it, then this whole problem\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content=\"what we are going to test it on are two different things. So we would like a model which does well with respect to the training data. So if you think about it as a situation where you want to minimize the errors, some kind of quantity, right? So you have a model which makes some predictions with respect to your training data, and you have the actual training data answers. So in a way, you would like to make these match as closely as possible. So it's a kind of optimization problem. You want to minimize the discrepancy between what your model does and what your training data tells you is the true value. So this is all possible for the training data because we know the answers. But when I move to a situation where I'm predicting the answers, the whole situation changes. Because if I could tell you what the right answers were, I wouldn't have to use this approach. So if I had an algorithmic way of telling you that this person should get a loan and that person should not get a loan, then\", metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"distribution changes slightly, one attribute might get better entropy than another attribute. And you will choose that question first. So the shape of the tree might vary a lot between these, so you may not get the same. Okay, so the question about cross validation, cross validation. What I'm saying is that you do not make a hard and fast choice once and for all to split your data this way. So you're taking your data and you're saying, hey, first let me leave out this 20% and build one model. So I am building multiple models. I'm building m one, m two, m three, m four, and m five. Each of these models is built using 80% of the test data, but a different 80%. So each of them leaves out some 20%. So I'm building five different models and comparing what I see in all these five. So the first thing is that if this approach is good, then in general, these five models should behave in a similar way with respect to the outcome. So if they all behave similarly, then I can decide what to do. So\", metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content='decision. One reason that this is important is remember that we talked about this cross validation. So what do we do in cross validation or even in normal validation? In normal validation or in cross validation. In either case, you pull out some random data, set data points from your input and build a tree on the rest. Now the question is, the model that you built from the rest, is it a stable structure in the sense that is it sensitive to how you chose your training and test data? And this example suggests that you are likely to get different models from different subsets of training data. So you then have to worry about what is your final model going to be. So if you test it on a particular test set, then maybe if you test it on a different test set, you might get an equivalent model, but it might have a very different structure, and maybe for other reasons, that structure might be more natural. So this is a generic problem with this kind of tree model, that it is very sensitive to', metadata={'source': 'audio_5.txt'}),\n",
              " Document(page_content=\"a specification, but generating a concrete program from a generic program you can think of a model template as. So I just give you a line here which says a Y equal to Mx plus c. And here I want a specific m zero and C zero, which fits the given data in the best possible way, which is also part of the description of the algorithm. What does it mean for one line to be better than another line? So that's part of it. So the other side of this picture, as I mentioned, is when you don't have this training data, you don't have any examples that people have already labeled, but you are looking for some kind of patterns. So there is no guidance. So the thing about supervised learning is somebody should have gone through and given you this information before. Now, this information could have been collected over time. Maybe it's not manually collected, like the school marks. The school marks over time. Everybody has. I mean, the school keeps a record of what happens in the model exams, what\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"can measure it, but you cannot. In some sense, if you could validate it, then this whole problem becomes much simpler. But the whole difficulty with machine learning is exactly this, that there is no sensible way to predict the behavior or even when it will fail. Even if you could say that this is likely to work well here and not likely to work well there that itself would be a huge improvement, not even necessarily a very precise quantification, but just some indication. But all these are very speculative. So that's a huge challenge. So there are some theoretical things you can say which we may not, may or may not get to in this course. But it's a big problem. This is the so called generalization problem. So that's what basically, if you build association rules, is it going to hold or not? I don't know. Okay, sir, I had a question. We have been talking about items and transactions, but we haven't anywhere spoken about whether one customer, suppose if you're talking about somebody\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content=\"if given this input, will give you that corresponding output. So there is a correct output that you expect for every input, and then you can validate whether or not the program does it by feeding it different inputs of different types and seeing that it gives the expected output on a. But here the problem is that this expected output is not something that you know how to compute, and yet you need to make some statement, obviously, as to whether the model is good or bad, because if you can't validate that your model is doing well, you have no basis to use it. So this will be another issue that we will take up. How do we deal with this problem of evaluating performance when actually we don't have a yardstick measuring standard in some sense? So please stop me at any time. So if you think something, I'm going to get into a specific module very soon. So what we are going to look at are different. Start with very simple models. So what I'm going to do today is something called a decision\", metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"the best one. So more or less what you want to validate is, I have, say, seven different strategies to solve this problem using machine learning. And I want to check which one of them is good. So I need to compare. So once I compare, then I'm stuck with whatever I have. It may be that this gives me only 70% accuracy or some such metric, but the other models give me even less. So it's more really a comparative statement to start with, which is, is this method actually giving me something reasonable? And is it giving me better something than something else? So, for this one strategy is so called cross validation. So what you do is you take some fraction, say, in this case, it is one fifth. So 20%. So you take your data and you leave out. So you randomize it into five groups, which preserve, in some sense, the random behavior, so they partition it five ways. So if I leave out any one of these groups, it's as though I have taken the remaining 80% for training and this 20% for test. But\", metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"So we have this horizontal line and we have this one blue thing which is above the horizontal. So what if you remove the one or two of the bottom green things which come below the line, these are also wrong. Right? So the blue thing above the line was wrong in some sense. The green triangles below the line are also wrong. So instead of removing the widest blue, you can try to remove the narrowest green or the two narrowest greens or something and see what happens. So you just have to copy paste this code and just change the condition and all the visualization. You can just run it as it is and see, I have not done it, but I think you might get something different. So the predict outcome is not predictable. I cannot guarantee you that one or the other will produce a different output, but it can happen. And this is generally something which one has to be mindful of when you're dealing with decision. One reason that this is important is remember that we talked about this cross validation.\", metadata={'source': 'audio_5.txt'}),\n",
              " Document(page_content=\"these values in that example. So unless you specify that, you cannot say that this is unlikely to be true or this is likely to be true, because it will be specific to the interpretation. But the problem now, we have now reduced it to one which is uninterpreted. We are just talking about abstract items. So there was a question in the chat about dimensionality reduction connected to PCA. So, yes, the PCA will be one part of it, but there are also other things that we will see. Sir, I have a question. Yeah. When we model this problem in this way, we didn't capture the frequency of items in a transaction. We were just looking at whether or not that item is present or not. Yeah, but I think for most of these examples, whether someone's buying from a shop or whether it's words in a document, that frequency will be a part of it. Right. Which we are not capturing here. Yeah. So usually this is how mathematical models work. You start with the most basic model, and then you add any features\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"its topic is education. And if it has game and team, then it is. So this is a special kind of association rule, where the right hand side of the rule is always a singleton, and it corresponds to a special type of item which is a category, in this case a topic. So when you have a table like this, the rows may have different number of entries. Or if you want to think about it as a table, then some columns may be blank. So not all columns are filled, but there's one last column which is filled. And now you want to now construct these rules corresponding to the first k minus one columns, implying the last column. And this now becomes something where these rules that you get out of this give you a kind of classifier. So this is a kind of a simplistic way of doing supervised learning. So that kind of brings me to the end of this basic treatment of classes, this market basket analysis. So what you're really looking for is correlations between sets of items across transactions, which we\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content=\"what you're saying is that this is a representative sample. So you're not saying that the examples have every possible situation that you are likely to use for judgment, but you're really talking at the level of probabilities. You're saying that the probability distribution of the training is somehow identical to that of the unseen data on which you are going to make the prediction. So this is going to be your training and this is going to be your prediction. So this is a fundamental assumption. If this assumption does not hold, then whatever you try to do doesn't really have any basis. So this is something that we have to assume, and we will talk about later on what this assumption means. Maybe not in this course, but in later courses. But for now, let us assume this. So the next question is, what does it mean to learn? Right? So, as we saw in this loan data set, 60% of the loans were granted, so nine by 15. So if I were to just say yes, supposing I don't have an interesting model\", metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content='we had, which is, how do we query the effectiveness of our model? So how do we validate whether the model we have built is right or not? So what I had mentioned earlier is that there is a fundamental difference between normal, say test software validation and this machine learning model validation, because when we have some software, we write some code in a traditional setting, we have an expectation, we know something about how the inputs should be mapped to the outputs. So we can create this kind of a test suite which maybe checks the boundary conditions, as they are called. Some extreme cases where you want to make sure that the software is doing the right thing, and then we compare the output with what we know should be the output. Right? So we take the program output and compare it with the expected answer. So we have a notion of a correct answer to compare. So the problem with the classification model is that this correct answer is not known. I mean, if we knew the answer, if we', metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"So remember, these x I's are actually not unknowns. These x I's known. So these x I's are what you get. The theta I's are the unknowns that you are trying to figure out. But in terms of the theta I's you can predict for a given xi. If you have computed some theta I's, then the answer is summation theta I x. So therefore the target that we have is clearly to find the best line. So let's assume we have a simple case of only one variable. So we have x one, x two, and so on. So we have n points. In reality, each of these x size is a vector of k, and the y, by convention, is always the label. So in the earlier thing, when we looked at that scikitlearn decision tree thing, we used capital x to denote the input matrix and y to denote the column matrix of output. So this is a normal standard convention in supervised learning that x's denote the inputs of the training data and y denotes the output label. In this case, it's not a label, but it's an output value. So in order to compute the best\", metadata={'source': 'audio_5.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reordered_docs = reordering.transform_documents(docs)\n",
        "\n",
        "# Confirm that the 4 relevant documents are at beginning and end.\n",
        "reordered_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Mi9A5YLgrxI",
        "outputId": "fad14fef-5e98-4aae-cf36-3a4ed06c347e"
      },
      "id": "7Mi9A5YLgrxI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"But there may be more subtle things. Somebody might be talking about events, some kind of pop music events, and another person may be talking about some spiritual gatherings. And one person may be interested in pop music events, and another person may not be. And another person may be interested in spiritual gatherings, and the first person may not be. So then you can train these things. So in that sense, which words signal junk and which words don't signal junk is also a parameter of the model. Just like in this linear fit, the shape of the line is a parameter. So this is the learning part. So what we are going to do is look at different types of models, as I said, and then we will look at this parameter adjustment. How does the data actually determine the model, the concrete model? How do we build the best model that we can get for the given data? That's the algorithm. So we have a kind of model template. So we have a model template on this side, we have training data, and what we\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"what we are going to test it on are two different things. So we would like a model which does well with respect to the training data. So if you think about it as a situation where you want to minimize the errors, some kind of quantity, right? So you have a model which makes some predictions with respect to your training data, and you have the actual training data answers. So in a way, you would like to make these match as closely as possible. So it's a kind of optimization problem. You want to minimize the discrepancy between what your model does and what your training data tells you is the true value. So this is all possible for the training data because we know the answers. But when I move to a situation where I'm predicting the answers, the whole situation changes. Because if I could tell you what the right answers were, I wouldn't have to use this approach. So if I had an algorithmic way of telling you that this person should get a loan and that person should not get a loan, then\", metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content='decision. One reason that this is important is remember that we talked about this cross validation. So what do we do in cross validation or even in normal validation? In normal validation or in cross validation. In either case, you pull out some random data, set data points from your input and build a tree on the rest. Now the question is, the model that you built from the rest, is it a stable structure in the sense that is it sensitive to how you chose your training and test data? And this example suggests that you are likely to get different models from different subsets of training data. So you then have to worry about what is your final model going to be. So if you test it on a particular test set, then maybe if you test it on a different test set, you might get an equivalent model, but it might have a very different structure, and maybe for other reasons, that structure might be more natural. So this is a generic problem with this kind of tree model, that it is very sensitive to', metadata={'source': 'audio_5.txt'}),\n",
              " Document(page_content=\"can measure it, but you cannot. In some sense, if you could validate it, then this whole problem becomes much simpler. But the whole difficulty with machine learning is exactly this, that there is no sensible way to predict the behavior or even when it will fail. Even if you could say that this is likely to work well here and not likely to work well there that itself would be a huge improvement, not even necessarily a very precise quantification, but just some indication. But all these are very speculative. So that's a huge challenge. So there are some theoretical things you can say which we may not, may or may not get to in this course. But it's a big problem. This is the so called generalization problem. So that's what basically, if you build association rules, is it going to hold or not? I don't know. Okay, sir, I had a question. We have been talking about items and transactions, but we haven't anywhere spoken about whether one customer, suppose if you're talking about somebody\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content=\"the best one. So more or less what you want to validate is, I have, say, seven different strategies to solve this problem using machine learning. And I want to check which one of them is good. So I need to compare. So once I compare, then I'm stuck with whatever I have. It may be that this gives me only 70% accuracy or some such metric, but the other models give me even less. So it's more really a comparative statement to start with, which is, is this method actually giving me something reasonable? And is it giving me better something than something else? So, for this one strategy is so called cross validation. So what you do is you take some fraction, say, in this case, it is one fifth. So 20%. So you take your data and you leave out. So you randomize it into five groups, which preserve, in some sense, the random behavior, so they partition it five ways. So if I leave out any one of these groups, it's as though I have taken the remaining 80% for training and this 20% for test. But\", metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"these values in that example. So unless you specify that, you cannot say that this is unlikely to be true or this is likely to be true, because it will be specific to the interpretation. But the problem now, we have now reduced it to one which is uninterpreted. We are just talking about abstract items. So there was a question in the chat about dimensionality reduction connected to PCA. So, yes, the PCA will be one part of it, but there are also other things that we will see. Sir, I have a question. Yeah. When we model this problem in this way, we didn't capture the frequency of items in a transaction. We were just looking at whether or not that item is present or not. Yeah, but I think for most of these examples, whether someone's buying from a shop or whether it's words in a document, that frequency will be a part of it. Right. Which we are not capturing here. Yeah. So usually this is how mathematical models work. You start with the most basic model, and then you add any features\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"what you're saying is that this is a representative sample. So you're not saying that the examples have every possible situation that you are likely to use for judgment, but you're really talking at the level of probabilities. You're saying that the probability distribution of the training is somehow identical to that of the unseen data on which you are going to make the prediction. So this is going to be your training and this is going to be your prediction. So this is a fundamental assumption. If this assumption does not hold, then whatever you try to do doesn't really have any basis. So this is something that we have to assume, and we will talk about later on what this assumption means. Maybe not in this course, but in later courses. But for now, let us assume this. So the next question is, what does it mean to learn? Right? So, as we saw in this loan data set, 60% of the loans were granted, so nine by 15. So if I were to just say yes, supposing I don't have an interesting model\", metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"So remember, these x I's are actually not unknowns. These x I's known. So these x I's are what you get. The theta I's are the unknowns that you are trying to figure out. But in terms of the theta I's you can predict for a given xi. If you have computed some theta I's, then the answer is summation theta I x. So therefore the target that we have is clearly to find the best line. So let's assume we have a simple case of only one variable. So we have x one, x two, and so on. So we have n points. In reality, each of these x size is a vector of k, and the y, by convention, is always the label. So in the earlier thing, when we looked at that scikitlearn decision tree thing, we used capital x to denote the input matrix and y to denote the column matrix of output. So this is a normal standard convention in supervised learning that x's denote the inputs of the training data and y denotes the output label. In this case, it's not a label, but it's an output value. So in order to compute the best\", metadata={'source': 'audio_5.txt'}),\n",
              " Document(page_content='we had, which is, how do we query the effectiveness of our model? So how do we validate whether the model we have built is right or not? So what I had mentioned earlier is that there is a fundamental difference between normal, say test software validation and this machine learning model validation, because when we have some software, we write some code in a traditional setting, we have an expectation, we know something about how the inputs should be mapped to the outputs. So we can create this kind of a test suite which maybe checks the boundary conditions, as they are called. Some extreme cases where you want to make sure that the software is doing the right thing, and then we compare the output with what we know should be the output. Right? So we take the program output and compare it with the expected answer. So we have a notion of a correct answer to compare. So the problem with the classification model is that this correct answer is not known. I mean, if we knew the answer, if we', metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"its topic is education. And if it has game and team, then it is. So this is a special kind of association rule, where the right hand side of the rule is always a singleton, and it corresponds to a special type of item which is a category, in this case a topic. So when you have a table like this, the rows may have different number of entries. Or if you want to think about it as a table, then some columns may be blank. So not all columns are filled, but there's one last column which is filled. And now you want to now construct these rules corresponding to the first k minus one columns, implying the last column. And this now becomes something where these rules that you get out of this give you a kind of classifier. So this is a kind of a simplistic way of doing supervised learning. So that kind of brings me to the end of this basic treatment of classes, this market basket analysis. So what you're really looking for is correlations between sets of items across transactions, which we\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content=\"So we have this horizontal line and we have this one blue thing which is above the horizontal. So what if you remove the one or two of the bottom green things which come below the line, these are also wrong. Right? So the blue thing above the line was wrong in some sense. The green triangles below the line are also wrong. So instead of removing the widest blue, you can try to remove the narrowest green or the two narrowest greens or something and see what happens. So you just have to copy paste this code and just change the condition and all the visualization. You can just run it as it is and see, I have not done it, but I think you might get something different. So the predict outcome is not predictable. I cannot guarantee you that one or the other will produce a different output, but it can happen. And this is generally something which one has to be mindful of when you're dealing with decision. One reason that this is important is remember that we talked about this cross validation.\", metadata={'source': 'audio_5.txt'}),\n",
              " Document(page_content=\"if given this input, will give you that corresponding output. So there is a correct output that you expect for every input, and then you can validate whether or not the program does it by feeding it different inputs of different types and seeing that it gives the expected output on a. But here the problem is that this expected output is not something that you know how to compute, and yet you need to make some statement, obviously, as to whether the model is good or bad, because if you can't validate that your model is doing well, you have no basis to use it. So this will be another issue that we will take up. How do we deal with this problem of evaluating performance when actually we don't have a yardstick measuring standard in some sense? So please stop me at any time. So if you think something, I'm going to get into a specific module very soon. So what we are going to look at are different. Start with very simple models. So what I'm going to do today is something called a decision\", metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"a specification, but generating a concrete program from a generic program you can think of a model template as. So I just give you a line here which says a Y equal to Mx plus c. And here I want a specific m zero and C zero, which fits the given data in the best possible way, which is also part of the description of the algorithm. What does it mean for one line to be better than another line? So that's part of it. So the other side of this picture, as I mentioned, is when you don't have this training data, you don't have any examples that people have already labeled, but you are looking for some kind of patterns. So there is no guidance. So the thing about supervised learning is somebody should have gone through and given you this information before. Now, this information could have been collected over time. Maybe it's not manually collected, like the school marks. The school marks over time. Everybody has. I mean, the school keeps a record of what happens in the model exams, what\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"distribution changes slightly, one attribute might get better entropy than another attribute. And you will choose that question first. So the shape of the tree might vary a lot between these, so you may not get the same. Okay, so the question about cross validation, cross validation. What I'm saying is that you do not make a hard and fast choice once and for all to split your data this way. So you're taking your data and you're saying, hey, first let me leave out this 20% and build one model. So I am building multiple models. I'm building m one, m two, m three, m four, and m five. Each of these models is built using 80% of the test data, but a different 80%. So each of them leaves out some 20%. So I'm building five different models and comparing what I see in all these five. So the first thing is that if this approach is good, then in general, these five models should behave in a similar way with respect to the outcome. So if they all behave similarly, then I can decide what to do. So\", metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"a fixed data set, and you are going to apply it to some other data set which you have not seen. So will that generalize well or not is the biggest question in machine learning. How do you know that the training data that you have used to build your model is actually representative of the data that you see in the real. And there could be many reasons why it is not valid. It could be because of sampling. So, for instance, you might be building a model for some consumer behavior by looking at one segment of. Maybe you are looking at the behavior of people who are in their, are building a model of how people behave, but your actual market consists of people who are maybe in their 40s, who have a very different way of dealing with things. So you need to be careful that the data that you're looking at is actually representative. So this is a huge problem and there's no correct answer. You can measure it, but you cannot. In some sense, if you could validate it, then this whole problem\", metadata={'source': 'audio_2.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_5=\"What is the process of building the decision tree classifier, and how is it trained on the dataset?\""
      ],
      "metadata": {
        "id": "zK57Px_bhELw"
      },
      "id": "zK57Px_bhELw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = lotr.get_relevant_documents(query_5)\n",
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPb4bN-MhbL4",
        "outputId": "a974df8c-712b-4036-a466-5c12bdb195de"
      },
      "id": "RPb4bN-MhbL4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"the order, sometimes we don't. Addresses, of course, are written in a million different ways. So there are all kinds of issues with just getting the data to a format where you can work on it. So this is an entirely different ballgame. It's much more kind of. There are lots of tools and techniques to deal with this, but that's not really going to be the focus of the course. So, in a sense, if you want to think about it, this data mining aspect will be almost missing from this course, even though it's part of the title. So what we are really going to look at is the machine learning aspect. So learning means you are trying to understand something that you don't know before. And machine learning suggests that it's done automatically. It's done by now machine as a computer. So there is an algorithm which learns something about, again, it's always with respect to data. So it's something about the data. So what we are trying to do is learn some kind of mathematical models from data. And this\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"its topic is education. And if it has game and team, then it is. So this is a special kind of association rule, where the right hand side of the rule is always a singleton, and it corresponds to a special type of item which is a category, in this case a topic. So when you have a table like this, the rows may have different number of entries. Or if you want to think about it as a table, then some columns may be blank. So not all columns are filled, but there's one last column which is filled. And now you want to now construct these rules corresponding to the first k minus one columns, implying the last column. And this now becomes something where these rules that you get out of this give you a kind of classifier. So this is a kind of a simplistic way of doing supervised learning. So that kind of brings me to the end of this basic treatment of classes, this market basket analysis. So what you're really looking for is correlations between sets of items across transactions, which we\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content=\"too, in order to. Sorry, I didn't hear you. Can you say louder? We need to take all the possible tree. That could happen. Is it not necessarily all the possible trees? So that is some issue that we have to deal with. So what we are going to try and build, we are not going to build all the possible trees because that would be too many trees. So we want to build a good tree as far as possible. And then we have to ask, what is a good tree? And with respect to that good tree, what we are saying is there is a kind of ranking of attributes. So even in this order, the tree will tell us a little bit about which attribute is more important. So there is a kind of implicit ranking which is happening, and an attribute which does not appear at all in our tree that we construct. If it turns out that otherwise it's a good tree, then we could argue that that attribute has no significance. But even otherwise, one of the things, byproducts of decision tree will be a kind of implicit importance, ranking\", metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"So last time we started with supervised learning in earnest, and we were looking at this decision tree model. So, in a decision tree model, you have essentially a table in which the columns are the. So each item, a row in the table and one special column denotes the class that you are trying to classify. And we assumed it's a binary classification. So the class is zero, one, or yes, no, or whatever you want to interpret. The two categories are. So the algorithm was very simple. At a high level, what you want to do is choose an attribute based on the answer to the question that you're asking about that attribute. The data splits, and then you take each partition, ask another question. So that's the thing. And you want to come down and keep asking questions until either you have reached, like these situations here, a partition in which all the values of the class variable are the same. So you have a uniform class level or you have run out of questions, because we can only ask in any path\", metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"about what's going on behind the algorithm in order to make best use of it when it is not producing an answer which is acceptable to you. But as far as using it per se, all you need to do is set up the appropriate. So here it's a decision tree, but pretty much the same. So these two steps that you see here, these two steps. So this is a pretty canonical style of programming. In this scikitler, you import the right type of classifier, you build a classifier of that type with the parameters that you think are reasonable for your application, and then you pass it the training data and say fit. Now here, one thing we have not done, for instance, is to segregate out, we talked about training and test data. We have not done that. So that's also something that you can do. Before you set it up, you might get all the iris data and then you might want to keep part of it aside. So we will see as we go along how to do all these things. But for the moment, it's taking all the iris data which has\", metadata={'source': 'audio_5.txt'}),\n",
              " Document(page_content=\"the data. So what we are trying to do is learn some kind of mathematical models from data. And this falls into two broad categories. So the first category is where you are given examples. So you are told, for instance, this is, for example, supposing you're trying to evaluate whether a painting is painted by a particular type of. So there are these pools of painting, like impressionists and cubists and so on. So you might be given a lot of pictures saying, this is cubist, this is not cubist, this is impressionist, and so on. And then from these examples which are given to you with manual information, you are supposed to build a model which will take an unknown painting and tell you from its features which of these categories. So this is a typical supervised learning kind of thing. Unsupervised learning is something which is closer to the data mining situation where you have data and you are looking for patterns, but you don't really have a clear idea beforehand what patterns they are.\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"a slightly different presentation of the idea. Okay, so now, as I said, this whole thing so far has been largely a kind of algorithmic problem, right? In some sense, if I fix the set of items and the set of transactions and the set of the two thresholds, the support and the confidence threshold, then for any x, comma y, either it is true or it is not true. So there is a fixed answer. So the answer is known. So this is something where I can actually validate that you have either enumerated all the association rules or you have missed out something. So technically there is no learning involved. It's more an algorithmic question. So how does one connect this to learning? Well, in specific context, you can interpret these rules as classification rules. So, remember we said supervised learning says, if this is there, then it is a fraud. If this thing is not there, then it is not a fraud, and so on. So, let's look at a typical example. So, supposing we have some question which is about\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content=\"happen. So in such a situation, also you have to stop and say something. So your decision tree, finally, what is the goal of decision tree is going to say? If this is the combination of attributes that I see in an item, is my answer yes? Or if I am this case, the answer is clear. Whatever label I have reached uniformly in that group, that is the answer. So if I have, as we said, if the person is young and has a job, I will say yes, I don't have to say anything else. If the person is middle aged and does not own a house, I will just say no. But if there is a mixed thing, then how do I choose? So there is no good way to choose. So all we can say is that at that point, hopefully there is some uneven distribution. So maybe there are a few unusual cases who had all the correct attributes as far as what is written down, but due to some mess up and some hidden kind of features, they didn't get it. So you could basically just take the majority, right? So supposing you do all this and you come\", metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"these groups, it's as though I have taken the remaining 80% for training and this 20% for test. But because I don't want to actually think of it this way, I will actually keep doing this for each one. So I will leave out the first 20%, then I will leave out the second 20%. So I will do this. If I have like k chunks, I will do this k times in each chunk. Each model I will build using all but one of these chunks. So this then will solve this problem of many problems. It will solve the problem where you don't have enough data to begin with to afford to not look at some of it, because the interesting things may be in that sum of it. Now, what will you do with this whole thing? Well, you have five. Typically, each of these models are going to be different because we saw with the decision tree, it's kind of the questions that you ask depend on the distribution of the different roles. And if the distribution changes slightly, one attribute might get better entropy than another attribute. And\", metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"and is therefore picking up some peculiarities which don't necessarily exist within. So one of the things we mentioned in passing was that we like short trees for two reasons. One is because they are easier to explain. The second thing, which I claim without any justification, is that they generalize better. So here, priority, they are saying, let us not construct deep trees. So whatever tree we construct, we are going to stop it when it reaches. So it's a two step process, right? So what you do is you first say what are the parameters for the decision tree classifier? So he says, I want a decision tree classifier to be set up with this random state 42 and which will not grow to more than depth two. And then I have to actually construct the classifier for a particular data set. So that's the next thing. So I use this fit function, right? In some sense this creates a decision tree object with certain operating parameters, and then you pass it the training data in the form of the input\", metadata={'source': 'audio_5.txt'}),\n",
              " Document(page_content=\"model, or a numerical prediction model. As we said, in case you're trying to predict a number or category, the category is more like a classification problem, right? I give you something and I want you to classify it as yes, no, good, bad, or we have unsupervised learning where we are looking for some kind of structure, and usually this is in the form of clustering, but we also have to deal with these outliers, as I said. And then sometimes you want to simplify the problem by knocking off certain dimensions or unrolling the object or so. So if you look at the popular press and the popular literature, all the focus of machine learning is actually on the first one. So supervised learning is by far the more attractive and the more, should I say, attractive, I guess it's the more attractive aspect of machine learning. This is what people are really looking for. But unfortunately, as I said, to do effective supervised learning, you need to actually get a lot of training data. And this is\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"can measure it, but you cannot. In some sense, if you could validate it, then this whole problem becomes much simpler. But the whole difficulty with machine learning is exactly this, that there is no sensible way to predict the behavior or even when it will fail. Even if you could say that this is likely to work well here and not likely to work well there that itself would be a huge improvement, not even necessarily a very precise quantification, but just some indication. But all these are very speculative. So that's a huge challenge. So there are some theoretical things you can say which we may not, may or may not get to in this course. But it's a big problem. This is the so called generalization problem. So that's what basically, if you build association rules, is it going to hold or not? I don't know. Okay, sir, I had a question. We have been talking about items and transactions, but we haven't anywhere spoken about whether one customer, suppose if you're talking about somebody\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content=\"use either this entropy idea or this genie index, and it gives me a sharper curve, and this sharper practice turns out to be better. So that's how you build a decision tree. You build a decision tree by applying one of these two nonlinear things. If you come up to the third thing, it's also fine. It's just that these two happen to be standard things. They are both borrowed from other areas where they have a long and established history. So, as I said, genie index comes from economics and entropy comes from information. So we'll stop here. So, any questions? It okay, if there are no questions, then we'll continue with this discussion of decision trees on Thursday when we meet next.\", metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"we are building our model. So that's the only source of inputs and outputs where we know the answer. So the only solution really is to use that. So we have to take that training data and keep some of it aside because we are optimizing our model, as we saw in the decision tree, we are optimizing it to give correct answers on the training data. So therefore it doesn't make sense to evaluate it back on the same inputs which are used to construct the tree. But if I withhold some data, so I basically take my training data as a whole and then I kind of split this and say, do not use this part, only use this part to build my model. Then the model that I built from here, I can ask, how does it do on that? And there is no problem with this because the model has not seen that data. So it's not been biased by the data on the right hand side. So this data has never been used in the model building process. So it is as good as unknown data as far as the model, but it is unknown to the model. But\", metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"and two, three are the petal. So it's taking all the rows and it's taking the third and fourth columns. So we're throwing away two columns. So we are making it, instead of a four column array, we are making it into a two column array. And in this data. So when you load the data set, it automatically produces these two subparts, iris data and iris target. So target is the classification variable. Remember, in this case, the classification variable is a three way thing, right? There are three types of irises that we saw, so it will be essentially a number, zero, one. And now to get this decision tree classifier, we have to take this function that we have imported. So we say take a decision tree classifier. We have to provide it with, again some random state because it uses some randomization inside it. And now this is something which we have not talked about. So we discussed that we will build a decision tree normally until we either achieve some pure leaf nodes where the classification\", metadata={'source': 'audio_5.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reordered_docs = reordering.transform_documents(docs)\n",
        "\n",
        "# Confirm that the 4 relevant documents are at beginning and end.\n",
        "reordered_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QT1xXXXRhpqf",
        "outputId": "380c7cfb-275a-4f2c-98a5-6e58cf6a750c"
      },
      "id": "QT1xXXXRhpqf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"the order, sometimes we don't. Addresses, of course, are written in a million different ways. So there are all kinds of issues with just getting the data to a format where you can work on it. So this is an entirely different ballgame. It's much more kind of. There are lots of tools and techniques to deal with this, but that's not really going to be the focus of the course. So, in a sense, if you want to think about it, this data mining aspect will be almost missing from this course, even though it's part of the title. So what we are really going to look at is the machine learning aspect. So learning means you are trying to understand something that you don't know before. And machine learning suggests that it's done automatically. It's done by now machine as a computer. So there is an algorithm which learns something about, again, it's always with respect to data. So it's something about the data. So what we are trying to do is learn some kind of mathematical models from data. And this\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"too, in order to. Sorry, I didn't hear you. Can you say louder? We need to take all the possible tree. That could happen. Is it not necessarily all the possible trees? So that is some issue that we have to deal with. So what we are going to try and build, we are not going to build all the possible trees because that would be too many trees. So we want to build a good tree as far as possible. And then we have to ask, what is a good tree? And with respect to that good tree, what we are saying is there is a kind of ranking of attributes. So even in this order, the tree will tell us a little bit about which attribute is more important. So there is a kind of implicit ranking which is happening, and an attribute which does not appear at all in our tree that we construct. If it turns out that otherwise it's a good tree, then we could argue that that attribute has no significance. But even otherwise, one of the things, byproducts of decision tree will be a kind of implicit importance, ranking\", metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"about what's going on behind the algorithm in order to make best use of it when it is not producing an answer which is acceptable to you. But as far as using it per se, all you need to do is set up the appropriate. So here it's a decision tree, but pretty much the same. So these two steps that you see here, these two steps. So this is a pretty canonical style of programming. In this scikitler, you import the right type of classifier, you build a classifier of that type with the parameters that you think are reasonable for your application, and then you pass it the training data and say fit. Now here, one thing we have not done, for instance, is to segregate out, we talked about training and test data. We have not done that. So that's also something that you can do. Before you set it up, you might get all the iris data and then you might want to keep part of it aside. So we will see as we go along how to do all these things. But for the moment, it's taking all the iris data which has\", metadata={'source': 'audio_5.txt'}),\n",
              " Document(page_content=\"a slightly different presentation of the idea. Okay, so now, as I said, this whole thing so far has been largely a kind of algorithmic problem, right? In some sense, if I fix the set of items and the set of transactions and the set of the two thresholds, the support and the confidence threshold, then for any x, comma y, either it is true or it is not true. So there is a fixed answer. So the answer is known. So this is something where I can actually validate that you have either enumerated all the association rules or you have missed out something. So technically there is no learning involved. It's more an algorithmic question. So how does one connect this to learning? Well, in specific context, you can interpret these rules as classification rules. So, remember we said supervised learning says, if this is there, then it is a fraud. If this thing is not there, then it is not a fraud, and so on. So, let's look at a typical example. So, supposing we have some question which is about\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content=\"these groups, it's as though I have taken the remaining 80% for training and this 20% for test. But because I don't want to actually think of it this way, I will actually keep doing this for each one. So I will leave out the first 20%, then I will leave out the second 20%. So I will do this. If I have like k chunks, I will do this k times in each chunk. Each model I will build using all but one of these chunks. So this then will solve this problem of many problems. It will solve the problem where you don't have enough data to begin with to afford to not look at some of it, because the interesting things may be in that sum of it. Now, what will you do with this whole thing? Well, you have five. Typically, each of these models are going to be different because we saw with the decision tree, it's kind of the questions that you ask depend on the distribution of the different roles. And if the distribution changes slightly, one attribute might get better entropy than another attribute. And\", metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"model, or a numerical prediction model. As we said, in case you're trying to predict a number or category, the category is more like a classification problem, right? I give you something and I want you to classify it as yes, no, good, bad, or we have unsupervised learning where we are looking for some kind of structure, and usually this is in the form of clustering, but we also have to deal with these outliers, as I said. And then sometimes you want to simplify the problem by knocking off certain dimensions or unrolling the object or so. So if you look at the popular press and the popular literature, all the focus of machine learning is actually on the first one. So supervised learning is by far the more attractive and the more, should I say, attractive, I guess it's the more attractive aspect of machine learning. This is what people are really looking for. But unfortunately, as I said, to do effective supervised learning, you need to actually get a lot of training data. And this is\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"use either this entropy idea or this genie index, and it gives me a sharper curve, and this sharper practice turns out to be better. So that's how you build a decision tree. You build a decision tree by applying one of these two nonlinear things. If you come up to the third thing, it's also fine. It's just that these two happen to be standard things. They are both borrowed from other areas where they have a long and established history. So, as I said, genie index comes from economics and entropy comes from information. So we'll stop here. So, any questions? It okay, if there are no questions, then we'll continue with this discussion of decision trees on Thursday when we meet next.\", metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"and two, three are the petal. So it's taking all the rows and it's taking the third and fourth columns. So we're throwing away two columns. So we are making it, instead of a four column array, we are making it into a two column array. And in this data. So when you load the data set, it automatically produces these two subparts, iris data and iris target. So target is the classification variable. Remember, in this case, the classification variable is a three way thing, right? There are three types of irises that we saw, so it will be essentially a number, zero, one. And now to get this decision tree classifier, we have to take this function that we have imported. So we say take a decision tree classifier. We have to provide it with, again some random state because it uses some randomization inside it. And now this is something which we have not talked about. So we discussed that we will build a decision tree normally until we either achieve some pure leaf nodes where the classification\", metadata={'source': 'audio_5.txt'}),\n",
              " Document(page_content=\"we are building our model. So that's the only source of inputs and outputs where we know the answer. So the only solution really is to use that. So we have to take that training data and keep some of it aside because we are optimizing our model, as we saw in the decision tree, we are optimizing it to give correct answers on the training data. So therefore it doesn't make sense to evaluate it back on the same inputs which are used to construct the tree. But if I withhold some data, so I basically take my training data as a whole and then I kind of split this and say, do not use this part, only use this part to build my model. Then the model that I built from here, I can ask, how does it do on that? And there is no problem with this because the model has not seen that data. So it's not been biased by the data on the right hand side. So this data has never been used in the model building process. So it is as good as unknown data as far as the model, but it is unknown to the model. But\", metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"can measure it, but you cannot. In some sense, if you could validate it, then this whole problem becomes much simpler. But the whole difficulty with machine learning is exactly this, that there is no sensible way to predict the behavior or even when it will fail. Even if you could say that this is likely to work well here and not likely to work well there that itself would be a huge improvement, not even necessarily a very precise quantification, but just some indication. But all these are very speculative. So that's a huge challenge. So there are some theoretical things you can say which we may not, may or may not get to in this course. But it's a big problem. This is the so called generalization problem. So that's what basically, if you build association rules, is it going to hold or not? I don't know. Okay, sir, I had a question. We have been talking about items and transactions, but we haven't anywhere spoken about whether one customer, suppose if you're talking about somebody\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content=\"and is therefore picking up some peculiarities which don't necessarily exist within. So one of the things we mentioned in passing was that we like short trees for two reasons. One is because they are easier to explain. The second thing, which I claim without any justification, is that they generalize better. So here, priority, they are saying, let us not construct deep trees. So whatever tree we construct, we are going to stop it when it reaches. So it's a two step process, right? So what you do is you first say what are the parameters for the decision tree classifier? So he says, I want a decision tree classifier to be set up with this random state 42 and which will not grow to more than depth two. And then I have to actually construct the classifier for a particular data set. So that's the next thing. So I use this fit function, right? In some sense this creates a decision tree object with certain operating parameters, and then you pass it the training data in the form of the input\", metadata={'source': 'audio_5.txt'}),\n",
              " Document(page_content=\"happen. So in such a situation, also you have to stop and say something. So your decision tree, finally, what is the goal of decision tree is going to say? If this is the combination of attributes that I see in an item, is my answer yes? Or if I am this case, the answer is clear. Whatever label I have reached uniformly in that group, that is the answer. So if I have, as we said, if the person is young and has a job, I will say yes, I don't have to say anything else. If the person is middle aged and does not own a house, I will just say no. But if there is a mixed thing, then how do I choose? So there is no good way to choose. So all we can say is that at that point, hopefully there is some uneven distribution. So maybe there are a few unusual cases who had all the correct attributes as far as what is written down, but due to some mess up and some hidden kind of features, they didn't get it. So you could basically just take the majority, right? So supposing you do all this and you come\", metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"the data. So what we are trying to do is learn some kind of mathematical models from data. And this falls into two broad categories. So the first category is where you are given examples. So you are told, for instance, this is, for example, supposing you're trying to evaluate whether a painting is painted by a particular type of. So there are these pools of painting, like impressionists and cubists and so on. So you might be given a lot of pictures saying, this is cubist, this is not cubist, this is impressionist, and so on. And then from these examples which are given to you with manual information, you are supposed to build a model which will take an unknown painting and tell you from its features which of these categories. So this is a typical supervised learning kind of thing. Unsupervised learning is something which is closer to the data mining situation where you have data and you are looking for patterns, but you don't really have a clear idea beforehand what patterns they are.\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"So last time we started with supervised learning in earnest, and we were looking at this decision tree model. So, in a decision tree model, you have essentially a table in which the columns are the. So each item, a row in the table and one special column denotes the class that you are trying to classify. And we assumed it's a binary classification. So the class is zero, one, or yes, no, or whatever you want to interpret. The two categories are. So the algorithm was very simple. At a high level, what you want to do is choose an attribute based on the answer to the question that you're asking about that attribute. The data splits, and then you take each partition, ask another question. So that's the thing. And you want to come down and keep asking questions until either you have reached, like these situations here, a partition in which all the values of the class variable are the same. So you have a uniform class level or you have run out of questions, because we can only ask in any path\", metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"its topic is education. And if it has game and team, then it is. So this is a special kind of association rule, where the right hand side of the rule is always a singleton, and it corresponds to a special type of item which is a category, in this case a topic. So when you have a table like this, the rows may have different number of entries. Or if you want to think about it as a table, then some columns may be blank. So not all columns are filled, but there's one last column which is filled. And now you want to now construct these rules corresponding to the first k minus one columns, implying the last column. And this now becomes something where these rules that you get out of this give you a kind of classifier. So this is a kind of a simplistic way of doing supervised learning. So that kind of brings me to the end of this basic treatment of classes, this market basket analysis. So what you're really looking for is correlations between sets of items across transactions, which we\", metadata={'source': 'audio_2.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_6=\" What is the challenge associated with supervised learning? \""
      ],
      "metadata": {
        "id": "Nt53hYvqAH6Y"
      },
      "id": "Nt53hYvqAH6Y",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_6 = lotr.get_relevant_documents(query_6)\n",
        "docs_6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6w-DVP1AtXo",
        "outputId": "34dddb5e-6158-4e2e-f276-4ba0233eb088"
      },
      "id": "z6w-DVP1AtXo",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"to work in two dimensions, I actually unroll it. And then I find I have to deal with these multiple bands. So now it's not so obvious anymore because I do have lines, but I have many of them, and I have to decide between which bands it is, green, and which bands it is, this yellow color. So the peril of supervised unsupervised learning is that you don't know that the step that you are taking in advance, you don't know is going to be good or bad because it's unsupervised. You're just taking a guess in some sense, but then you have to have some way of evaluating whether the guess was good or not. And then going back and doing it again may be a different. So this is kind of a quick overview, a summary of the main things that we will be looking at. So, supervised learning and unsupervised learning. So we are looking either to build a predictive model, which is broadly a classification model, or a numerical prediction model. As we said, in case you're trying to predict a number or\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"a slightly different presentation of the idea. Okay, so now, as I said, this whole thing so far has been largely a kind of algorithmic problem, right? In some sense, if I fix the set of items and the set of transactions and the set of the two thresholds, the support and the confidence threshold, then for any x, comma y, either it is true or it is not true. So there is a fixed answer. So the answer is known. So this is something where I can actually validate that you have either enumerated all the association rules or you have missed out something. So technically there is no learning involved. It's more an algorithmic question. So how does one connect this to learning? Well, in specific context, you can interpret these rules as classification rules. So, remember we said supervised learning says, if this is there, then it is a fraud. If this thing is not there, then it is not a fraud, and so on. So, let's look at a typical example. So, supposing we have some question which is about\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content='whatever, income, stuff like that. So each row in the table will be one item, and there will be a special column which indicates what category, depending on the classification problem that we are dealing with, what category it is. And the goal of supervised learning is to now construct, given a new row, a new item, a new combination of these attributes, which is not seen before, try to predict what would be the most appropriate category to assign. So these items which are given with the labels. So these are called labels. So these items are called training data. So we have labeled training data, which consists of some information which has been collected in the past, some historical information about items and their classification. And we want to find a model that generalizes the training data. So since normally this categorization corresponds to some kind of classification, this is typically called a classification problem. So classification problem could be a binary problem. It', metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"in, and they will choose which one to emphasize. Depending on the context. Depending on the context, you may want precision predominant. You may want recall to be predominant. So there, it really becomes a domain specific. So I'll stop there. Any questions, sir? Like, given a precision or recall value, what we need to change in the algorithm so that we could come to the certain value of precision and recall. So that is a harder question. Right. So what is it that you need to change? So that's a very difficult question. All you can say is that given what we are doing now, this is what, then it becomes really an experimental issue. So this is the hardest question in machine learning, which is, how do I improve the quality of the classifier? So there is no answer to that question, which is obvious. So you have to start experimenting. You have to do something. So there is something called feature engineering. You might want to come out with some combination of features. So some of you may\", metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"to do effective supervised learning, you need to actually get a lot of training data. And this is sometimes expensive. Very often it is expensive. So unsupervised learning is often the starting. A few. Couple of years back, there were three turing awards given for deep learning. So one of them was given to Jan Lakun. And his observation is that if you think of this whole thing as a cake, then actually most of the cake is actually unsupervised learning. So supervised learning is just a thin layer on the top, but it's like the icing on the cake. So when we go to a bakery and we see an attractive cake, what we're attracted to is the surface, the icing surface. But if the inside is made of sodas, then the cake will not taste good. So you have to be careful not to be fooled by the icing. So the unsupervised learning is also important. But supervised learning is where all the, in some sense, the jazz is. That's what people spend up spending a lot more time on. And we will also probably\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"can measure it, but you cannot. In some sense, if you could validate it, then this whole problem becomes much simpler. But the whole difficulty with machine learning is exactly this, that there is no sensible way to predict the behavior or even when it will fail. Even if you could say that this is likely to work well here and not likely to work well there that itself would be a huge improvement, not even necessarily a very precise quantification, but just some indication. But all these are very speculative. So that's a huge challenge. So there are some theoretical things you can say which we may not, may or may not get to in this course. But it's a big problem. This is the so called generalization problem. So that's what basically, if you build association rules, is it going to hold or not? I don't know. Okay, sir, I had a question. We have been talking about items and transactions, but we haven't anywhere spoken about whether one customer, suppose if you're talking about somebody\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content=\"a dog picture from a cat picture, or one which is neither of the two, it's not very easy to quantify that. And there the attributes in a picture are typically just if you think of a picture as a digital picture, it's a bunch of pixels and their colors and their arrangement, their relative arrangement. So even I'm saying, even if you have some domain knowledge, it's not very clear that the person's domain knowledge is aligned to the way the data is available. So what we are going to be doing is agnostic to the domain. So we are not going to believe anything or assume anything about the domain, except that we will want to validate at the end somehow that what we are doing is right. But in practice what you're saying makes sense that really to do supervised learning, it's much more realistic to do it in collaboration with somebody who understands a domain rather than just doing it blindly. Because also when you do it blindly, you might come up with solutions which make no sense. You\", metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"against some unknown data. So, one other strategy where you don't want to, I mean, one of the other disadvantages of doing this is that maybe that there are some minority anomalies in this data, and the choice of test data that you have made might have hidden all these anomalies, so you never see them, and maybe it's important to see them. So there may be many situations, or maybe you just don't have enough data as a whole to build a good model by only looking at 80%. So another strategy is to systematically do this with different subsets. So what you're really asking at some higher level is that machine learning approach. So, remember, we have seen only decision trees, but we are going to see many models. So there are clearly many ways to build models. And the reason, whenever you see that there are many ways to do something, it's only because there is no guarantee that a given way is the best one. So more or less what you want to validate is, I have, say, seven different strategies\", metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"are looking for patterns, but you don't really have a clear idea beforehand what patterns they are. So a typical example of this might be that a company which is selling something might want to know some information about the demographics of its customers. So what are the groupings? I mean, which age groups? What is the proportion of people who are, say, between 20 and 30 who buy their products, people above 50 who buy their products, and so on. So this kind of thing is unsupervised because you don't know what you're going to get. But after you get this information, maybe you can put it to good use. So these are broadly the two types of things that are there. There is a third type of machine learning called reinforcement learning, which we are not going to talk about at all in this course. So to look at supervised learning a little more detail, we will of course do it in much more detail in the lectures to come. So we are trying to extrapolate. So basically it's a prediction problem,\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"an algorithmic problem. It's not technically a learning problem, because this is a fixed problem once we fix these quantities. So given the set of items and the transactions, and given these two thresholds, find every x y which satisfies these two constraints. This is what we are trying to do. So we will see what it means for learning at the end. Sir, I had a question. So therefore, what we said is that a rule is interesting only if this ratio is bigger than sigma. Right? The fraction of transactions where we see the target of the rule must be at least sigma. If I take m to the other side, this is same as saying there's a number of transactions with x and Y. So this is just the number of transactions. X and y exceeds sigma times the total. It's some whatever, 1% of the total, 10% of the total, and so on. So what we said last time is that this would be our first criteria. So we want to find x union Y, which is plausible. Once we have x union Y, which is plausible, we will try to break\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content='So we were looking at this market basket analysis, and towards the end of the discussion on association rule rules, if everybody who buys X is also likely to buy y. We said that one application of this association rule idea is to have these so called class association rules. So the right hand side was a category. We looked at an example with documents and we said, if a document has these words, it is also likely to belong to this category. So that brings us to the topic that is one of the central ones in machine learning, which is the idea of supervised learning. So in supervised learning, we have a set of items, and each item belongs to some category or class, and we distinguish these items in terms of some attributes. So you can think about this, as we will see shortly, you can think about these attributes as columns, right? So the attributes will be things like name, age, date of birth, whatever, income, stuff like that. So each row in the table will be one item, and there will be a', metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"engineering. You might want to come out with some combination of features. So some of you may have heard of this thing, for example, which is used in medicine called body mass index. So, body mass index is some combination of some formula involving height and weight. So you might have a data set in which you have something like height and weight usually classify using height and weight. It doesn't work. But if you combine it in this strange way using body mass index, then you actually get a better class. So this is an example of what is called feature engineering. You make new features out of the old one, or you might need to change your model, maybe decision tree is not good, you need to use another model. So these are all very open questions, and I don't think there is any. If there were a clear answer to that, then life would be a lot simpler. But unfortunately, that is the real challenge of machine learning. What is it? If I am given a certain measure of performance, what is it\", metadata={'source': 'audio_4.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_7=\"What considerations should businesses keep in mind when applying market basket analysis to understand customer behavior?\""
      ],
      "metadata": {
        "id": "lLfdjRuTCnOW"
      },
      "id": "lLfdjRuTCnOW",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_7 = lotr.get_relevant_documents(query_7)\n",
        "docs_7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnekLxovCuCP",
        "outputId": "c82854e5-de9c-4e1b-9580-f298b4db5cd1"
      },
      "id": "pnekLxovCuCP",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"the more I would say, believable end of the story is that somebody found this out. There was no immediate way to make use of this fact. Maybe it is true that at certain times, people who buy diapers also buy beer, but it at least gave some confidence to the idea that you can use this kind of information to make decisions. So from that perspective, it possibly had a role to play. But anyway, so these are the origins of this problem, and it's called market basket analysis because it's really talking in this context about shopping baskets. I have a shopping basket when I reach the counter to pay. So question is, what is in that basket? If it has x, does it also have y? So this is a very literal interpretation of markets and baskets. But the notion of markets and baskets can be a little bit more abstract. So, for instance, if you are thinking about, especially nowadays, given the number of elearning platforms, there's a lot of emphasis on trying to structure topics in such a way that\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"as a different transaction every time. But for somebody who's buying everything together, doesn't that give rise to a lot of. Yeah, so there are all these situations, I agree, which are not directly addressed in this. There are many different variations of this that you could ask. So there's no doubt about that. So some of them people have looked at because they have kind of natural solutions in this. Some of them maybe you cannot do. So that's another thing about this whole model building thing is that the same model may or may not be capable of tackling every different question that you ask. So this model, some of these things maybe you can segregate and answer. But if, as you're saying, you want to compare behaviors across customers of different types, you cannot, as you said, sensibly aggregate them into a single market basket model because they all have different profiles. So you will have to, in the example that you gave, you would have to first separate out the data for these,\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content='So we were looking at this market basket analysis, and towards the end of the discussion on association rule rules, if everybody who buys X is also likely to buy y. We said that one application of this association rule idea is to have these so called class association rules. So the right hand side was a category. We looked at an example with documents and we said, if a document has these words, it is also likely to belong to this category. So that brings us to the topic that is one of the central ones in machine learning, which is the idea of supervised learning. So in supervised learning, we have a set of items, and each item belongs to some category or class, and we distinguish these items in terms of some attributes. So you can think about this, as we will see shortly, you can think about these attributes as columns, right? So the attributes will be things like name, age, date of birth, whatever, income, stuff like that. So each row in the table will be one item, and there will be a', metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"value, and in that case, the choice of the value matters. Whereas if it's a relative value, is this value better than that value? Then all of them, I agree, will be the same. Okay, so one thing to emphasize is that I think in either the previous class or before, there was this question about whether interpreting the answers will help build a model. So if we know what these attributes mean, then can we build a better model? And I said that, of course, in practice that would be true. That if you know what attributes mean, you might focus on some attributes. You might, for example, if it is, say, some medical diagnosis, then you may not be very concerned about the date of birth. I mean the year of birth, yes, but you may not need to know whether it's January 27 or May 30. It may not make much difference. So there are some things which may be less or more relevant, or even place of birth may not be so relevant, and so on. So you might automatically throw out some. But in general, we don't\", metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"you, of course, figure out through some sampling or through some explicit customer information, details about all your customers or some large fraction of your customers. And then you have to look for patterns. You have to look for these groups. So the same thing might happen in a shop. If it's a shop which sells multiple types of things, it could be a clothing shop which addresses different age groups, different genders, different types of clothing, might be sending fashion clothing and sports clothing and baby clothes and various things. Then depending on how many people are visiting of different types, you might realize that one category is taking up a lot of space and it's totally useless, or you're making a lot of money in one category, but it's underrepresented. So, again, this customer segmentation helps. And, of course, something which has become almost a kind of a disease now is that we are no longer seeing entertainment because we would like it, but we are saying it because\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"all refinements. And there are many situations where these are important and interesting, but this model as such, this kind of market basket analysis approach is very naive. And so that's why these things will come up in different contexts. But we are not going to explore it anymore in this context, because in this context, the kind of model that you build, this association rule model is rather simplistic. So we are going to look at more sophisticated models as we go along. And there you can ask the same question, but it'll take a slightly different format. Does this also include tendencies like, if there is one person who tends to buy all his groceries once a month, then they will have a bigger basket size. But for people who buy their groceries, say like per week or every other day, they will have smaller basket size, but their transactions would be spread out. So that will be treated as a different transaction every time. But for somebody who's buying everything together, doesn't\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content=\"one of the things, byproducts of decision tree will be a kind of implicit importance, ranking among the attributes, which ones are more significant. So not all columns are equally important. So these are all things that have come up. So maybe when we come to it later on we'll discuss it. But these questions that you're asking, sir, when we form this decision tree, is it important or in some sense beneficial if we know the data? Insight of the data. Insight of the data, meaning what? As in the relevance of each column? Yes, normally that is, I mean, I cannot say it will always help. Normally it will help. But on the other hand, you have to assume that you do not have. So what you're really asking in a more general setting is that if a person who is kind of aware of the context, who's a specialist or a domain expert in this, so say somebody who has been associated with this, whatever, this bank, this particular bank, which is giving out loans, if they build the tree, they might make\", metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"these groups, it's as though I have taken the remaining 80% for training and this 20% for test. But because I don't want to actually think of it this way, I will actually keep doing this for each one. So I will leave out the first 20%, then I will leave out the second 20%. So I will do this. If I have like k chunks, I will do this k times in each chunk. Each model I will build using all but one of these chunks. So this then will solve this problem of many problems. It will solve the problem where you don't have enough data to begin with to afford to not look at some of it, because the interesting things may be in that sum of it. Now, what will you do with this whole thing? Well, you have five. Typically, each of these models are going to be different because we saw with the decision tree, it's kind of the questions that you ask depend on the distribution of the different roles. And if the distribution changes slightly, one attribute might get better entropy than another attribute. And\", metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"made the simplifying assumption that a transaction, we are only going to look at distinct items in the transaction. So we are looking at the shopping basket, we are only looking at different items. We're not looking at multiplicities of items. So now, over a period of time, we collect information about these transactions. So we have a set of transactions, some m transactions, and again, we can assume that M is large. And now we are going to base this as our starting point to kind of figure out this. People who buy X also buy y, so that we now need to formulate. So we want a kind of association. So people who buy x also by Y says that these are, first of all items. So in general, now these are sets of items, not individual items. And very often in this area, these are called item sets. The question about m, no, m is not the number of items in a basket. M is the total number of baskets. So each Ti is a subset. So each Ti is a basket. And I observe many customers. I'm trying to\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"have to, in the example that you gave, you would have to first separate out the data for these, say, the people who buy one month groceries at a time from those who buy at more frequent intervals and then analyze them separately, because together the data doesn't actually make sense. So last time, if you remember, I talked about this unsupervised. So maybe one thing that you need to do when you have something large is to basically look at the transactions and categorize them according to some criteria, maybe by the size of the basket. And then you will find that there are maybe a lot of people who buy five items at a time. There are people who buy ten to 20 items at a time. There are people who buy 40 items at a time. And maybe you need to then apply the model separately to each of these clusters. So all of these things are not a one size fits all solution. And also, it's not that one approach on its own is going to solve the problem at any given time. So you need to have a kind of\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content=\"it. So you could basically just take the majority, right? So supposing you do all this and you come to a situation where you have eight customers who have similar attributes, and five of them are given a loan and three are not given, then if you get a new case which looks like this, you will say, okay, five out of eight were given. So let me predict that the new case will be given, because that's a nature. So that's really the only sensible thing you can do in the absence of any other information. So that's typically what a decision three algorithm. So it'll ask question by question. The real we have to figure out is this part, which is how do you pick the next attribute to question. But once you pick an attribute, then the next step is automatic. It will filter out the rows according to the values attribute. And then you have to ask the same algorithm applied on each of those subtables, and these subtables be destroyed. And when you stop, you ask whether it's uniform or whether\", metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"these things, so I am really somehow thinking of them as being like probabilities. So this would be the entropy, and there is a corresponding version for gini. So remember the earlier on was one minus p zero squared plus p one squared. So now I'm just saying add up all the PI squareds, again, assuming that the summation of the PI squared. So in this way, I can apply the same criterion that I'm using to discuss the class randomness in this group of rows that I have. What is the split between yes and no? So that is, at the category level, I'm looking at the randomness of the entropy, and I can look at an attribute and ask, in this group of rows, what is the randomness of this attribute? And what we are saying is that we want to penalize those attributes which are scattered, which have lots of values. In the worst case, they have this one value per one item per value, like a passport number or something. So if I take this extreme case that each of these values occurs exactly once, then\", metadata={'source': 'audio_4.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_8=\"How does the genie index contribute to decision tree construction, and why is it favored in practice, particularly in Python libraries?\"\n"
      ],
      "metadata": {
        "id": "z3Zov1VuC1iu"
      },
      "id": "z3Zov1VuC1iu",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_8 = lotr.get_relevant_documents(query_8)\n",
        "docs_8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFWsA4-RC8EH",
        "outputId": "8f957ac6-822c-4969-f044-7f51b080c8cc"
      },
      "id": "PFWsA4-RC8EH",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"a specification, but generating a concrete program from a generic program you can think of a model template as. So I just give you a line here which says a Y equal to Mx plus c. And here I want a specific m zero and C zero, which fits the given data in the best possible way, which is also part of the description of the algorithm. What does it mean for one line to be better than another line? So that's part of it. So the other side of this picture, as I mentioned, is when you don't have this training data, you don't have any examples that people have already labeled, but you are looking for some kind of patterns. So there is no guidance. So the thing about supervised learning is somebody should have gone through and given you this information before. Now, this information could have been collected over time. Maybe it's not manually collected, like the school marks. The school marks over time. Everybody has. I mean, the school keeps a record of what happens in the model exams, what\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"its topic is education. And if it has game and team, then it is. So this is a special kind of association rule, where the right hand side of the rule is always a singleton, and it corresponds to a special type of item which is a category, in this case a topic. So when you have a table like this, the rows may have different number of entries. Or if you want to think about it as a table, then some columns may be blank. So not all columns are filled, but there's one last column which is filled. And now you want to now construct these rules corresponding to the first k minus one columns, implying the last column. And this now becomes something where these rules that you get out of this give you a kind of classifier. So this is a kind of a simplistic way of doing supervised learning. So that kind of brings me to the end of this basic treatment of classes, this market basket analysis. So what you're really looking for is correlations between sets of items across transactions, which we\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content=\"is actually easier to compute. And if you look at, as we will see, libraries in Python and all that, typically they use genie index. So although there is a very famous system built using entropy, which was one of the early decision tree systems, the ones that we will practically encounter in Python, libraries actually use the genie index. Basically. I'll stop with this right now. So what we are saying now is that we want to choose the next question to evaluate the next question to ask. So for the next question to ask, we have to say something about the quality of the impurity before and after. So if I just look at the raw ratio of the impurity, I have a motion of just that impurity percentage. But that gives me this linear relationship, which apparently is not that good in practice. So I want a nonlinear way of giving some impurity value to this ratio of yes or no, and then we can use either this entropy idea or this genie index, and it gives me a sharper curve, and this sharper\", metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"There is a kind of gray area because there are these mixed up things which have, like here for instance, we have similar values and you have flowers from both categories with similar values. So this is this iris data set, and you can actually construct a decision tree for it. So this tree, which we will see later, this python library for decision trees, actually constructs from the iris data set, this decision tree. So what you do is you start with these 150 samples, and initially there are three categories, so there are 50 in each category. And remember that if there are at any point when you have an impure sample, you have to decide on something. So this particular algorithm, they're equal. It happens to just choose one of them more or less at random as the category to predict nation. So this last line is the prediction, this is the distribution of values. This is how many samples there are. And this is the actual genie index calculation. If you think about it, it is the three\", metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"to find, which is fixed once I fix the basic data. So the basic data is only the items on the transactions, but my parameters, which are user defined parameters, this chi and sigma. Once I fix those, then the answer is no. So this is not, therefore, in the typical sense of a machine learning problem. Because usually in a machine learning problem, the answer is not known. If the answer were known, then you would not need machine learning. If you knew exactly how to tell whether somebody is going to pass their exam. You don't need to build a model, you just use whatever deterministic algorithm they have. Or if you know whether or not this loan is going to default, then you directly decide. The problem is that those things, there is no fixed way of knowing the answer. So it's also difficult, therefore, to tell whether the answer you are predicting is right or wrong. So here it's not yet, first of all, a truly learning problem. It's more an algorithmic problem, but still, it's an\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"what you're really looking for is correlations between sets of items across transactions, which we formalize in terms of these association groups. X implies y. And what we saw is that as such, combinatorially, this is a very complicated thing to calculate, given the sizes of the sets involved. But we can use this appropriate principle both to identify the sets which are interesting, those which are frequent enough to be considered, and also to decompose them into rules. So you use the approoriate twice, once to find the frequent z, and second to decompose each frequent z into possible x implies y. And using this, we can build our first simple supervised learning model. So these rules now become a very simple way of prediction. Now, if you give me a document, I will look at what words are there in the document in this previous example. And if it contains the word student and school, I will predict education. If it contains game and team, I'll predict sports. Now, of course, if it has\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content=\"use either this entropy idea or this genie index, and it gives me a sharper curve, and this sharper practice turns out to be better. So that's how you build a decision tree. You build a decision tree by applying one of these two nonlinear things. If you come up to the third thing, it's also fine. It's just that these two happen to be standard things. They are both borrowed from other areas where they have a long and established history. So, as I said, genie index comes from economics and entropy comes from information. So we'll stop here. So, any questions? It okay, if there are no questions, then we'll continue with this discussion of decision trees on Thursday when we meet next.\", metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"there are. And this is the actual genie index calculation. If you think about it, it is the three samples are each with one third. So you have one third squared plus one third, one third squared, one nine. So one minus one 9th plus one nine plus one nine, which is one minus one thirds. So that's why you get this in the index of zero, 67. So this is the initial. Now, what this decision tree is doing at this point is it is choosing one of the attributes, which is in this case the petal length. And it is asking whether the petal number, so this number, if you look at it, is somewhere here, it's asking whether the petal length is less than 2.45, which is a number between two and three, which splits the yellow from the blue and the green. So if it is, yes, less than 2.45, then all the samples are in the smallest category and it is 500 zero. So all the yellow flowers fall in this category. On the other hand, the remaining hundred are equally distributed and it basically picks the first\", metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"But there may be more subtle things. Somebody might be talking about events, some kind of pop music events, and another person may be talking about some spiritual gatherings. And one person may be interested in pop music events, and another person may not be. And another person may be interested in spiritual gatherings, and the first person may not be. So then you can train these things. So in that sense, which words signal junk and which words don't signal junk is also a parameter of the model. Just like in this linear fit, the shape of the line is a parameter. So this is the learning part. So what we are going to do is look at different types of models, as I said, and then we will look at this parameter adjustment. How does the data actually determine the model, the concrete model? How do we build the best model that we can get for the given data? That's the algorithm. So we have a kind of model template. So we have a model template on this side, we have training data, and what we\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"advantage of this is that it's relatively speaking, given that our assumption is at every level, this fks are small. This is much faster than going to the bigger set and then filtering down, building up the bigger set from the smaller set. Even if we over approximate is going to be much faster and it's not going to be too much, because we know that the smaller sets are actually quite small. So, so the, if you want to think about how to do this algorithmically. So what we have is we have our set f k minus one. So supposing we just sort it in dictionary order, right? We think of these as k minus one tuples, and we sort it in dictionary order. Meaning that if the first position where two entries differ, if one is smaller than the other one, then that tuple is smaller. The first letter in the word which differs is smaller. So that's a dictionary. So what I will have is I will have a bunch of things where I have the same I one, two, I k minus one, and then I have a different j one. Jk, jk.\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content='and otherwise it will turn out to be much smaller than that. So again, you can plot the genie index curve. And now these are the curves. So this is our old friend. This is entropy. I mean, sorry, this is our misclassification, linear. This green dashed line is genie impurity, and this is entropy. But entropy goes up to one, which is out of scale. So we have the entropy, we get this red. So, notice that the entropy line is slightly steeper than the genie impurity line. And our logic earlier was that somehow nonlinear is better empirically, whatever that means. So empirically just means that somebody built these systems and found that if they use these kind of things, it works better. So empirically, entropy is, I mean, theoretically, entropy is slightly steeper, but entropy requires us to compute logs and all that, whereas genie index is just this computing squares of numbers. So genie index is actually easier to compute. And if you look at, as we will see, libraries in Python and all', metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"to split here and then split there and split there so I can ask more and more questions. But we don't. So that's one reason, going back to the earlier thing, that you might want to stop, because your genie index is small enough, and it may be different to stop for the genie index versus to stop for the entropy versus to stop for the linear misclassification. But our question is not that. Our question is where did you get these numbers from? Why did you choose 2.45? Why did you choose 1.75? Where did these numbers come from? Because obviously the algorithm is not seeing the picture. It's not like us interpreting that picture and saying these are natural places to draw the line. So how do you get that? This number will minimize the getney index? Is that. Yeah, but then how do you know it is Y 2.45 and not say 2.75 and not 2.15? So that's one question, and in this direction, it's a little more tricky, because the tricky is that if I move it slightly, if I move it slightly up or down, I\", metadata={'source': 'audio_4.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_9=\"What is the harmonic mean, and how does it enable the combination of precision and recall into a single metric?\""
      ],
      "metadata": {
        "id": "l5VV7vUTDDoA"
      },
      "id": "l5VV7vUTDDoA",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_9= lotr.get_relevant_documents(query_9)\n",
        "docs_9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhDvAEwHDRSu",
        "outputId": "2e18eec7-6f16-46e2-ad50-514b1587a157"
      },
      "id": "jhDvAEwHDRSu",
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"But there may be more subtle things. Somebody might be talking about events, some kind of pop music events, and another person may be talking about some spiritual gatherings. And one person may be interested in pop music events, and another person may not be. And another person may be interested in spiritual gatherings, and the first person may not be. So then you can train these things. So in that sense, which words signal junk and which words don't signal junk is also a parameter of the model. Just like in this linear fit, the shape of the line is a parameter. So this is the learning part. So what we are going to do is look at different types of models, as I said, and then we will look at this parameter adjustment. How does the data actually determine the model, the concrete model? How do we build the best model that we can get for the given data? That's the algorithm. So we have a kind of model template. So we have a model template on this side, we have training data, and what we\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"can measure it, but you cannot. In some sense, if you could validate it, then this whole problem becomes much simpler. But the whole difficulty with machine learning is exactly this, that there is no sensible way to predict the behavior or even when it will fail. Even if you could say that this is likely to work well here and not likely to work well there that itself would be a huge improvement, not even necessarily a very precise quantification, but just some indication. But all these are very speculative. So that's a huge challenge. So there are some theoretical things you can say which we may not, may or may not get to in this course. But it's a big problem. This is the so called generalization problem. So that's what basically, if you build association rules, is it going to hold or not? I don't know. Okay, sir, I had a question. We have been talking about items and transactions, but we haven't anywhere spoken about whether one customer, suppose if you're talking about somebody\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content=\"that if you can design such a function where you can. So what are you trying to do? You're just trying to associate a quantity given the split of yes and no, you're trying to associate a numerical quantity with that which is different from just the ratio of the minority. The ratio of the minority has this linear thing. You're trying to find a different function which gives you a better nonlinear curve. So it turns out that there are two such functions which have been studied. So one comes from information theory and it's called entropy, and the other one comes from economics and it's called genie index. So let's just quickly look at what these are. So entropy comes from information theory. So it comes from a fundamental piece of work by Claude Shannon related to how many bits you need to transmit a message. So what Shannon said is that you need to really look at the relative frequency. So if you are taking a typical message, which is, say, written in a natural language like English,\", metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"that these are not the only things so people also talk about. So there is, for instance, the corresponding thing to precision for the negative part is specificity. Right? So what is the ratio of correct negative results compared to all negative results as a true negative? So that's with respect to this column, then accuracy, of course, is just saying how many of the correct things are there with respect to the whole thing. So that also can be measured easily from this. And then there's something called thread score and so on. So I'm not going to talk about it. And just for completeness, I will mention that sometimes people want to say that we want a single number. It's not good to give me a precision separately and a recall separately and let me choose, I want a single number. There is this thing called harmonic mean. So the harmonic mean is one upon p plus one upon R. So remember, p is precision one and it's one upon that. So it's the reciprocal of the mean of the reciprocals. So if\", metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"not be just two ways. So the idea is that these are kind of more or less binary. Either it is junk or it is not junk. Either you pay or you say it's a fraud, but this is something which is a multicade. So it's not just yes or no, it is no. And then in yes, there are two parts. Another similar thing could be that if I try to tell you whether this news item is sports or arts or it is a book review, so if I look at some news feed which is coming from some press agency, and I want to put it in the right place, I need to know whether it's about politics or sports or something. So, topic classification of documents is also a multi category classification topic. So, as we said, these models that we use are of different kinds, and each of them is, there are two parts to it. There is a generic model, and then there is some specific part of it which needs to be tuned according to the data. That is the learning part. So we have to fit the parameters to the model. So, for instance, the simplest\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"1 million items, ten to the power six, one with 60. So this is 1 million, this is 1 billion. So we have a large number of transactions. And now we need to remember we are only dealing with the frequent item set problem. So the confidence level is not yet an important factor for us. We are just trying to find this thing. All the z such that z dot count is bigger than sigma times m. So the only factor that is important to us right now is the sigma. So sigma, let's assume, is very small. Sigma is just 1%. We just want to know which subsets appear in 1% of the transactions. So as we said before, if you actually naively count the possible subsets that you have to keep track of, then it will be for every size up to ten, right? Because we have at most ten items. For every size up to ten, we have to choose that many items, I items from the 1 million. Each of those is a different subset. So we will have that many possible subsets. If you are counting every possible subset, on the other hand,\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content=\"is actually easier to compute. And if you look at, as we will see, libraries in Python and all that, typically they use genie index. So although there is a very famous system built using entropy, which was one of the early decision tree systems, the ones that we will practically encounter in Python, libraries actually use the genie index. Basically. I'll stop with this right now. So what we are saying now is that we want to choose the next question to evaluate the next question to ask. So for the next question to ask, we have to say something about the quality of the impurity before and after. So if I just look at the raw ratio of the impurity, I have a motion of just that impurity percentage. But that gives me this linear relationship, which apparently is not that good in practice. So I want a nonlinear way of giving some impurity value to this ratio of yes or no, and then we can use either this entropy idea or this genie index, and it gives me a sharper curve, and this sharper\", metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"precision one and it's one upon that. So it's the reciprocal of the mean of the reciprocals. So if you work this out, you get two PR on two p plus. So this is sometimes called the f score. Right? So this is a way of combining the precision score and the recall score into a single number. But as I said, in most cases, it really depends on the application. So either you want something based on the context which has higher precision or higher recall, and you choose. So these are some of the ways in which we can actually. So, remember, our starting point was that we wanted to evaluate classifiers. So we said first we have a strategy for evaluating classifiers, so we can test them by using this test set and so on. But what is it that you want to measure on the test set? So that we have this wide variety of things, but most commonly this precision and recall are the ones that people are interested in, and they will choose which one to emphasize. Depending on the context. Depending on the\", metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"of elearning platforms, there's a lot of emphasis on trying to structure topics in such a way that students who find difficulty, you can identify why they are facing difficulties. So you want to think of these concepts that you're teaching as these items. And basket is now a set of concepts. And if you look at a concept which is difficult to learn, maybe there is a connection to another concept which is also difficult to learn. So you might want to group this. So people who misunderstand a also misunderstand b. And therefore maybe the problems has to start by fixing b. And another place where this is used is in kind of similarities of documents. So if two documents share words, then you can think they are similar. So you can think of documents. Or if two words appear rather in many documents, you can think they are similar. So if you say that wherever x appears, y also appears, then it may mean that x and y are actually related concepts. So this is the context. So the real question is\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"what you're really looking for is correlations between sets of items across transactions, which we formalize in terms of these association groups. X implies y. And what we saw is that as such, combinatorially, this is a very complicated thing to calculate, given the sizes of the sets involved. But we can use this appropriate principle both to identify the sets which are interesting, those which are frequent enough to be considered, and also to decompose them into rules. So you use the approoriate twice, once to find the frequent z, and second to decompose each frequent z into possible x implies y. And using this, we can build our first simple supervised learning model. So these rules now become a very simple way of prediction. Now, if you give me a document, I will look at what words are there in the document in this previous example. And if it contains the word student and school, I will predict education. If it contains game and team, I'll predict sports. Now, of course, if it has\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content=\"tabulating the answer. So I just mug it up in a very literal sense. But now, if you give me a new combination which is not there in this table, which I have not memorized, I don't have any principle way of giving you chance. So this model, in the terminology of machine learning, does not generalize very well. So generalize in this sense means that if I make some number of mistakes on my training data, and if you give me some random new data, I should make a correspondingly similar ratio of mistakes, which, of course, comes back to this question of how do I know which mistakes are made? Process is part of that. But this is the fundamental thing, right? I want to trade off this optimization. I don't want to make it fit my training data very closely, because then maybe it is learning things which are peculiar to the sample, which I have, which are not representative of the general case. So I might want to intentionally, in some sense, make mistakes so that I generalize well. And then, of\", metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"in, and they will choose which one to emphasize. Depending on the context. Depending on the context, you may want precision predominant. You may want recall to be predominant. So there, it really becomes a domain specific. So I'll stop there. Any questions, sir? Like, given a precision or recall value, what we need to change in the algorithm so that we could come to the certain value of precision and recall. So that is a harder question. Right. So what is it that you need to change? So that's a very difficult question. All you can say is that given what we are doing now, this is what, then it becomes really an experimental issue. So this is the hardest question in machine learning, which is, how do I improve the quality of the classifier? So there is no answer to that question, which is obvious. So you have to start experimenting. You have to do something. So there is something called feature engineering. You might want to come out with some combination of features. So some of you may\", metadata={'source': 'audio_4.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_10=\"What are the implications of choosing a large or small learning rate?\""
      ],
      "metadata": {
        "id": "kz_JCKXxDU3G"
      },
      "id": "kz_JCKXxDU3G",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_10 = lotr.get_relevant_documents(query_10)\n",
        "docs_10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfyyY4rTDbHW",
        "outputId": "24451bb3-9adb-470a-dcc1-73d0e088ee8d"
      },
      "id": "rfyyY4rTDbHW",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"lot of well trained examples? Because the assumption is that the labels that these training samples have are somehow correct so they can be trusted. Because if you can't trust those labels, and the model that you build will obviously not work. So if you want to write generalization of the labels that people have given you, you have to trust those labels. So that means somebody reasonably conversant with the domain has to be willing to put in the time to label it. Now, if it happens, as I said, because historically you're collecting data, it's fine. But more and more, we are finding that this is not the only source of data. A lot of data now actually comes out of automatic things like we have systems which are producing. For example, if you run a computerized device like a network switch or something like that, all these things generate a lot of diagnostic data. Even things like cars and planes and vehicles, which run with a lot of electronic components, generate a lot of diagnostic\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"an algorithmic problem. It's not technically a learning problem, because this is a fixed problem once we fix these quantities. So given the set of items and the transactions, and given these two thresholds, find every x y which satisfies these two constraints. This is what we are trying to do. So we will see what it means for learning at the end. Sir, I had a question. So therefore, what we said is that a rule is interesting only if this ratio is bigger than sigma. Right? The fraction of transactions where we see the target of the rule must be at least sigma. If I take m to the other side, this is same as saying there's a number of transactions with x and Y. So this is just the number of transactions. X and y exceeds sigma times the total. It's some whatever, 1% of the total, 10% of the total, and so on. So what we said last time is that this would be our first criteria. So we want to find x union Y, which is plausible. Once we have x union Y, which is plausible, we will try to break\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content=\"what you're saying is that this is a representative sample. So you're not saying that the examples have every possible situation that you are likely to use for judgment, but you're really talking at the level of probabilities. You're saying that the probability distribution of the training is somehow identical to that of the unseen data on which you are going to make the prediction. So this is going to be your training and this is going to be your prediction. So this is a fundamental assumption. If this assumption does not hold, then whatever you try to do doesn't really have any basis. So this is something that we have to assume, and we will talk about later on what this assumption means. Maybe not in this course, but in later courses. But for now, let us assume this. So the next question is, what does it mean to learn? Right? So, as we saw in this loan data set, 60% of the loans were granted, so nine by 15. So if I were to just say yes, supposing I don't have an interesting model\", metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"you have, the usual practice is about 20% to 30% of it you keep aside. So you use about 70% to 80% of the data that you are given to build a model, and you keep a random subset of 20% to 30% away from the model building process. So it is unknown as far as the model or unknown data. It's the same as getting new data from the real world in some sense, and you can use it to validate. So, of course, this is a little bit of a disadvantage, because one of the features I mentioned right at the beginning is that typically machine learning expects that the model improves in quality with the amount of training data you give it. So you are in some sense artificially, or because you have no choice, more or less. You are building a suboptimal model, you have 100% data, but you are forcing yourself to use only 70% to 80% of the data available to build the model, because you need to validate it against some unknown data. So, one other strategy where you don't want to, I mean, one of the other\", metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"the jazz is. That's what people spend up spending a lot more time on. And we will also probably spend, in terms of lecture hours, more time on supervised learning than on unsupervised learning. But don't forget that the bread and butter work of machine learning actually rests on a strong foundation of unsupervised learning. Okay, so I'm going to start with something after this. So if there are any questions at this point, I'll just take a minute to pause and ask, then continue. Okay, you. Fine. So now let's start our first topic. So this is a topic which is actually of historical interest because it is how this whole idea about learning from data and data driven decision making, as it's called, came into the forefront. So this is something which was originally suggested in the context of retail people who run supermarkets and department stores, that their various aspects of their business could be improved if they had information about this kind of thing. People who buy one product\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"can measure it, but you cannot. In some sense, if you could validate it, then this whole problem becomes much simpler. But the whole difficulty with machine learning is exactly this, that there is no sensible way to predict the behavior or even when it will fail. Even if you could say that this is likely to work well here and not likely to work well there that itself would be a huge improvement, not even necessarily a very precise quantification, but just some indication. But all these are very speculative. So that's a huge challenge. So there are some theoretical things you can say which we may not, may or may not get to in this course. But it's a big problem. This is the so called generalization problem. So that's what basically, if you build association rules, is it going to hold or not? I don't know. Okay, sir, I had a question. We have been talking about items and transactions, but we haven't anywhere spoken about whether one customer, suppose if you're talking about somebody\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content=\"this, and therefore. Or you say, number two is this and number seven is that. And therefore, in any context, generally, a smaller model is usually better, but everything is. And this is a common feature of anything we do, machine learning. Nothing is. You cannot say it universally, as I said, like newtonian mechanics and quantum mechanics, you could always have a situation where the more complicated model is required. So, yes, certainly small trees are more explainable. It'll turn out that small trees also solve the other problem that we mentioned earlier about generalization. So, question is whether or not this model is better for the data you have not seen compared to the data that you are training on. So is it too specific for the training data that you are building. So if you ask more questions, then you are really focusing on more individual eto synchronicity or complications in the data that you see. That might be an isolated case, and it may not appear in general, so maybe you\", metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"gain by the scattering of that attribute. So if we have high gain divided by high scattering, then the net normalized effect is lower. So you might choose an attribute which actually gives you a lower information gain overall, because the attribute itself is less static. And that's the thing that we want to achieve. Just to emphasize that when you have a machine learning algorithm, see generally, what is a machine learning algorithm doing? It's trying to describe the shape of the model based on the training data. So in this case it's literally the shape, because you have many possible trees you could construct, and the actual tree you construct depends on the data. And because you don't have any preconceptions about the data, you have to make sure that if there are these kind of patterns which disturb your algorithm, you will be able to counteract. So that's the purpose of this information gain ratio. It's not just the information gain alone, which is what we had initially. It's\", metadata={'source': 'audio_4.txt'}),\n",
              " Document(page_content=\"yet, first of all, a truly learning problem. It's more an algorithmic problem, but still, it's an interesting problem nonetheless, because I just want to illustrate how it affects the way in which we calculate what might be trivial with small data becomes nontrivial with large data. So this is the problem. So given a set of items, capital n, which is large, and a, given a set of transactions m, which is again large, and given these two ratios between zero and one, find every pair x and y such that x implies y is a valid association. So we can break up this thing into two steps. I mean, we want to first check whether x implies y is worth looking at at all. So we first look at the support part. We want to know whether x, sorry, yeah, we want to know whether the count divided by m is bigger than the support, which is the same as taking this m to the other side and saying whether the count is bigger than a certain fraction of the total. So the first idea is to identify those sets whose\", metadata={'source': 'audio_1.txt'}),\n",
              " Document(page_content=\"what you're really looking for is correlations between sets of items across transactions, which we formalize in terms of these association groups. X implies y. And what we saw is that as such, combinatorially, this is a very complicated thing to calculate, given the sizes of the sets involved. But we can use this appropriate principle both to identify the sets which are interesting, those which are frequent enough to be considered, and also to decompose them into rules. So you use the approoriate twice, once to find the frequent z, and second to decompose each frequent z into possible x implies y. And using this, we can build our first simple supervised learning model. So these rules now become a very simple way of prediction. Now, if you give me a document, I will look at what words are there in the document in this previous example. And if it contains the word student and school, I will predict education. If it contains game and team, I'll predict sports. Now, of course, if it has\", metadata={'source': 'audio_2.txt'}),\n",
              " Document(page_content=\"granted, so nine by 15. So if I were to just say yes, supposing I don't have an interesting model and I just say yes all the time, every answer that every person who comes, I just say yes, give this person. And if the previous assumption is actually valid, that the distribution of training examples and the distribution of unseen data are actually the same, then if I used to give 60% of the training people loans, then giving 60% of the unseen data loans is something that I would expect to do. So if I just keep on saying yes, then 60% of the time I would be correct, which might suggest that you're doing better than half, but actually the answer is biased, so you're not doing any better than random guessing. It's like if I toss a fair coin and I keep saying heads all the time, then I should expect to call it right 50% of the time. So doing 50% of the time in a fair situation is no better than random guessing in this kind of situation, where you know that the answer is biased because the\", metadata={'source': 'audio_3.txt'}),\n",
              " Document(page_content=\"against some unknown data. So, one other strategy where you don't want to, I mean, one of the other disadvantages of doing this is that maybe that there are some minority anomalies in this data, and the choice of test data that you have made might have hidden all these anomalies, so you never see them, and maybe it's important to see them. So there may be many situations, or maybe you just don't have enough data as a whole to build a good model by only looking at 80%. So another strategy is to systematically do this with different subsets. So what you're really asking at some higher level is that machine learning approach. So, remember, we have seen only decision trees, but we are going to see many models. So there are clearly many ways to build models. And the reason, whenever you see that there are many ways to do something, it's only because there is no guarantee that a given way is the best one. So more or less what you want to validate is, I have, say, seven different strategies\", metadata={'source': 'audio_4.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for chunks in lotr.get_relevant_documents(\"What is supervised Learning\"):\n",
        "    print(chunks.page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHTARGQt7sjS",
        "outputId": "ff39498c-c636-42c0-ccad-05db12539fe9"
      },
      "id": "XHTARGQt7sjS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "are looking for patterns, but you don't really have a clear idea beforehand what patterns they are. So a typical example of this might be that a company which is selling something might want to know some information about the demographics of its customers. So what are the groupings? I mean, which age groups? What is the proportion of people who are, say, between 20 and 30 who buy their products, people above 50 who buy their products, and so on. So this kind of thing is unsupervised because you don't know what you're going to get. But after you get this information, maybe you can put it to good use. So these are broadly the two types of things that are there. There is a third type of machine learning called reinforcement learning, which we are not going to talk about at all in this course. So to look at supervised learning a little more detail, we will of course do it in much more detail in the lectures to come. So we are trying to extrapolate. So basically it's a prediction problem,\n",
            "a slightly different presentation of the idea. Okay, so now, as I said, this whole thing so far has been largely a kind of algorithmic problem, right? In some sense, if I fix the set of items and the set of transactions and the set of the two thresholds, the support and the confidence threshold, then for any x, comma y, either it is true or it is not true. So there is a fixed answer. So the answer is known. So this is something where I can actually validate that you have either enumerated all the association rules or you have missed out something. So technically there is no learning involved. It's more an algorithmic question. So how does one connect this to learning? Well, in specific context, you can interpret these rules as classification rules. So, remember we said supervised learning says, if this is there, then it is a fraud. If this thing is not there, then it is not a fraud, and so on. So, let's look at a typical example. So, supposing we have some question which is about\n",
            "whatever, income, stuff like that. So each row in the table will be one item, and there will be a special column which indicates what category, depending on the classification problem that we are dealing with, what category it is. And the goal of supervised learning is to now construct, given a new row, a new item, a new combination of these attributes, which is not seen before, try to predict what would be the most appropriate category to assign. So these items which are given with the labels. So these are called labels. So these items are called training data. So we have labeled training data, which consists of some information which has been collected in the past, some historical information about items and their classification. And we want to find a model that generalizes the training data. So since normally this categorization corresponds to some kind of classification, this is typically called a classification problem. So classification problem could be a binary problem. It\n",
            "So last time we started with supervised learning in earnest, and we were looking at this decision tree model. So, in a decision tree model, you have essentially a table in which the columns are the. So each item, a row in the table and one special column denotes the class that you are trying to classify. And we assumed it's a binary classification. So the class is zero, one, or yes, no, or whatever you want to interpret. The two categories are. So the algorithm was very simple. At a high level, what you want to do is choose an attribute based on the answer to the question that you're asking about that attribute. The data splits, and then you take each partition, ask another question. So that's the thing. And you want to come down and keep asking questions until either you have reached, like these situations here, a partition in which all the values of the class variable are the same. So you have a uniform class level or you have run out of questions, because we can only ask in any path\n",
            "So remember, these x I's are actually not unknowns. These x I's known. So these x I's are what you get. The theta I's are the unknowns that you are trying to figure out. But in terms of the theta I's you can predict for a given xi. If you have computed some theta I's, then the answer is summation theta I x. So therefore the target that we have is clearly to find the best line. So let's assume we have a simple case of only one variable. So we have x one, x two, and so on. So we have n points. In reality, each of these x size is a vector of k, and the y, by convention, is always the label. So in the earlier thing, when we looked at that scikitlearn decision tree thing, we used capital x to denote the input matrix and y to denote the column matrix of output. So this is a normal standard convention in supervised learning that x's denote the inputs of the training data and y denotes the output label. In this case, it's not a label, but it's an output value. So in order to compute the best\n",
            "model, or a numerical prediction model. As we said, in case you're trying to predict a number or category, the category is more like a classification problem, right? I give you something and I want you to classify it as yes, no, good, bad, or we have unsupervised learning where we are looking for some kind of structure, and usually this is in the form of clustering, but we also have to deal with these outliers, as I said. And then sometimes you want to simplify the problem by knocking off certain dimensions or unrolling the object or so. So if you look at the popular press and the popular literature, all the focus of machine learning is actually on the first one. So supervised learning is by far the more attractive and the more, should I say, attractive, I guess it's the more attractive aspect of machine learning. This is what people are really looking for. But unfortunately, as I said, to do effective supervised learning, you need to actually get a lot of training data. And this is\n",
            "its topic is education. And if it has game and team, then it is. So this is a special kind of association rule, where the right hand side of the rule is always a singleton, and it corresponds to a special type of item which is a category, in this case a topic. So when you have a table like this, the rows may have different number of entries. Or if you want to think about it as a table, then some columns may be blank. So not all columns are filled, but there's one last column which is filled. And now you want to now construct these rules corresponding to the first k minus one columns, implying the last column. And this now becomes something where these rules that you get out of this give you a kind of classifier. So this is a kind of a simplistic way of doing supervised learning. So that kind of brings me to the end of this basic treatment of classes, this market basket analysis. So what you're really looking for is correlations between sets of items across transactions, which we\n",
            "So we were looking at this market basket analysis, and towards the end of the discussion on association rule rules, if everybody who buys X is also likely to buy y. We said that one application of this association rule idea is to have these so called class association rules. So the right hand side was a category. We looked at an example with documents and we said, if a document has these words, it is also likely to belong to this category. So that brings us to the topic that is one of the central ones in machine learning, which is the idea of supervised learning. So in supervised learning, we have a set of items, and each item belongs to some category or class, and we distinguish these items in terms of some attributes. So you can think about this, as we will see shortly, you can think about these attributes as columns, right? So the attributes will be things like name, age, date of birth, whatever, income, stuff like that. So each row in the table will be one item, and there will be a\n",
            "set of model predictions. What fraction of these predictions are correct? So, this is what's called accuracy. So, on how much of that test set that I kept aside, on what fraction of the test set did my model make the right prediction? So, the problem is that very often the choice, let us stick to our assumption that we are doing a binary prediction. The choice between the two parts is not symmetric. So the thing we are looking for is very often a minority. So this is a graphic. Blow it up a little bit. So this is saying that if I look at credit card fraud, which is one of the typical examples of where supervised learning is used, right? So we look at different parameters of the transaction, where it was made, what time of day it was made, and how does it compare to other transactions made by the customer and whatever, and the bank wants to know if it's a fraud. So if you actually look at the attempted fraud. So this is going into the future, it's going into 2027. But let us look at\n",
            "to show you some code before we come back to theory. So, is there any further questions? So, at the moment, I'm not expecting you to do anything with this code other than to understand that a, it is relatively painless to invoke a model in scikitlearn. And if you have this kind of visualization code, either you write it yourself or somebody gives it to you. You can then interpret your models visually and then experiment with it by changing things and seeing what happens. And that gives you a fair amount of insight both into your data and into how the model is working. So it gives you some indications about the limitations of that particular. Okay, so now I'm going to do something hopefully not unfamiliar to you. Right? At the beginning, when we talked about supervised learning, we said that we are trying to predict something, and those predictions can be in two types. So one type is what we have seen in the decision tree, which is a classification. So the predicted value is one from a\n",
            "to do effective supervised learning, you need to actually get a lot of training data. And this is sometimes expensive. Very often it is expensive. So unsupervised learning is often the starting. A few. Couple of years back, there were three turing awards given for deep learning. So one of them was given to Jan Lakun. And his observation is that if you think of this whole thing as a cake, then actually most of the cake is actually unsupervised learning. So supervised learning is just a thin layer on the top, but it's like the icing on the cake. So when we go to a bakery and we see an attractive cake, what we're attracted to is the surface, the icing surface. But if the inside is made of sodas, then the cake will not taste good. So you have to be careful not to be fooled by the icing. So the unsupervised learning is also important. But supervised learning is where all the, in some sense, the jazz is. That's what people spend up spending a lot more time on. And we will also probably\n",
            "what you're really looking for is correlations between sets of items across transactions, which we formalize in terms of these association groups. X implies y. And what we saw is that as such, combinatorially, this is a very complicated thing to calculate, given the sizes of the sets involved. But we can use this appropriate principle both to identify the sets which are interesting, those which are frequent enough to be considered, and also to decompose them into rules. So you use the approoriate twice, once to find the frequent z, and second to decompose each frequent z into possible x implies y. And using this, we can build our first simple supervised learning model. So these rules now become a very simple way of prediction. Now, if you give me a document, I will look at what words are there in the document in this previous example. And if it contains the word student and school, I will predict education. If it contains game and team, I'll predict sports. Now, of course, if it has\n",
            "a dog picture from a cat picture, or one which is neither of the two, it's not very easy to quantify that. And there the attributes in a picture are typically just if you think of a picture as a digital picture, it's a bunch of pixels and their colors and their arrangement, their relative arrangement. So even I'm saying, even if you have some domain knowledge, it's not very clear that the person's domain knowledge is aligned to the way the data is available. So what we are going to be doing is agnostic to the domain. So we are not going to believe anything or assume anything about the domain, except that we will want to validate at the end somehow that what we are doing is right. But in practice what you're saying makes sense that really to do supervised learning, it's much more realistic to do it in collaboration with somebody who understands a domain rather than just doing it blindly. Because also when you do it blindly, you might come up with solutions which make no sense. You\n",
            "against some unknown data. So, one other strategy where you don't want to, I mean, one of the other disadvantages of doing this is that maybe that there are some minority anomalies in this data, and the choice of test data that you have made might have hidden all these anomalies, so you never see them, and maybe it's important to see them. So there may be many situations, or maybe you just don't have enough data as a whole to build a good model by only looking at 80%. So another strategy is to systematically do this with different subsets. So what you're really asking at some higher level is that machine learning approach. So, remember, we have seen only decision trees, but we are going to see many models. So there are clearly many ways to build models. And the reason, whenever you see that there are many ways to do something, it's only because there is no guarantee that a given way is the best one. So more or less what you want to validate is, I have, say, seven different strategies\n",
            "things, you break it up into 1000 batches. So you do 1000 points. So you get some averaging out of the behavior of the function, make an adjustment, then you do another 1000, make an adjustment and so on. So all these things are actually used, we will see later in neural networks and all that. But in this linear prediction also, that's where they arise. I mean, that's the origin of this. So this is, of course, as you have all probably read. So this strategy is conventionally called linear regression, but it's basically this iterative linear prediction, which you can do theoretically by a matrix operation, but computationally, it's more feasible to do it in this. So I'll stop here, and we'll continue next time to discuss why we are using some square error and also what to do when you have nonlinear things, how to deal with nonlinear approximations. Having done decision trees, I think at the next class, at the end of the class, the last 15 have a small moodle quiz covering up to last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install cohere"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVDvdN1E-Y8O",
        "outputId": "a911f02f-fe30-40de-87fb-02e2653dc642"
      },
      "id": "RVDvdN1E-Y8O",
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cohere\n",
            "  Downloading cohere-5.2.2-py3-none-any.whl (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.3/144.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
            "  Downloading fastavro-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx>=0.21.2 (from cohere)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.6.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.16.0,>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.15.2)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
            "  Downloading types_requests-2.31.0.20240406-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (4.10.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx>=0.21.2->cohere)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.2->cohere) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.2->cohere) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (2.0.7)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<0.16.0,>=0.15.2->cohere) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<0.16.0,>=0.15.2->cohere) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<0.16.0,>=0.15.2->cohere) (2023.6.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<0.16.0,>=0.15.2->cohere) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<0.16.0,>=0.15.2->cohere) (6.0.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<0.16.0,>=0.15.2->cohere) (23.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.2.0)\n",
            "Installing collected packages: types-requests, httpcore, fastavro, httpx, cohere\n",
            "Successfully installed cohere-5.2.2 fastavro-1.9.4 httpcore-1.0.5 httpx-0.27.0 types-requests-2.31.0.20240406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cohere\n",
        "co = cohere.Client('w8CnnlzVol2aZEiirZNLUs0onAqXUUYBZCw2Oj7g')"
      ],
      "metadata": {
        "id": "J4ttbycMADKK"
      },
      "id": "J4ttbycMADKK",
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Query 1 Merge Retriever with Cohere Reranker"
      ],
      "metadata": {
        "id": "oLoMTId7DnZp"
      },
      "id": "oLoMTId7DnZp"
    },
    {
      "cell_type": "code",
      "source": [
        "query_1=\" What are some challenges associated with data collection?\""
      ],
      "metadata": {
        "id": "DGs9YGCdD3Hw"
      },
      "id": "DGs9YGCdD3Hw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_query1=[]\n",
        "for i in range(len(docs)):\n",
        "  retrieved_docs_query1.append(docs[i].page_content)"
      ],
      "metadata": {
        "id": "Lmr1glvTDTH_"
      },
      "id": "Lmr1glvTDTH_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = co.rerank(query=query_1, documents=retrieved_docs_query1, top_n=3, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
        "for idx, r in enumerate(results):\n",
        "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
        "  print(f\"Document: {r.document['text']}\")\n",
        "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmHIQ_JuDwk7",
        "outputId": "e72b8f0e-c620-420c-cec8-93c8820ea53e"
      },
      "id": "KmHIQ_JuDwk7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document Rank: 1, Document Index: 0\n",
            "Document: entered by somebody and then converted to electronic forms. So that would be, there would be two levels of potential sources for errors. The person writing down the information and then the person typing in the information. Now, gradually these kind of electronic forms are spilled in directly, so at least the source of the error is reduced to one step. But still, people mistype things. I mean, there are any number of situations where people type their email address wrong and so notifications don't reach them and so on. So there is this data collection. How do you collect the data and how do you clean it? And the third thing is, how do you make it uniform? So when data is being collected by different people, they may collect different things. And for instance, if you look at the government, typically the government collects data in different forms. For instance, there is a public distribution system which the ration shops, so they collect some information about who is collecting ration\n",
            "Relevance Score: 0.92\n",
            "\n",
            "\n",
            "Document Rank: 2, Document Index: 12\n",
            "Document: we have to realize that different people with the same bit different symptoms. So conversely, two people with the same symptoms don't necessarily. So there are asymptomatic COVID patients. There are people who have a sore throat and a cold and a headache who don't necessarily have COVID. So I cannot just say that if you have these symptoms, you must have COVID, and if you don't have these symptoms, you cannot have COVID. Right. So clearly there is something going on in the background, and that background in our case is revealed through a test. Even tests have errors and all that. So it's really a problem that we have to grapple with in real life. And we will just assume for now that when we reach such a situation, we just take a majority decision. But realistically, if we get too many of these situations, and it's very difficult to decide, we have to ask whether the data has actually been collected correctly. So as we said, the main issue is really how to ask the questions and what\n",
            "Relevance Score: 0.76\n",
            "\n",
            "\n",
            "Document Rank: 3, Document Index: 2\n",
            "Document: been collected correctly. So as we said, the main issue is really how to ask the questions and what this means. Sorry, sir, I have a question. Yeah. Sir, once we know that the data is young, we are making a prediction and it come out that young. So like other attribute is not relevant other than it has job or not, and then we are not going to look at other attributes. Yeah, but remember that which attributes to look like vary from one path to another. So maybe in this case, right, so what we are saying is that if they are young, we only need the job, right? But if they are middle aged, we need to know the house. So for different combinations, so they may not occur together. I agree, in any decision, but each of these plays a role somewhere. Now, if there is another attribute which never plays a role, as we will see the next tree as that example. Right? So if you look at this, this is a different tree for the same data set. So instead of first asking the age, we ask whether you own a\n",
            "Relevance Score: 0.62\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Query 2"
      ],
      "metadata": {
        "id": "__SC-vghERKz"
      },
      "id": "__SC-vghERKz"
    },
    {
      "cell_type": "code",
      "source": [
        "query_2=\" What are the advantages of the Apriori algorithm?\""
      ],
      "metadata": {
        "id": "_qhcYE8TEUpX"
      },
      "id": "_qhcYE8TEUpX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_query2=[]\n",
        "for i in range(len(docs)):\n",
        "  retrieved_docs_query2.append(docs[i].page_content)"
      ],
      "metadata": {
        "id": "XpbLdyknBEcD"
      },
      "id": "XpbLdyknBEcD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = co.rerank(query=query_2, documents=retrieved_docs_query2, top_n=3, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
        "for idx, r in enumerate(results):\n",
        "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
        "  print(f\"Document: {r.document['text']}\")\n",
        "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaFPkfrDE649",
        "outputId": "ff03e71f-7cb6-496d-9905-6f70ceeac7d6"
      },
      "id": "IaFPkfrDE649",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document Rank: 1, Document Index: 3\n",
            "Document: against some unknown data. So, one other strategy where you don't want to, I mean, one of the other disadvantages of doing this is that maybe that there are some minority anomalies in this data, and the choice of test data that you have made might have hidden all these anomalies, so you never see them, and maybe it's important to see them. So there may be many situations, or maybe you just don't have enough data as a whole to build a good model by only looking at 80%. So another strategy is to systematically do this with different subsets. So what you're really asking at some higher level is that machine learning approach. So, remember, we have seen only decision trees, but we are going to see many models. So there are clearly many ways to build models. And the reason, whenever you see that there are many ways to do something, it's only because there is no guarantee that a given way is the best one. So more or less what you want to validate is, I have, say, seven different strategies\n",
            "Relevance Score: 0.35\n",
            "\n",
            "\n",
            "Document Rank: 2, Document Index: 6\n",
            "Document: what you're really looking for is correlations between sets of items across transactions, which we formalize in terms of these association groups. X implies y. And what we saw is that as such, combinatorially, this is a very complicated thing to calculate, given the sizes of the sets involved. But we can use this appropriate principle both to identify the sets which are interesting, those which are frequent enough to be considered, and also to decompose them into rules. So you use the approoriate twice, once to find the frequent z, and second to decompose each frequent z into possible x implies y. And using this, we can build our first simple supervised learning model. So these rules now become a very simple way of prediction. Now, if you give me a document, I will look at what words are there in the document in this previous example. And if it contains the word student and school, I will predict education. If it contains game and team, I'll predict sports. Now, of course, if it has\n",
            "Relevance Score: 0.32\n",
            "\n",
            "\n",
            "Document Rank: 3, Document Index: 8\n",
            "Document: have to do a search through all possible trees. So instead we wanted to use some heuristic. So the main question that we have to ask is, which attribute do we query next at any stage. So our goal is to achieve this partition, which is pure, where all the values are yes or all the values are no. So we try to accelerate towards that. So that is this greedy strategy. So we use a heuristic, which will reduce the impurity as much as possible. And when we move from one node to its children by asking a question, what we do is we compute a weighted average. So for each node we can compute the impurity. But then when we combine the children of a node to get the impurity at the next level, we use a weighted average. So going simple weighted average just assigns a weight one to every item. So you just take the fraction. So for node one, you take the fraction of instances in that partition, multiply that fraction by the impurity and so on, and add it up, and we choose the one which reduces. So\n",
            "Relevance Score: 0.30\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Query 3"
      ],
      "metadata": {
        "id": "ItFKtqngFyYB"
      },
      "id": "ItFKtqngFyYB"
    },
    {
      "cell_type": "code",
      "source": [
        "query_3=\" What distinguishes training data in supervised learning, and what is its purpose? \""
      ],
      "metadata": {
        "id": "pby-49QVF4l6"
      },
      "id": "pby-49QVF4l6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_query3=[]\n",
        "for i in range(len(docs)):\n",
        "  retrieved_docs_query3.append(docs[i].page_content)"
      ],
      "metadata": {
        "id": "NIdqvsLnGVuv"
      },
      "id": "NIdqvsLnGVuv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = co.rerank(query=query_3, documents=retrieved_docs_query3, top_n=3, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
        "for idx, r in enumerate(results):\n",
        "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
        "  print(f\"Document: {r.document['text']}\")\n",
        "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBLz3hS3F2m-",
        "outputId": "8a5069b8-fe6c-44cc-98dc-7d84cba1a422"
      },
      "id": "hBLz3hS3F2m-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document Rank: 1, Document Index: 2\n",
            "Document: whatever, income, stuff like that. So each row in the table will be one item, and there will be a special column which indicates what category, depending on the classification problem that we are dealing with, what category it is. And the goal of supervised learning is to now construct, given a new row, a new item, a new combination of these attributes, which is not seen before, try to predict what would be the most appropriate category to assign. So these items which are given with the labels. So these are called labels. So these items are called training data. So we have labeled training data, which consists of some information which has been collected in the past, some historical information about items and their classification. And we want to find a model that generalizes the training data. So since normally this categorization corresponds to some kind of classification, this is typically called a classification problem. So classification problem could be a binary problem. It\n",
            "Relevance Score: 0.99\n",
            "\n",
            "\n",
            "Document Rank: 2, Document Index: 0\n",
            "Document: a specification, but generating a concrete program from a generic program you can think of a model template as. So I just give you a line here which says a Y equal to Mx plus c. And here I want a specific m zero and C zero, which fits the given data in the best possible way, which is also part of the description of the algorithm. What does it mean for one line to be better than another line? So that's part of it. So the other side of this picture, as I mentioned, is when you don't have this training data, you don't have any examples that people have already labeled, but you are looking for some kind of patterns. So there is no guidance. So the thing about supervised learning is somebody should have gone through and given you this information before. Now, this information could have been collected over time. Maybe it's not manually collected, like the school marks. The school marks over time. Everybody has. I mean, the school keeps a record of what happens in the model exams, what\n",
            "Relevance Score: 0.90\n",
            "\n",
            "\n",
            "Document Rank: 3, Document Index: 5\n",
            "Document: the data. So what we are trying to do is learn some kind of mathematical models from data. And this falls into two broad categories. So the first category is where you are given examples. So you are told, for instance, this is, for example, supposing you're trying to evaluate whether a painting is painted by a particular type of. So there are these pools of painting, like impressionists and cubists and so on. So you might be given a lot of pictures saying, this is cubist, this is not cubist, this is impressionist, and so on. And then from these examples which are given to you with manual information, you are supposed to build a model which will take an unknown painting and tell you from its features which of these categories. So this is a typical supervised learning kind of thing. Unsupervised learning is something which is closer to the data mining situation where you have data and you are looking for patterns, but you don't really have a clear idea beforehand what patterns they are.\n",
            "Relevance Score: 0.86\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### query 4"
      ],
      "metadata": {
        "id": "m1hd4OrnGqvo"
      },
      "id": "m1hd4OrnGqvo"
    },
    {
      "cell_type": "code",
      "source": [
        "query_4=\" What is cross-validation?\""
      ],
      "metadata": {
        "id": "EySRfbG6GqQJ"
      },
      "id": "EySRfbG6GqQJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_query4=[]\n",
        "for i in range(len(docs)):\n",
        "  retrieved_docs_query4.append(docs[i].page_content)"
      ],
      "metadata": {
        "id": "Abc2DEQrHYZ_"
      },
      "id": "Abc2DEQrHYZ_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = co.rerank(query=query_4, documents=retrieved_docs_query4, top_n=3, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
        "for idx, r in enumerate(results):\n",
        "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
        "  print(f\"Document: {r.document['text']}\")\n",
        "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsE3-bRQHe4E",
        "outputId": "ffc7bc57-03f2-4027-90c9-ad94b9fa3121"
      },
      "id": "CsE3-bRQHe4E",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document Rank: 1, Document Index: 3\n",
            "Document: distribution changes slightly, one attribute might get better entropy than another attribute. And you will choose that question first. So the shape of the tree might vary a lot between these, so you may not get the same. Okay, so the question about cross validation, cross validation. What I'm saying is that you do not make a hard and fast choice once and for all to split your data this way. So you're taking your data and you're saying, hey, first let me leave out this 20% and build one model. So I am building multiple models. I'm building m one, m two, m three, m four, and m five. Each of these models is built using 80% of the test data, but a different 80%. So each of them leaves out some 20%. So I'm building five different models and comparing what I see in all these five. So the first thing is that if this approach is good, then in general, these five models should behave in a similar way with respect to the outcome. So if they all behave similarly, then I can decide what to do. So\n",
            "Relevance Score: 0.92\n",
            "\n",
            "\n",
            "Document Rank: 2, Document Index: 8\n",
            "Document: the best one. So more or less what you want to validate is, I have, say, seven different strategies to solve this problem using machine learning. And I want to check which one of them is good. So I need to compare. So once I compare, then I'm stuck with whatever I have. It may be that this gives me only 70% accuracy or some such metric, but the other models give me even less. So it's more really a comparative statement to start with, which is, is this method actually giving me something reasonable? And is it giving me better something than something else? So, for this one strategy is so called cross validation. So what you do is you take some fraction, say, in this case, it is one fifth. So 20%. So you take your data and you leave out. So you randomize it into five groups, which preserve, in some sense, the random behavior, so they partition it five ways. So if I leave out any one of these groups, it's as though I have taken the remaining 80% for training and this 20% for test. But\n",
            "Relevance Score: 0.91\n",
            "\n",
            "\n",
            "Document Rank: 3, Document Index: 1\n",
            "Document: a fixed data set, and you are going to apply it to some other data set which you have not seen. So will that generalize well or not is the biggest question in machine learning. How do you know that the training data that you have used to build your model is actually representative of the data that you see in the real. And there could be many reasons why it is not valid. It could be because of sampling. So, for instance, you might be building a model for some consumer behavior by looking at one segment of. Maybe you are looking at the behavior of people who are in their, are building a model of how people behave, but your actual market consists of people who are maybe in their 40s, who have a very different way of dealing with things. So you need to be careful that the data that you're looking at is actually representative. So this is a huge problem and there's no correct answer. You can measure it, but you cannot. In some sense, if you could validate it, then this whole problem\n",
            "Relevance Score: 0.85\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Query 5"
      ],
      "metadata": {
        "id": "OnFLlmukHvk_"
      },
      "id": "OnFLlmukHvk_"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eE5M6rBXHu6T"
      },
      "id": "eE5M6rBXHu6T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"What is the process of building the decision tree classifier, and how is it trained on the dataset?\"\n"
      ],
      "metadata": {
        "id": "xmlHBzMRAL2I"
      },
      "id": "xmlHBzMRAL2I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = co.rerank(query=query, documents=retrieved_docs, top_n=3, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
        "for idx, r in enumerate(results):\n",
        "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
        "  print(f\"Document: {r.document['text']}\")\n",
        "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdTjpPSWB_Vp",
        "outputId": "b9ebe7d7-12b8-42a7-d83d-ef5336c0dcae"
      },
      "id": "EdTjpPSWB_Vp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document Rank: 1, Document Index: 9\n",
            "Document: and is therefore picking up some peculiarities which don't necessarily exist within. So one of the things we mentioned in passing was that we like short trees for two reasons. One is because they are easier to explain. The second thing, which I claim without any justification, is that they generalize better. So here, priority, they are saying, let us not construct deep trees. So whatever tree we construct, we are going to stop it when it reaches. So it's a two step process, right? So what you do is you first say what are the parameters for the decision tree classifier? So he says, I want a decision tree classifier to be set up with this random state 42 and which will not grow to more than depth two. And then I have to actually construct the classifier for a particular data set. So that's the next thing. So I use this fit function, right? In some sense this creates a decision tree object with certain operating parameters, and then you pass it the training data in the form of the input\n",
            "Relevance Score: 0.99\n",
            "\n",
            "\n",
            "Document Rank: 2, Document Index: 14\n",
            "Document: and two, three are the petal. So it's taking all the rows and it's taking the third and fourth columns. So we're throwing away two columns. So we are making it, instead of a four column array, we are making it into a two column array. And in this data. So when you load the data set, it automatically produces these two subparts, iris data and iris target. So target is the classification variable. Remember, in this case, the classification variable is a three way thing, right? There are three types of irises that we saw, so it will be essentially a number, zero, one. And now to get this decision tree classifier, we have to take this function that we have imported. So we say take a decision tree classifier. We have to provide it with, again some random state because it uses some randomization inside it. And now this is something which we have not talked about. So we discussed that we will build a decision tree normally until we either achieve some pure leaf nodes where the classification\n",
            "Relevance Score: 0.98\n",
            "\n",
            "\n",
            "Document Rank: 3, Document Index: 3\n",
            "Document: So last time we started with supervised learning in earnest, and we were looking at this decision tree model. So, in a decision tree model, you have essentially a table in which the columns are the. So each item, a row in the table and one special column denotes the class that you are trying to classify. And we assumed it's a binary classification. So the class is zero, one, or yes, no, or whatever you want to interpret. The two categories are. So the algorithm was very simple. At a high level, what you want to do is choose an attribute based on the answer to the question that you're asking about that attribute. The data splits, and then you take each partition, ask another question. So that's the thing. And you want to come down and keep asking questions until either you have reached, like these situations here, a partition in which all the values of the class variable are the same. So you have a uniform class level or you have run out of questions, because we can only ask in any path\n",
            "Relevance Score: 0.94\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Query 6"
      ],
      "metadata": {
        "id": "Pr4dcu8SENa2"
      },
      "id": "Pr4dcu8SENa2"
    },
    {
      "cell_type": "code",
      "source": [
        "query_6=\" What is the challenge associated with supervised learning? \""
      ],
      "metadata": {
        "id": "T4NjOsRMEgeP"
      },
      "id": "T4NjOsRMEgeP",
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_query6=[]\n",
        "for i in range(len(docs_6)):\n",
        "  retrieved_docs_query6.append(docs_6[i].page_content)"
      ],
      "metadata": {
        "id": "hiMx8_MeEhoG"
      },
      "id": "hiMx8_MeEhoG",
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(query_6,\"\\n\")\n",
        "results = co.rerank(query=query_6, documents=retrieved_docs_query6, top_n=5, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
        "for idx, r in enumerate(results.results):\n",
        "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
        "  print(f\"Document: {retrieved_docs_query6[r.index]}\")\n",
        "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIdRoAyPEw-w",
        "outputId": "8093e68b-b672-4dbe-d2ca-fa147435ec78"
      },
      "id": "AIdRoAyPEw-w",
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " What is the challenge associated with supervised learning?  \n",
            "\n",
            "Document Rank: 1, Document Index: 6\n",
            "Document: a dog picture from a cat picture, or one which is neither of the two, it's not very easy to quantify that. And there the attributes in a picture are typically just if you think of a picture as a digital picture, it's a bunch of pixels and their colors and their arrangement, their relative arrangement. So even I'm saying, even if you have some domain knowledge, it's not very clear that the person's domain knowledge is aligned to the way the data is available. So what we are going to be doing is agnostic to the domain. So we are not going to believe anything or assume anything about the domain, except that we will want to validate at the end somehow that what we are doing is right. But in practice what you're saying makes sense that really to do supervised learning, it's much more realistic to do it in collaboration with somebody who understands a domain rather than just doing it blindly. Because also when you do it blindly, you might come up with solutions which make no sense. You\n",
            "Relevance Score: 0.95\n",
            "\n",
            "\n",
            "Document Rank: 2, Document Index: 4\n",
            "Document: to do effective supervised learning, you need to actually get a lot of training data. And this is sometimes expensive. Very often it is expensive. So unsupervised learning is often the starting. A few. Couple of years back, there were three turing awards given for deep learning. So one of them was given to Jan Lakun. And his observation is that if you think of this whole thing as a cake, then actually most of the cake is actually unsupervised learning. So supervised learning is just a thin layer on the top, but it's like the icing on the cake. So when we go to a bakery and we see an attractive cake, what we're attracted to is the surface, the icing surface. But if the inside is made of sodas, then the cake will not taste good. So you have to be careful not to be fooled by the icing. So the unsupervised learning is also important. But supervised learning is where all the, in some sense, the jazz is. That's what people spend up spending a lot more time on. And we will also probably\n",
            "Relevance Score: 0.89\n",
            "\n",
            "\n",
            "Document Rank: 3, Document Index: 8\n",
            "Document: are looking for patterns, but you don't really have a clear idea beforehand what patterns they are. So a typical example of this might be that a company which is selling something might want to know some information about the demographics of its customers. So what are the groupings? I mean, which age groups? What is the proportion of people who are, say, between 20 and 30 who buy their products, people above 50 who buy their products, and so on. So this kind of thing is unsupervised because you don't know what you're going to get. But after you get this information, maybe you can put it to good use. So these are broadly the two types of things that are there. There is a third type of machine learning called reinforcement learning, which we are not going to talk about at all in this course. So to look at supervised learning a little more detail, we will of course do it in much more detail in the lectures to come. So we are trying to extrapolate. So basically it's a prediction problem,\n",
            "Relevance Score: 0.72\n",
            "\n",
            "\n",
            "Document Rank: 4, Document Index: 2\n",
            "Document: whatever, income, stuff like that. So each row in the table will be one item, and there will be a special column which indicates what category, depending on the classification problem that we are dealing with, what category it is. And the goal of supervised learning is to now construct, given a new row, a new item, a new combination of these attributes, which is not seen before, try to predict what would be the most appropriate category to assign. So these items which are given with the labels. So these are called labels. So these items are called training data. So we have labeled training data, which consists of some information which has been collected in the past, some historical information about items and their classification. And we want to find a model that generalizes the training data. So since normally this categorization corresponds to some kind of classification, this is typically called a classification problem. So classification problem could be a binary problem. It\n",
            "Relevance Score: 0.71\n",
            "\n",
            "\n",
            "Document Rank: 5, Document Index: 0\n",
            "Document: to work in two dimensions, I actually unroll it. And then I find I have to deal with these multiple bands. So now it's not so obvious anymore because I do have lines, but I have many of them, and I have to decide between which bands it is, green, and which bands it is, this yellow color. So the peril of supervised unsupervised learning is that you don't know that the step that you are taking in advance, you don't know is going to be good or bad because it's unsupervised. You're just taking a guess in some sense, but then you have to have some way of evaluating whether the guess was good or not. And then going back and doing it again may be a different. So this is kind of a quick overview, a summary of the main things that we will be looking at. So, supervised learning and unsupervised learning. So we are looking either to build a predictive model, which is broadly a classification model, or a numerical prediction model. As we said, in case you're trying to predict a number or\n",
            "Relevance Score: 0.68\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Query 7"
      ],
      "metadata": {
        "id": "GT2BD56FHFDi"
      },
      "id": "GT2BD56FHFDi"
    },
    {
      "cell_type": "code",
      "source": [
        "query_7=\"What considerations should businesses keep in mind when applying market basket analysis to understand customer behavior?\"\n"
      ],
      "metadata": {
        "id": "XA-Z253oHDaY"
      },
      "id": "XA-Z253oHDaY",
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_query7=[]\n",
        "for i in range(len(docs_7)):\n",
        "  retrieved_docs_query7.append(docs_7[i].page_content)"
      ],
      "metadata": {
        "id": "PHf1gjMyHPPb"
      },
      "id": "PHf1gjMyHPPb",
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(query_7,\"\\n\")\n",
        "results = co.rerank(query=query_7, documents=retrieved_docs_query7, top_n=5, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
        "for idx, r in enumerate(results.results):\n",
        "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
        "  print(f\"Document: {retrieved_docs_query7[r.index]}\")\n",
        "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jezTXfkGHVuE",
        "outputId": "53e99205-ead4-4bda-a9ad-849656e7d8ee"
      },
      "id": "jezTXfkGHVuE",
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What considerations should businesses keep in mind when applying market basket analysis to understand customer behavior? \n",
            "\n",
            "Document Rank: 1, Document Index: 1\n",
            "Document: as a different transaction every time. But for somebody who's buying everything together, doesn't that give rise to a lot of. Yeah, so there are all these situations, I agree, which are not directly addressed in this. There are many different variations of this that you could ask. So there's no doubt about that. So some of them people have looked at because they have kind of natural solutions in this. Some of them maybe you cannot do. So that's another thing about this whole model building thing is that the same model may or may not be capable of tackling every different question that you ask. So this model, some of these things maybe you can segregate and answer. But if, as you're saying, you want to compare behaviors across customers of different types, you cannot, as you said, sensibly aggregate them into a single market basket model because they all have different profiles. So you will have to, in the example that you gave, you would have to first separate out the data for these,\n",
            "Relevance Score: 0.94\n",
            "\n",
            "\n",
            "Document Rank: 2, Document Index: 0\n",
            "Document: the more I would say, believable end of the story is that somebody found this out. There was no immediate way to make use of this fact. Maybe it is true that at certain times, people who buy diapers also buy beer, but it at least gave some confidence to the idea that you can use this kind of information to make decisions. So from that perspective, it possibly had a role to play. But anyway, so these are the origins of this problem, and it's called market basket analysis because it's really talking in this context about shopping baskets. I have a shopping basket when I reach the counter to pay. So question is, what is in that basket? If it has x, does it also have y? So this is a very literal interpretation of markets and baskets. But the notion of markets and baskets can be a little bit more abstract. So, for instance, if you are thinking about, especially nowadays, given the number of elearning platforms, there's a lot of emphasis on trying to structure topics in such a way that\n",
            "Relevance Score: 0.94\n",
            "\n",
            "\n",
            "Document Rank: 3, Document Index: 5\n",
            "Document: all refinements. And there are many situations where these are important and interesting, but this model as such, this kind of market basket analysis approach is very naive. And so that's why these things will come up in different contexts. But we are not going to explore it anymore in this context, because in this context, the kind of model that you build, this association rule model is rather simplistic. So we are going to look at more sophisticated models as we go along. And there you can ask the same question, but it'll take a slightly different format. Does this also include tendencies like, if there is one person who tends to buy all his groceries once a month, then they will have a bigger basket size. But for people who buy their groceries, say like per week or every other day, they will have smaller basket size, but their transactions would be spread out. So that will be treated as a different transaction every time. But for somebody who's buying everything together, doesn't\n",
            "Relevance Score: 0.89\n",
            "\n",
            "\n",
            "Document Rank: 4, Document Index: 9\n",
            "Document: have to, in the example that you gave, you would have to first separate out the data for these, say, the people who buy one month groceries at a time from those who buy at more frequent intervals and then analyze them separately, because together the data doesn't actually make sense. So last time, if you remember, I talked about this unsupervised. So maybe one thing that you need to do when you have something large is to basically look at the transactions and categorize them according to some criteria, maybe by the size of the basket. And then you will find that there are maybe a lot of people who buy five items at a time. There are people who buy ten to 20 items at a time. There are people who buy 40 items at a time. And maybe you need to then apply the model separately to each of these clusters. So all of these things are not a one size fits all solution. And also, it's not that one approach on its own is going to solve the problem at any given time. So you need to have a kind of\n",
            "Relevance Score: 0.88\n",
            "\n",
            "\n",
            "Document Rank: 5, Document Index: 2\n",
            "Document: So we were looking at this market basket analysis, and towards the end of the discussion on association rule rules, if everybody who buys X is also likely to buy y. We said that one application of this association rule idea is to have these so called class association rules. So the right hand side was a category. We looked at an example with documents and we said, if a document has these words, it is also likely to belong to this category. So that brings us to the topic that is one of the central ones in machine learning, which is the idea of supervised learning. So in supervised learning, we have a set of items, and each item belongs to some category or class, and we distinguish these items in terms of some attributes. So you can think about this, as we will see shortly, you can think about these attributes as columns, right? So the attributes will be things like name, age, date of birth, whatever, income, stuff like that. So each row in the table will be one item, and there will be a\n",
            "Relevance Score: 0.84\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Query 8"
      ],
      "metadata": {
        "id": "2xisLWODH3MH"
      },
      "id": "2xisLWODH3MH"
    },
    {
      "cell_type": "code",
      "source": [
        "query_8=\"How does the genie index contribute to decision tree construction, and why is it favored in practice, particularly in Python libraries?\"\n"
      ],
      "metadata": {
        "id": "b5YbiKUlHwyr"
      },
      "id": "b5YbiKUlHwyr",
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_query8=[]\n",
        "for i in range(len(docs_8)):\n",
        "  retrieved_docs_query8.append(docs_8[i].page_content)"
      ],
      "metadata": {
        "id": "1uGvzJ2pH6Ps"
      },
      "id": "1uGvzJ2pH6Ps",
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(query_8,\"\\n\")\n",
        "results = co.rerank(query=query_8, documents=retrieved_docs_query8, top_n=5, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
        "for idx, r in enumerate(results.results):\n",
        "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
        "  print(f\"Document: {retrieved_docs_query8[r.index]}\")\n",
        "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eDeZv5WID1T",
        "outputId": "e1c51873-e4c7-4a7b-f1d8-a98b44ccb4f5"
      },
      "id": "7eDeZv5WID1T",
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How does the genie index contribute to decision tree construction, and why is it favored in practice, particularly in Python libraries? \n",
            "\n",
            "Document Rank: 1, Document Index: 2\n",
            "Document: is actually easier to compute. And if you look at, as we will see, libraries in Python and all that, typically they use genie index. So although there is a very famous system built using entropy, which was one of the early decision tree systems, the ones that we will practically encounter in Python, libraries actually use the genie index. Basically. I'll stop with this right now. So what we are saying now is that we want to choose the next question to evaluate the next question to ask. So for the next question to ask, we have to say something about the quality of the impurity before and after. So if I just look at the raw ratio of the impurity, I have a motion of just that impurity percentage. But that gives me this linear relationship, which apparently is not that good in practice. So I want a nonlinear way of giving some impurity value to this ratio of yes or no, and then we can use either this entropy idea or this genie index, and it gives me a sharper curve, and this sharper\n",
            "Relevance Score: 1.00\n",
            "\n",
            "\n",
            "Document Rank: 2, Document Index: 3\n",
            "Document: There is a kind of gray area because there are these mixed up things which have, like here for instance, we have similar values and you have flowers from both categories with similar values. So this is this iris data set, and you can actually construct a decision tree for it. So this tree, which we will see later, this python library for decision trees, actually constructs from the iris data set, this decision tree. So what you do is you start with these 150 samples, and initially there are three categories, so there are 50 in each category. And remember that if there are at any point when you have an impure sample, you have to decide on something. So this particular algorithm, they're equal. It happens to just choose one of them more or less at random as the category to predict nation. So this last line is the prediction, this is the distribution of values. This is how many samples there are. And this is the actual genie index calculation. If you think about it, it is the three\n",
            "Relevance Score: 1.00\n",
            "\n",
            "\n",
            "Document Rank: 3, Document Index: 6\n",
            "Document: use either this entropy idea or this genie index, and it gives me a sharper curve, and this sharper practice turns out to be better. So that's how you build a decision tree. You build a decision tree by applying one of these two nonlinear things. If you come up to the third thing, it's also fine. It's just that these two happen to be standard things. They are both borrowed from other areas where they have a long and established history. So, as I said, genie index comes from economics and entropy comes from information. So we'll stop here. So, any questions? It okay, if there are no questions, then we'll continue with this discussion of decision trees on Thursday when we meet next.\n",
            "Relevance Score: 0.99\n",
            "\n",
            "\n",
            "Document Rank: 4, Document Index: 10\n",
            "Document: and otherwise it will turn out to be much smaller than that. So again, you can plot the genie index curve. And now these are the curves. So this is our old friend. This is entropy. I mean, sorry, this is our misclassification, linear. This green dashed line is genie impurity, and this is entropy. But entropy goes up to one, which is out of scale. So we have the entropy, we get this red. So, notice that the entropy line is slightly steeper than the genie impurity line. And our logic earlier was that somehow nonlinear is better empirically, whatever that means. So empirically just means that somebody built these systems and found that if they use these kind of things, it works better. So empirically, entropy is, I mean, theoretically, entropy is slightly steeper, but entropy requires us to compute logs and all that, whereas genie index is just this computing squares of numbers. So genie index is actually easier to compute. And if you look at, as we will see, libraries in Python and all\n",
            "Relevance Score: 0.99\n",
            "\n",
            "\n",
            "Document Rank: 5, Document Index: 7\n",
            "Document: there are. And this is the actual genie index calculation. If you think about it, it is the three samples are each with one third. So you have one third squared plus one third, one third squared, one nine. So one minus one 9th plus one nine plus one nine, which is one minus one thirds. So that's why you get this in the index of zero, 67. So this is the initial. Now, what this decision tree is doing at this point is it is choosing one of the attributes, which is in this case the petal length. And it is asking whether the petal number, so this number, if you look at it, is somewhere here, it's asking whether the petal length is less than 2.45, which is a number between two and three, which splits the yellow from the blue and the green. So if it is, yes, less than 2.45, then all the samples are in the smallest category and it is 500 zero. So all the yellow flowers fall in this category. On the other hand, the remaining hundred are equally distributed and it basically picks the first\n",
            "Relevance Score: 0.71\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Query 9"
      ],
      "metadata": {
        "id": "NteICTkJIKH8"
      },
      "id": "NteICTkJIKH8"
    },
    {
      "cell_type": "code",
      "source": [
        "query_9=\"What is the harmonic mean, and how does it enable the combination of precision and recall into a single metric?\"\n"
      ],
      "metadata": {
        "id": "EWZ-tbwBIM0a"
      },
      "id": "EWZ-tbwBIM0a",
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_query9=[]\n",
        "for i in range(len(docs_9)):\n",
        "  retrieved_docs_query9.append(docs_9[i].page_content)"
      ],
      "metadata": {
        "id": "AiZ6iQ9UIQRz"
      },
      "id": "AiZ6iQ9UIQRz",
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(query_9,\"\\n\")\n",
        "results = co.rerank(query=query_9, documents=retrieved_docs_query9, top_n=5, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
        "for idx, r in enumerate(results.results):\n",
        "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
        "  print(f\"Document: {retrieved_docs_query9[r.index]}\")\n",
        "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9t57KxbIU36",
        "outputId": "ccf10f6c-4224-4934-c5d5-de56c2a1f3fc"
      },
      "id": "b9t57KxbIU36",
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the harmonic mean, and how does it enable the combination of precision and recall into a single metric? \n",
            "\n",
            "Document Rank: 1, Document Index: 7\n",
            "Document: precision one and it's one upon that. So it's the reciprocal of the mean of the reciprocals. So if you work this out, you get two PR on two p plus. So this is sometimes called the f score. Right? So this is a way of combining the precision score and the recall score into a single number. But as I said, in most cases, it really depends on the application. So either you want something based on the context which has higher precision or higher recall, and you choose. So these are some of the ways in which we can actually. So, remember, our starting point was that we wanted to evaluate classifiers. So we said first we have a strategy for evaluating classifiers, so we can test them by using this test set and so on. But what is it that you want to measure on the test set? So that we have this wide variety of things, but most commonly this precision and recall are the ones that people are interested in, and they will choose which one to emphasize. Depending on the context. Depending on the\n",
            "Relevance Score: 0.99\n",
            "\n",
            "\n",
            "Document Rank: 2, Document Index: 3\n",
            "Document: that these are not the only things so people also talk about. So there is, for instance, the corresponding thing to precision for the negative part is specificity. Right? So what is the ratio of correct negative results compared to all negative results as a true negative? So that's with respect to this column, then accuracy, of course, is just saying how many of the correct things are there with respect to the whole thing. So that also can be measured easily from this. And then there's something called thread score and so on. So I'm not going to talk about it. And just for completeness, I will mention that sometimes people want to say that we want a single number. It's not good to give me a precision separately and a recall separately and let me choose, I want a single number. There is this thing called harmonic mean. So the harmonic mean is one upon p plus one upon R. So remember, p is precision one and it's one upon that. So it's the reciprocal of the mean of the reciprocals. So if\n",
            "Relevance Score: 0.95\n",
            "\n",
            "\n",
            "Document Rank: 3, Document Index: 11\n",
            "Document: in, and they will choose which one to emphasize. Depending on the context. Depending on the context, you may want precision predominant. You may want recall to be predominant. So there, it really becomes a domain specific. So I'll stop there. Any questions, sir? Like, given a precision or recall value, what we need to change in the algorithm so that we could come to the certain value of precision and recall. So that is a harder question. Right. So what is it that you need to change? So that's a very difficult question. All you can say is that given what we are doing now, this is what, then it becomes really an experimental issue. So this is the hardest question in machine learning, which is, how do I improve the quality of the classifier? So there is no answer to that question, which is obvious. So you have to start experimenting. You have to do something. So there is something called feature engineering. You might want to come out with some combination of features. So some of you may\n",
            "Relevance Score: 0.49\n",
            "\n",
            "\n",
            "Document Rank: 4, Document Index: 5\n",
            "Document: 1 million items, ten to the power six, one with 60. So this is 1 million, this is 1 billion. So we have a large number of transactions. And now we need to remember we are only dealing with the frequent item set problem. So the confidence level is not yet an important factor for us. We are just trying to find this thing. All the z such that z dot count is bigger than sigma times m. So the only factor that is important to us right now is the sigma. So sigma, let's assume, is very small. Sigma is just 1%. We just want to know which subsets appear in 1% of the transactions. So as we said before, if you actually naively count the possible subsets that you have to keep track of, then it will be for every size up to ten, right? Because we have at most ten items. For every size up to ten, we have to choose that many items, I items from the 1 million. Each of those is a different subset. So we will have that many possible subsets. If you are counting every possible subset, on the other hand,\n",
            "Relevance Score: 0.03\n",
            "\n",
            "\n",
            "Document Rank: 5, Document Index: 9\n",
            "Document: what you're really looking for is correlations between sets of items across transactions, which we formalize in terms of these association groups. X implies y. And what we saw is that as such, combinatorially, this is a very complicated thing to calculate, given the sizes of the sets involved. But we can use this appropriate principle both to identify the sets which are interesting, those which are frequent enough to be considered, and also to decompose them into rules. So you use the approoriate twice, once to find the frequent z, and second to decompose each frequent z into possible x implies y. And using this, we can build our first simple supervised learning model. So these rules now become a very simple way of prediction. Now, if you give me a document, I will look at what words are there in the document in this previous example. And if it contains the word student and school, I will predict education. If it contains game and team, I'll predict sports. Now, of course, if it has\n",
            "Relevance Score: 0.02\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Query 10"
      ],
      "metadata": {
        "id": "LW486RSqIeO7"
      },
      "id": "LW486RSqIeO7"
    },
    {
      "cell_type": "code",
      "source": [
        "query_10=\"What are the implications of choosing a large or small learning rate?\"\n"
      ],
      "metadata": {
        "id": "wq858y57Iday"
      },
      "id": "wq858y57Iday",
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_query10=[]\n",
        "for i in range(len(docs_10)):\n",
        "  retrieved_docs_query10.append(docs_10[i].page_content)"
      ],
      "metadata": {
        "id": "lUXqs97VIlny"
      },
      "id": "lUXqs97VIlny",
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(query_10,\"\\n\")\n",
        "results = co.rerank(query=query_10, documents=retrieved_docs_query10, top_n=5, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
        "for idx, r in enumerate(results.results):\n",
        "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
        "  print(f\"Document: {retrieved_docs_query10[r.index]}\")\n",
        "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPbP2gTQIr4q",
        "outputId": "351e17a3-4953-4209-9a81-30ebeca0ee54"
      },
      "id": "CPbP2gTQIr4q",
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What are the implications of choosing a large or small learning rate? \n",
            "\n",
            "Document Rank: 1, Document Index: 7\n",
            "Document: gain by the scattering of that attribute. So if we have high gain divided by high scattering, then the net normalized effect is lower. So you might choose an attribute which actually gives you a lower information gain overall, because the attribute itself is less static. And that's the thing that we want to achieve. Just to emphasize that when you have a machine learning algorithm, see generally, what is a machine learning algorithm doing? It's trying to describe the shape of the model based on the training data. So in this case it's literally the shape, because you have many possible trees you could construct, and the actual tree you construct depends on the data. And because you don't have any preconceptions about the data, you have to make sure that if there are these kind of patterns which disturb your algorithm, you will be able to counteract. So that's the purpose of this information gain ratio. It's not just the information gain alone, which is what we had initially. It's\n",
            "Relevance Score: 0.08\n",
            "\n",
            "\n",
            "Document Rank: 2, Document Index: 6\n",
            "Document: this, and therefore. Or you say, number two is this and number seven is that. And therefore, in any context, generally, a smaller model is usually better, but everything is. And this is a common feature of anything we do, machine learning. Nothing is. You cannot say it universally, as I said, like newtonian mechanics and quantum mechanics, you could always have a situation where the more complicated model is required. So, yes, certainly small trees are more explainable. It'll turn out that small trees also solve the other problem that we mentioned earlier about generalization. So, question is whether or not this model is better for the data you have not seen compared to the data that you are training on. So is it too specific for the training data that you are building. So if you ask more questions, then you are really focusing on more individual eto synchronicity or complications in the data that you see. That might be an isolated case, and it may not appear in general, so maybe you\n",
            "Relevance Score: 0.04\n",
            "\n",
            "\n",
            "Document Rank: 3, Document Index: 8\n",
            "Document: yet, first of all, a truly learning problem. It's more an algorithmic problem, but still, it's an interesting problem nonetheless, because I just want to illustrate how it affects the way in which we calculate what might be trivial with small data becomes nontrivial with large data. So this is the problem. So given a set of items, capital n, which is large, and a, given a set of transactions m, which is again large, and given these two ratios between zero and one, find every pair x and y such that x implies y is a valid association. So we can break up this thing into two steps. I mean, we want to first check whether x implies y is worth looking at at all. So we first look at the support part. We want to know whether x, sorry, yeah, we want to know whether the count divided by m is bigger than the support, which is the same as taking this m to the other side and saying whether the count is bigger than a certain fraction of the total. So the first idea is to identify those sets whose\n",
            "Relevance Score: 0.03\n",
            "\n",
            "\n",
            "Document Rank: 4, Document Index: 11\n",
            "Document: against some unknown data. So, one other strategy where you don't want to, I mean, one of the other disadvantages of doing this is that maybe that there are some minority anomalies in this data, and the choice of test data that you have made might have hidden all these anomalies, so you never see them, and maybe it's important to see them. So there may be many situations, or maybe you just don't have enough data as a whole to build a good model by only looking at 80%. So another strategy is to systematically do this with different subsets. So what you're really asking at some higher level is that machine learning approach. So, remember, we have seen only decision trees, but we are going to see many models. So there are clearly many ways to build models. And the reason, whenever you see that there are many ways to do something, it's only because there is no guarantee that a given way is the best one. So more or less what you want to validate is, I have, say, seven different strategies\n",
            "Relevance Score: 0.03\n",
            "\n",
            "\n",
            "Document Rank: 5, Document Index: 2\n",
            "Document: what you're saying is that this is a representative sample. So you're not saying that the examples have every possible situation that you are likely to use for judgment, but you're really talking at the level of probabilities. You're saying that the probability distribution of the training is somehow identical to that of the unseen data on which you are going to make the prediction. So this is going to be your training and this is going to be your prediction. So this is a fundamental assumption. If this assumption does not hold, then whatever you try to do doesn't really have any basis. So this is something that we have to assume, and we will talk about later on what this assumption means. Maybe not in this course, but in later courses. But for now, let us assume this. So the next question is, what does it mean to learn? Right? So, as we saw in this loan data set, 60% of the loans were granted, so nine by 15. So if I were to just say yes, supposing I don't have an interesting model\n",
            "Relevance Score: 0.02\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VouX2EmaItRS"
      },
      "id": "VouX2EmaItRS",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9b3b86e2e178403dbea480033f0c3170": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b4aff0d02ce4fba9abe27dc24eaccb4",
              "IPY_MODEL_64167dffcaaf4a27b90f28d4d7d2deab",
              "IPY_MODEL_5392b5f56bee461b80113e4e8b6d55ba"
            ],
            "layout": "IPY_MODEL_33478a3d277841f39e0a2ae43f9f01de"
          }
        },
        "3b4aff0d02ce4fba9abe27dc24eaccb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b977658d6d044f5dba826005f1703683",
            "placeholder": "​",
            "style": "IPY_MODEL_d45b716f201f4ab29e20097ee15db082",
            "value": "modules.json: 100%"
          }
        },
        "64167dffcaaf4a27b90f28d4d7d2deab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_909cc46b5c22478dbc8550906b1deb97",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8f595f940820473387490fcded5db6a1",
            "value": 349
          }
        },
        "5392b5f56bee461b80113e4e8b6d55ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca62535d6be64037a31aba858cc8eb3d",
            "placeholder": "​",
            "style": "IPY_MODEL_2c1fc2a4951643d5836f313453a3035c",
            "value": " 349/349 [00:00&lt;00:00, 9.98kB/s]"
          }
        },
        "33478a3d277841f39e0a2ae43f9f01de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b977658d6d044f5dba826005f1703683": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d45b716f201f4ab29e20097ee15db082": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "909cc46b5c22478dbc8550906b1deb97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f595f940820473387490fcded5db6a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ca62535d6be64037a31aba858cc8eb3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c1fc2a4951643d5836f313453a3035c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "302350358d7e45fab74d808c8e2c7c01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a392fb83a608434386f8137516a9af5a",
              "IPY_MODEL_d2a766064a854c51849d33abb21b3681",
              "IPY_MODEL_10907a0927c34ec49552c5ecbc0ee188"
            ],
            "layout": "IPY_MODEL_bc9522147c454b019bba31e7466722c0"
          }
        },
        "a392fb83a608434386f8137516a9af5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2889aa978c243849ae5481ffa6abd69",
            "placeholder": "​",
            "style": "IPY_MODEL_1d80e0b61db640e283e745303a78c8d2",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "d2a766064a854c51849d33abb21b3681": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2543684c940c4bf780e5f57911c4f8e5",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb2c6f996d0344889f0ed13061ec108e",
            "value": 124
          }
        },
        "10907a0927c34ec49552c5ecbc0ee188": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_149d8c9b65094cc3b9cd27a9fb1538b7",
            "placeholder": "​",
            "style": "IPY_MODEL_ac48af462a56446ba379c6bc6f34df0c",
            "value": " 124/124 [00:00&lt;00:00, 3.24kB/s]"
          }
        },
        "bc9522147c454b019bba31e7466722c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2889aa978c243849ae5481ffa6abd69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d80e0b61db640e283e745303a78c8d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2543684c940c4bf780e5f57911c4f8e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb2c6f996d0344889f0ed13061ec108e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "149d8c9b65094cc3b9cd27a9fb1538b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac48af462a56446ba379c6bc6f34df0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4344f55939f4de0bed4a55c46e487c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6c5f4cbb4bb1499dbfda05d60a03a462",
              "IPY_MODEL_83632d9a01044bcd954795d5e3612ee2",
              "IPY_MODEL_16bae1026ff14d738731f4769f1fe9fb"
            ],
            "layout": "IPY_MODEL_fa94f3425b604f62b551c42fe1cfc198"
          }
        },
        "6c5f4cbb4bb1499dbfda05d60a03a462": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6012ffbc02aa4bcdb586f89dfed9b703",
            "placeholder": "​",
            "style": "IPY_MODEL_152d90fb8ec84939b8aa18287116671c",
            "value": "README.md: 100%"
          }
        },
        "83632d9a01044bcd954795d5e3612ee2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3c59c09f7b14c69b638f9faf42b9971",
            "max": 90272,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b6887eae1851411d810175782cef953c",
            "value": 90272
          }
        },
        "16bae1026ff14d738731f4769f1fe9fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f4e349511bc4cb0bdbe5734887d2023",
            "placeholder": "​",
            "style": "IPY_MODEL_01ee9d68184d4d479ce5855ad772843f",
            "value": " 90.3k/90.3k [00:00&lt;00:00, 2.67MB/s]"
          }
        },
        "fa94f3425b604f62b551c42fe1cfc198": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6012ffbc02aa4bcdb586f89dfed9b703": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "152d90fb8ec84939b8aa18287116671c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3c59c09f7b14c69b638f9faf42b9971": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6887eae1851411d810175782cef953c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6f4e349511bc4cb0bdbe5734887d2023": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01ee9d68184d4d479ce5855ad772843f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef27a27e6a5844e4816e95d627f02ae3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b3fe4febc305400292d0c8c0625c99b3",
              "IPY_MODEL_629066a3730d44c69cdcc83f803b2146",
              "IPY_MODEL_23fe753a41984870a53ea5d38ae5542b"
            ],
            "layout": "IPY_MODEL_f8072534ac0b411ca7345b31e348c99a"
          }
        },
        "b3fe4febc305400292d0c8c0625c99b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9459f22b75604f1c9f3c6436ba47df64",
            "placeholder": "​",
            "style": "IPY_MODEL_9c7fd57e39fb4f0d8869ddf80267d618",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "629066a3730d44c69cdcc83f803b2146": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74f0fe9f395c4d779f5baeef9bcde5c0",
            "max": 52,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1e953a2c1b154d64913cda1e34287cc1",
            "value": 52
          }
        },
        "23fe753a41984870a53ea5d38ae5542b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f2f655b19a3429196f46bf96721f9bd",
            "placeholder": "​",
            "style": "IPY_MODEL_945fb22b9c254c0b9e059bd189790f67",
            "value": " 52.0/52.0 [00:00&lt;00:00, 1.49kB/s]"
          }
        },
        "f8072534ac0b411ca7345b31e348c99a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9459f22b75604f1c9f3c6436ba47df64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c7fd57e39fb4f0d8869ddf80267d618": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74f0fe9f395c4d779f5baeef9bcde5c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e953a2c1b154d64913cda1e34287cc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0f2f655b19a3429196f46bf96721f9bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "945fb22b9c254c0b9e059bd189790f67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8cb49bbdbf7b4964bb28e3557dde7bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_59d8ac2d3e374ac8a34761e2f17569a4",
              "IPY_MODEL_9c442f62ec5e4808b1230f586293458d",
              "IPY_MODEL_622f06d416304186b769f58c0ee462bf"
            ],
            "layout": "IPY_MODEL_46a640a2540044c3a50edb9f38b31c11"
          }
        },
        "59d8ac2d3e374ac8a34761e2f17569a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a82438beb9494546913b1426242bc19e",
            "placeholder": "​",
            "style": "IPY_MODEL_32c1423b9a31494db7ca9863796708c3",
            "value": "config.json: 100%"
          }
        },
        "9c442f62ec5e4808b1230f586293458d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39e542c560bc46f9923b306b4a8840f1",
            "max": 720,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ed0979c9224743fca17bd72672ed879b",
            "value": 720
          }
        },
        "622f06d416304186b769f58c0ee462bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b5ab048aef64fc1af016ffb03489277",
            "placeholder": "​",
            "style": "IPY_MODEL_e96b5cbd3b1e4229b45d92d4f97b4a35",
            "value": " 720/720 [00:00&lt;00:00, 31.8kB/s]"
          }
        },
        "46a640a2540044c3a50edb9f38b31c11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a82438beb9494546913b1426242bc19e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32c1423b9a31494db7ca9863796708c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39e542c560bc46f9923b306b4a8840f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed0979c9224743fca17bd72672ed879b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8b5ab048aef64fc1af016ffb03489277": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e96b5cbd3b1e4229b45d92d4f97b4a35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37410199c1e4440486d50cd8115761ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b4ab592d59dc4f45abf6bd561a0d9c3d",
              "IPY_MODEL_6968815fab0a487b961a2ca5034f6f90",
              "IPY_MODEL_7db492829a5a430cb369926102b515c7"
            ],
            "layout": "IPY_MODEL_a68a15a97f45410d83cc14079c669465"
          }
        },
        "b4ab592d59dc4f45abf6bd561a0d9c3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3eb26b143f23459c82833b34d589a10f",
            "placeholder": "​",
            "style": "IPY_MODEL_09f55e43ecda4204bdc14a32444a919c",
            "value": "model.safetensors: 100%"
          }
        },
        "6968815fab0a487b961a2ca5034f6f90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d280dd551d24262aaeb845c808665f0",
            "max": 1340616616,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e8133ee097714d91b725f97095af75e1",
            "value": 1340616616
          }
        },
        "7db492829a5a430cb369926102b515c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e60a1bc798b4e7cbdbbbaf9454117e2",
            "placeholder": "​",
            "style": "IPY_MODEL_3bddd5fabdf845108aba1ceb16e1b00e",
            "value": " 1.34G/1.34G [00:12&lt;00:00, 195MB/s]"
          }
        },
        "a68a15a97f45410d83cc14079c669465": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3eb26b143f23459c82833b34d589a10f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09f55e43ecda4204bdc14a32444a919c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d280dd551d24262aaeb845c808665f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8133ee097714d91b725f97095af75e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8e60a1bc798b4e7cbdbbbaf9454117e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bddd5fabdf845108aba1ceb16e1b00e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff41e1d5bfc24e5db318d728bf8c9c40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_55cb97c8ccec42f595339547e0402117",
              "IPY_MODEL_9a501ac2c2ac41c2acd9b242bc4ec4ec",
              "IPY_MODEL_567cfdfa6d46426b9acd2b96a5fdc93a"
            ],
            "layout": "IPY_MODEL_5133b22dc2c14aaaa4b067c7c06ea405"
          }
        },
        "55cb97c8ccec42f595339547e0402117": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef14d2d34c7347cfb605ed95ea0549c3",
            "placeholder": "​",
            "style": "IPY_MODEL_e6bcd3860048405ba3f08973ef1277bd",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "9a501ac2c2ac41c2acd9b242bc4ec4ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cf365e4e05e445b909c12b06c229f40",
            "max": 366,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_60a198237cdf42c58d5eb2360756d569",
            "value": 366
          }
        },
        "567cfdfa6d46426b9acd2b96a5fdc93a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df3f00a2e4574693a3fc07dbbe59f8ee",
            "placeholder": "​",
            "style": "IPY_MODEL_452ab265b0f742728e49086345cbeb35",
            "value": " 366/366 [00:00&lt;00:00, 12.4kB/s]"
          }
        },
        "5133b22dc2c14aaaa4b067c7c06ea405": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef14d2d34c7347cfb605ed95ea0549c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6bcd3860048405ba3f08973ef1277bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6cf365e4e05e445b909c12b06c229f40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60a198237cdf42c58d5eb2360756d569": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df3f00a2e4574693a3fc07dbbe59f8ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "452ab265b0f742728e49086345cbeb35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "adf7f663c97948ae8cbcc14682a170b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_70dae3a6d95a4e84a864b5f3571f77ed",
              "IPY_MODEL_91f9952d01e74481a5f830651cb50c42",
              "IPY_MODEL_9c66817193fb46efa973d8d7f94b8eda"
            ],
            "layout": "IPY_MODEL_3577bb86756d4948b760df4e01eff30b"
          }
        },
        "70dae3a6d95a4e84a864b5f3571f77ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e0d51954464465e91cac40a907cb1fa",
            "placeholder": "​",
            "style": "IPY_MODEL_afc93aed15c24dda91fe8c872b8f9bd0",
            "value": "vocab.txt: 100%"
          }
        },
        "91f9952d01e74481a5f830651cb50c42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0991f7732cf48d59cee65669584b9f1",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6bec3258325c4feb96fb4dbded66baa3",
            "value": 231508
          }
        },
        "9c66817193fb46efa973d8d7f94b8eda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49afe4205f274dc0bd8b1d2ab302ffa7",
            "placeholder": "​",
            "style": "IPY_MODEL_5300962f8b784c72a8496c02295ee02f",
            "value": " 232k/232k [00:00&lt;00:00, 11.4MB/s]"
          }
        },
        "3577bb86756d4948b760df4e01eff30b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e0d51954464465e91cac40a907cb1fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afc93aed15c24dda91fe8c872b8f9bd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0991f7732cf48d59cee65669584b9f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bec3258325c4feb96fb4dbded66baa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "49afe4205f274dc0bd8b1d2ab302ffa7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5300962f8b784c72a8496c02295ee02f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf6df2e32d7944d3aec0ca7b363a6b47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9b9a1abde3f54975bf347dd9ae01e5ea",
              "IPY_MODEL_f0c006cc88e2463cbb1358047c93e3e9",
              "IPY_MODEL_0671b863e09a4c5f8988e28193aca827"
            ],
            "layout": "IPY_MODEL_d3a9cc90ac5646d9ab49a6550ba18e74"
          }
        },
        "9b9a1abde3f54975bf347dd9ae01e5ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51c2457c43a947d28301a524a20ae88c",
            "placeholder": "​",
            "style": "IPY_MODEL_b2c963233cba46788835278b19a9de62",
            "value": "tokenizer.json: 100%"
          }
        },
        "f0c006cc88e2463cbb1358047c93e3e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1ec30624ead42dc970d73603eafa47b",
            "max": 711396,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_18b020f9c36b495fb59c927811ee2559",
            "value": 711396
          }
        },
        "0671b863e09a4c5f8988e28193aca827": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd09b62cdff34b47bac7ff21f4503d41",
            "placeholder": "​",
            "style": "IPY_MODEL_49890c51d54b431a82e94521c4e352a4",
            "value": " 711k/711k [00:00&lt;00:00, 11.4MB/s]"
          }
        },
        "d3a9cc90ac5646d9ab49a6550ba18e74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51c2457c43a947d28301a524a20ae88c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2c963233cba46788835278b19a9de62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f1ec30624ead42dc970d73603eafa47b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18b020f9c36b495fb59c927811ee2559": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dd09b62cdff34b47bac7ff21f4503d41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49890c51d54b431a82e94521c4e352a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "507a79ec4ff2436191ea2e35631dd313": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a70a414b6ede431ab305be0947ad2a2c",
              "IPY_MODEL_6ca1d786e596433a8a1f49ab9f2ed3f6",
              "IPY_MODEL_ea9075c84fa34988b93fd9a2c7c82302"
            ],
            "layout": "IPY_MODEL_ca3605253ec445878852ab46caba56ba"
          }
        },
        "a70a414b6ede431ab305be0947ad2a2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad87ac7f020f42fe8bcc555946fbd0de",
            "placeholder": "​",
            "style": "IPY_MODEL_bf5d50d92c6b4f3e85d86d1ab738b331",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "6ca1d786e596433a8a1f49ab9f2ed3f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae950e561b624c44b6ede9bf884042a6",
            "max": 125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_87a438c9a2204e948810d8d7261442c6",
            "value": 125
          }
        },
        "ea9075c84fa34988b93fd9a2c7c82302": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbde7f1709124ce4a613d23df14754a8",
            "placeholder": "​",
            "style": "IPY_MODEL_a489928602e64c97aac017b2a3f2c8a4",
            "value": " 125/125 [00:00&lt;00:00, 2.84kB/s]"
          }
        },
        "ca3605253ec445878852ab46caba56ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad87ac7f020f42fe8bcc555946fbd0de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf5d50d92c6b4f3e85d86d1ab738b331": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae950e561b624c44b6ede9bf884042a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87a438c9a2204e948810d8d7261442c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dbde7f1709124ce4a613d23df14754a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a489928602e64c97aac017b2a3f2c8a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e744b9c9788649d79192a4621d0953aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d4bdae2bf8904389bcb2e84a000a1d33",
              "IPY_MODEL_91cc54b84f9244b29acfb2adc86676a4",
              "IPY_MODEL_bd44082ae39842faa918d0ecdc62c740"
            ],
            "layout": "IPY_MODEL_dfff187735b94b23a2e3e52fa24fccd4"
          }
        },
        "d4bdae2bf8904389bcb2e84a000a1d33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86eb2f932f3b4f70b6d7a19f3613e787",
            "placeholder": "​",
            "style": "IPY_MODEL_ed346407aa5d46fb9ea1a912786b476f",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "91cc54b84f9244b29acfb2adc86676a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ffa1a7230d74b6bbde7895506fd0210",
            "max": 191,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bc8e62b33fef4c18a41873aee4468b84",
            "value": 191
          }
        },
        "bd44082ae39842faa918d0ecdc62c740": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15c5a5db4b9141c8b3fc3b9126e16b33",
            "placeholder": "​",
            "style": "IPY_MODEL_a9df433da035460197c12b8e328332d3",
            "value": " 191/191 [00:00&lt;00:00, 10.3kB/s]"
          }
        },
        "dfff187735b94b23a2e3e52fa24fccd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86eb2f932f3b4f70b6d7a19f3613e787": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed346407aa5d46fb9ea1a912786b476f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ffa1a7230d74b6bbde7895506fd0210": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc8e62b33fef4c18a41873aee4468b84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "15c5a5db4b9141c8b3fc3b9126e16b33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9df433da035460197c12b8e328332d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}