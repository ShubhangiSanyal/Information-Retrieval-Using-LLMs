{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 8000798,
          "sourceType": "datasetVersion",
          "datasetId": 4711455
        }
      ],
      "dockerImageVersionId": 30674,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7752a9776f084d9985ee839f43874ac3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de488f0641c74917aea2e8dbfce0e4aa",
              "IPY_MODEL_00ada49c8bdf49b4a10d229d704b0c1a",
              "IPY_MODEL_d3c1bd1702ef44e88ffcf11de3aefd15"
            ],
            "layout": "IPY_MODEL_fe9aa770584542f987bd36eecb1def2c"
          }
        },
        "de488f0641c74917aea2e8dbfce0e4aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dcf1af9f0fa249c9a3775b6bf7f930a4",
            "placeholder": "​",
            "style": "IPY_MODEL_227bef90fd254f128b0811521279a593",
            "value": "modules.json: 100%"
          }
        },
        "00ada49c8bdf49b4a10d229d704b0c1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7728a29e1e14b46ae8c59b8a68b70da",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7d5448a16e3645a287d62171a8dab989",
            "value": 349
          }
        },
        "d3c1bd1702ef44e88ffcf11de3aefd15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5009c62ed6464f1ba076fe99ec46295b",
            "placeholder": "​",
            "style": "IPY_MODEL_014cc5d179f9403d8ac36d65bc2fb140",
            "value": " 349/349 [00:00&lt;00:00, 6.14kB/s]"
          }
        },
        "fe9aa770584542f987bd36eecb1def2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcf1af9f0fa249c9a3775b6bf7f930a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "227bef90fd254f128b0811521279a593": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7728a29e1e14b46ae8c59b8a68b70da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d5448a16e3645a287d62171a8dab989": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5009c62ed6464f1ba076fe99ec46295b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "014cc5d179f9403d8ac36d65bc2fb140": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ad0061bbeae40c893f96f85560ebb64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1a68ab062b8f4f139fb76e24f08177e9",
              "IPY_MODEL_f6b4cb3308ae4ebabb6ddb71230661a6",
              "IPY_MODEL_254d9cae597949698ae287b8337490af"
            ],
            "layout": "IPY_MODEL_bd189b9febfa4916b16d1e9c94cc67a1"
          }
        },
        "1a68ab062b8f4f139fb76e24f08177e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43e18eb4342a4405a59f754fa85d1700",
            "placeholder": "​",
            "style": "IPY_MODEL_88db495107ba466d9b7e1cf7e83b236f",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "f6b4cb3308ae4ebabb6ddb71230661a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6927b468d76409fb51ef492252dbd9c",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6c71efcf68284737a34f26fb00df8283",
            "value": 124
          }
        },
        "254d9cae597949698ae287b8337490af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15afcc4331b146d4a932342d59a285ae",
            "placeholder": "​",
            "style": "IPY_MODEL_d0c803b79bc34adb946772dd6b47b420",
            "value": " 124/124 [00:00&lt;00:00, 1.44kB/s]"
          }
        },
        "bd189b9febfa4916b16d1e9c94cc67a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43e18eb4342a4405a59f754fa85d1700": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88db495107ba466d9b7e1cf7e83b236f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6927b468d76409fb51ef492252dbd9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c71efcf68284737a34f26fb00df8283": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "15afcc4331b146d4a932342d59a285ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0c803b79bc34adb946772dd6b47b420": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1df4e1ad96274adf993a8b190690f519": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a6927040841d45b0b4702637ee70a20f",
              "IPY_MODEL_a605da6fad7445e19c777fe5c218e57e",
              "IPY_MODEL_8d22f6fa3ccf4ae5b2a5fb3df0bf9f21"
            ],
            "layout": "IPY_MODEL_402f06c28e9945c6bcae965c1846ab05"
          }
        },
        "a6927040841d45b0b4702637ee70a20f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca922dbc53ab4c0ea47f682d2b7fdc2d",
            "placeholder": "​",
            "style": "IPY_MODEL_279b76fb09b341489ee444398ea58a61",
            "value": "README.md: 100%"
          }
        },
        "a605da6fad7445e19c777fe5c218e57e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1dc3c26d6bd94a41a6a9bfa71a894208",
            "max": 90272,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0b529fcf05284b2681fb37356e30ca6d",
            "value": 90272
          }
        },
        "8d22f6fa3ccf4ae5b2a5fb3df0bf9f21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43868c9079644f32b53047053a10f0d0",
            "placeholder": "​",
            "style": "IPY_MODEL_ab30df7a463c43959492410bada19c05",
            "value": " 90.3k/90.3k [00:00&lt;00:00, 1.25MB/s]"
          }
        },
        "402f06c28e9945c6bcae965c1846ab05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca922dbc53ab4c0ea47f682d2b7fdc2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "279b76fb09b341489ee444398ea58a61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1dc3c26d6bd94a41a6a9bfa71a894208": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b529fcf05284b2681fb37356e30ca6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "43868c9079644f32b53047053a10f0d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab30df7a463c43959492410bada19c05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "20b4b453c75546a08b1518bec2b6a577": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6ccbfb3b9dae4afeaa57916aba388870",
              "IPY_MODEL_307b6aaaf8ae44b2a3f572c610e21e5a",
              "IPY_MODEL_078722cc22cf44d586f871d279ff9588"
            ],
            "layout": "IPY_MODEL_da13547cd5854c4383beb943836dd601"
          }
        },
        "6ccbfb3b9dae4afeaa57916aba388870": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af5febe34bf84ddbae11d2ea996d1634",
            "placeholder": "​",
            "style": "IPY_MODEL_e73dfda02f0049a7a8f91563e1b983af",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "307b6aaaf8ae44b2a3f572c610e21e5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a79c6fd66e2e4d7da2c5e1f565fd2440",
            "max": 52,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6e37ab7f2b0a4af6bbec0ad1ff01b41c",
            "value": 52
          }
        },
        "078722cc22cf44d586f871d279ff9588": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3c0e1eae7994a86bbb775cfda814e7b",
            "placeholder": "​",
            "style": "IPY_MODEL_74e3e2549a4c44139758556082a953f1",
            "value": " 52.0/52.0 [00:00&lt;00:00, 373B/s]"
          }
        },
        "da13547cd5854c4383beb943836dd601": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af5febe34bf84ddbae11d2ea996d1634": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e73dfda02f0049a7a8f91563e1b983af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a79c6fd66e2e4d7da2c5e1f565fd2440": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e37ab7f2b0a4af6bbec0ad1ff01b41c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f3c0e1eae7994a86bbb775cfda814e7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74e3e2549a4c44139758556082a953f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85f24e4aba3e4820bb1cb1051a834ad8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_758ced432af4498ca1b13e333bc3a5dc",
              "IPY_MODEL_915daf3c844747bba899b41d75fb73eb",
              "IPY_MODEL_9a0fe2adb4194e67ba5bc56bb654be7f"
            ],
            "layout": "IPY_MODEL_5aa7999471a64086a46cdd02cc1b412d"
          }
        },
        "758ced432af4498ca1b13e333bc3a5dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ea54d0c06434ab5901acc225a6ecaa0",
            "placeholder": "​",
            "style": "IPY_MODEL_542e832887c8483984e88e29e0fe7f9c",
            "value": "config.json: 100%"
          }
        },
        "915daf3c844747bba899b41d75fb73eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e9f92092f554d62ac2065da57271dd2",
            "max": 720,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e83ad391c36940a282f28a863a22095d",
            "value": 720
          }
        },
        "9a0fe2adb4194e67ba5bc56bb654be7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbd1e7d7a7704cf6ad4a19d1614be49e",
            "placeholder": "​",
            "style": "IPY_MODEL_9fcd8d526c7043e1a401ea5451ec9833",
            "value": " 720/720 [00:00&lt;00:00, 29.7kB/s]"
          }
        },
        "5aa7999471a64086a46cdd02cc1b412d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ea54d0c06434ab5901acc225a6ecaa0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "542e832887c8483984e88e29e0fe7f9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e9f92092f554d62ac2065da57271dd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e83ad391c36940a282f28a863a22095d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fbd1e7d7a7704cf6ad4a19d1614be49e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fcd8d526c7043e1a401ea5451ec9833": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0fe71eb9e33b4cd6b5c0d377fd579b1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ec3238d053b946198098ffafc382a3ef",
              "IPY_MODEL_308a8805ca9e4f10ac2893561288e832",
              "IPY_MODEL_ffad32554828400babdfb2127e3709fb"
            ],
            "layout": "IPY_MODEL_6c498d78247245f7a1d7cf7595f8c727"
          }
        },
        "ec3238d053b946198098ffafc382a3ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b50dbce355544bc8d63d8ecb2363a23",
            "placeholder": "​",
            "style": "IPY_MODEL_c74917033b26445eba3bf0856c059d5a",
            "value": "model.safetensors: 100%"
          }
        },
        "308a8805ca9e4f10ac2893561288e832": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9723c7500e64df58b690ea1d32f0bc6",
            "max": 1340616616,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1c432e6eb48d4ddf99c359dee5983560",
            "value": 1340616616
          }
        },
        "ffad32554828400babdfb2127e3709fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fffb656ead79424ca518dd682b0dbbd2",
            "placeholder": "​",
            "style": "IPY_MODEL_fa4c5adff2db424098b0f55c1229a042",
            "value": " 1.34G/1.34G [00:08&lt;00:00, 179MB/s]"
          }
        },
        "6c498d78247245f7a1d7cf7595f8c727": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b50dbce355544bc8d63d8ecb2363a23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c74917033b26445eba3bf0856c059d5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a9723c7500e64df58b690ea1d32f0bc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c432e6eb48d4ddf99c359dee5983560": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fffb656ead79424ca518dd682b0dbbd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa4c5adff2db424098b0f55c1229a042": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a5ae0ef33a44397a221316e751b1a0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1fe03de424ec41e4a3da120254a9b53e",
              "IPY_MODEL_56379fddb71846cdbb6b4b174e7375b9",
              "IPY_MODEL_df0224f221c849278a2522b6d17f762c"
            ],
            "layout": "IPY_MODEL_40d3cd2b8dd24d4a896fe504d4ed9aed"
          }
        },
        "1fe03de424ec41e4a3da120254a9b53e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_788d083b277e4a4e91357cc6fac0d9fc",
            "placeholder": "​",
            "style": "IPY_MODEL_5c7ae5e6585845bab2439b45b8b87d37",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "56379fddb71846cdbb6b4b174e7375b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_153950f99f394d76b367942b9114e1cf",
            "max": 366,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6531f377d88a4a0ab10e6277dbd043e1",
            "value": 366
          }
        },
        "df0224f221c849278a2522b6d17f762c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a7aaa8bd38f4d51acaab1805286fbfa",
            "placeholder": "​",
            "style": "IPY_MODEL_4461b686785d49bf92ab1d4afdf5fd7b",
            "value": " 366/366 [00:00&lt;00:00, 20.3kB/s]"
          }
        },
        "40d3cd2b8dd24d4a896fe504d4ed9aed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "788d083b277e4a4e91357cc6fac0d9fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c7ae5e6585845bab2439b45b8b87d37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "153950f99f394d76b367942b9114e1cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6531f377d88a4a0ab10e6277dbd043e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9a7aaa8bd38f4d51acaab1805286fbfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4461b686785d49bf92ab1d4afdf5fd7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "152e642ea173418c90e70872b6081bbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2eac3df7fada46f1a3d5ec1916b2b7f5",
              "IPY_MODEL_94f03508843f4c1ca08cd1477b96c2a1",
              "IPY_MODEL_00e0b269f8ba449daafd58c38937e29a"
            ],
            "layout": "IPY_MODEL_861aec7a70bb4c508236add9b49d133e"
          }
        },
        "2eac3df7fada46f1a3d5ec1916b2b7f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50192445f89d46019ef0bc61a7079b8a",
            "placeholder": "​",
            "style": "IPY_MODEL_e7e95f79446c4f5b996c3073d1efdbda",
            "value": "vocab.txt: 100%"
          }
        },
        "94f03508843f4c1ca08cd1477b96c2a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb6cfd3facfd44209d69980d23d7a6c8",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2ab7767324ce42c6b1ecc1f1e83a3528",
            "value": 231508
          }
        },
        "00e0b269f8ba449daafd58c38937e29a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f32b9c84d1d64035a7eee3c23d1bc319",
            "placeholder": "​",
            "style": "IPY_MODEL_d4cb78badf0e4b78b6402b6dbc4701c8",
            "value": " 232k/232k [00:00&lt;00:00, 1.84MB/s]"
          }
        },
        "861aec7a70bb4c508236add9b49d133e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50192445f89d46019ef0bc61a7079b8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7e95f79446c4f5b996c3073d1efdbda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb6cfd3facfd44209d69980d23d7a6c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ab7767324ce42c6b1ecc1f1e83a3528": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f32b9c84d1d64035a7eee3c23d1bc319": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4cb78badf0e4b78b6402b6dbc4701c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e74c193557046c89a9c7b1f59156054": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3d9d10c1046443a5b98081d979f1c849",
              "IPY_MODEL_b4d345deaaa6428ab37019f0410e800e",
              "IPY_MODEL_9dc6156cfc824e7c90761856b99548dd"
            ],
            "layout": "IPY_MODEL_b6421a35ea3b41e28d2ca6fbee66ad42"
          }
        },
        "3d9d10c1046443a5b98081d979f1c849": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78c685a6b8114573ba492634f5467e73",
            "placeholder": "​",
            "style": "IPY_MODEL_ff7226f4476a4938aafabae29ce4f72a",
            "value": "tokenizer.json: 100%"
          }
        },
        "b4d345deaaa6428ab37019f0410e800e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9235cf74bfb3422aace1b3c265991730",
            "max": 711396,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c898874e6e6d4a19b78b8e848057ea07",
            "value": 711396
          }
        },
        "9dc6156cfc824e7c90761856b99548dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44600030d879448f857b379c944b2a14",
            "placeholder": "​",
            "style": "IPY_MODEL_7b3cd41bbb2b4e19997a0cf18c7f4a10",
            "value": " 711k/711k [00:00&lt;00:00, 9.82MB/s]"
          }
        },
        "b6421a35ea3b41e28d2ca6fbee66ad42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78c685a6b8114573ba492634f5467e73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff7226f4476a4938aafabae29ce4f72a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9235cf74bfb3422aace1b3c265991730": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c898874e6e6d4a19b78b8e848057ea07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "44600030d879448f857b379c944b2a14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b3cd41bbb2b4e19997a0cf18c7f4a10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5574dc268509418fb3207308beaace50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c7aec15d5c914821b9f4308eeb313cb6",
              "IPY_MODEL_defccabb0c36441f8b826af003ae3676",
              "IPY_MODEL_8f5450a0e69743eeb1efd5fe14277974"
            ],
            "layout": "IPY_MODEL_89140aadeacf4cba9b81e7ebf2bba116"
          }
        },
        "c7aec15d5c914821b9f4308eeb313cb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfa7171fe22a4029a92543e166a0f3c7",
            "placeholder": "​",
            "style": "IPY_MODEL_66f1c54dd16541c680fce5df737ae77f",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "defccabb0c36441f8b826af003ae3676": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1c9ae621a6c4925bee747be22accf05",
            "max": 125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_77408404052745e78dd0b48471fd1ade",
            "value": 125
          }
        },
        "8f5450a0e69743eeb1efd5fe14277974": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_343c06d34b7140de9d2b128b7c76259f",
            "placeholder": "​",
            "style": "IPY_MODEL_a3858bdc02f94ac8a82b0c20e5d04222",
            "value": " 125/125 [00:00&lt;00:00, 6.60kB/s]"
          }
        },
        "89140aadeacf4cba9b81e7ebf2bba116": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfa7171fe22a4029a92543e166a0f3c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66f1c54dd16541c680fce5df737ae77f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1c9ae621a6c4925bee747be22accf05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77408404052745e78dd0b48471fd1ade": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "343c06d34b7140de9d2b128b7c76259f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3858bdc02f94ac8a82b0c20e5d04222": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94b021e7ca234cb3a909b3eb1876daf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d9cf4006710845dc8d423670ccba8aed",
              "IPY_MODEL_166f47d3ba8349be829909cdd2d355ac",
              "IPY_MODEL_1367b54de3624c4498b31d853b73f6d0"
            ],
            "layout": "IPY_MODEL_9bfcd51f6e7f488b8f7730c10c761b8a"
          }
        },
        "d9cf4006710845dc8d423670ccba8aed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f371444b4d940d49c2f34ef2f360041",
            "placeholder": "​",
            "style": "IPY_MODEL_2006254a1772435f97babc3087d79866",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "166f47d3ba8349be829909cdd2d355ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa175ec534be4aa7a75d6d19c1dd4d29",
            "max": 191,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e825598360cb40308c737dd64f37f514",
            "value": 191
          }
        },
        "1367b54de3624c4498b31d853b73f6d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dcbe5a60847041ac9683e35d0cd86cac",
            "placeholder": "​",
            "style": "IPY_MODEL_4e07079174bc4cab9829f68eb21eebe4",
            "value": " 191/191 [00:00&lt;00:00, 10.3kB/s]"
          }
        },
        "9bfcd51f6e7f488b8f7730c10c761b8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f371444b4d940d49c2f34ef2f360041": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2006254a1772435f97babc3087d79866": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa175ec534be4aa7a75d6d19c1dd4d29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e825598360cb40308c737dd64f37f514": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dcbe5a60847041ac9683e35d0cd86cac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e07079174bc4cab9829f68eb21eebe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install chromadb"
      ],
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2024-04-03T14:30:00.110126Z",
          "iopub.execute_input": "2024-04-03T14:30:00.110761Z",
          "iopub.status.idle": "2024-04-03T14:30:37.862378Z",
          "shell.execute_reply.started": "2024-04-03T14:30:00.110731Z",
          "shell.execute_reply": "2024-04-03T14:30:37.861146Z"
        },
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "id": "lb13JbjzL7YP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a813f56-684a-43f5-9e16-fa6b56458061"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.10/dist-packages (0.4.24)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.1)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.6.4)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.7.3)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.110.1)\n",
            "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.29.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.25.2)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.10.0)\n",
            "Requirement already satisfied: pulsar-client>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.4.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.17.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.24.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.24.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.45b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.24.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.2)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.2)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.0)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.62.1)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.1.2)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.4)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (29.0.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.2.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.0)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (24.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (0.37.2)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
            "Requirement already satisfied: importlib-metadata<=7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (7.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.24.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.24.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.24.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.24.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.45b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.45b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.45b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.45b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.45b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.45b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.45b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.45b0)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.7.2)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-asgi==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.6)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.20.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.18.1)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.38.0,>=0.37.2->fastapi>=0.95.2->chromadb) (3.7.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi>=0.95.2->chromadb) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi>=0.95.2->chromadb) (1.2.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T14:30:48.224715Z",
          "iopub.execute_input": "2024-04-03T14:30:48.225526Z",
          "iopub.status.idle": "2024-04-03T14:31:06.148482Z",
          "shell.execute_reply.started": "2024-04-03T14:30:48.225489Z",
          "shell.execute_reply": "2024-04-03T14:31:06.147330Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "id": "h-GVrKBmL7YR",
        "outputId": "6c4c75ba-cfeb-435b-876f-d253afebc0df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.14-py3-none-any.whl (812 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.8/812.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.30 (from langchain)\n",
            "  Downloading langchain_community-0.0.31-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.37 (from langchain)\n",
            "  Downloading langchain_core-0.1.40-py3-none-any.whl (276 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.8/276.8 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.40-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.37->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: packaging, mypy-extensions, jsonpointer, typing-inspect, marshmallow, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed dataclasses-json-0.6.4 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.14 langchain-community-0.0.31 langchain-core-0.1.40 langchain-text-splitters-0.0.1 langsmith-0.1.40 marshmallow-3.21.1 mypy-extensions-1.0.0 packaging-23.2 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence_transformers"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T14:31:17.103484Z",
          "iopub.execute_input": "2024-04-03T14:31:17.104302Z",
          "iopub.status.idle": "2024-04-03T14:31:30.716705Z",
          "shell.execute_reply.started": "2024-04-03T14:31:17.104271Z",
          "shell.execute_reply": "2024-04-03T14:31:30.715619Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "id": "3LGq_NdaL7YS",
        "outputId": "326b3489-e2d2-4f39-fd7c-9da545ff82e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence_transformers\n",
            "  Downloading sentence_transformers-2.6.1-py3-none-any.whl (163 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/163.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m133.1/163.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.3/163.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.38.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.4.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence_transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 sentence_transformers-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import chromadb\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.document_transformers import (\n",
        "    EmbeddingsRedundantFilter,\n",
        "    EmbeddingsClusteringFilter,\n",
        ")\n",
        "# from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
        "# from langchain.retrievers import ContextualCompressionRetriever\n",
        "# from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T14:31:58.360066Z",
          "iopub.execute_input": "2024-04-03T14:31:58.360533Z",
          "iopub.status.idle": "2024-04-03T14:31:59.475274Z",
          "shell.execute_reply.started": "2024-04-03T14:31:58.360497Z",
          "shell.execute_reply": "2024-04-03T14:31:59.474346Z"
        },
        "trusted": true,
        "id": "jops1cdtL7YT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding Model"
      ],
      "metadata": {
        "id": "A5CZ3DfiL7YU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"BAAI/bge-large-en\"\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "hf = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n",
        "print(\"Embedding Model Loaded..........\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T14:32:56.543699Z",
          "iopub.execute_input": "2024-04-03T14:32:56.544195Z",
          "iopub.status.idle": "2024-04-03T14:33:14.955154Z",
          "shell.execute_reply.started": "2024-04-03T14:32:56.544165Z",
          "shell.execute_reply": "2024-04-03T14:33:14.954180Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "7752a9776f084d9985ee839f43874ac3",
            "de488f0641c74917aea2e8dbfce0e4aa",
            "00ada49c8bdf49b4a10d229d704b0c1a",
            "d3c1bd1702ef44e88ffcf11de3aefd15",
            "fe9aa770584542f987bd36eecb1def2c",
            "dcf1af9f0fa249c9a3775b6bf7f930a4",
            "227bef90fd254f128b0811521279a593",
            "d7728a29e1e14b46ae8c59b8a68b70da",
            "7d5448a16e3645a287d62171a8dab989",
            "5009c62ed6464f1ba076fe99ec46295b",
            "014cc5d179f9403d8ac36d65bc2fb140",
            "2ad0061bbeae40c893f96f85560ebb64",
            "1a68ab062b8f4f139fb76e24f08177e9",
            "f6b4cb3308ae4ebabb6ddb71230661a6",
            "254d9cae597949698ae287b8337490af",
            "bd189b9febfa4916b16d1e9c94cc67a1",
            "43e18eb4342a4405a59f754fa85d1700",
            "88db495107ba466d9b7e1cf7e83b236f",
            "c6927b468d76409fb51ef492252dbd9c",
            "6c71efcf68284737a34f26fb00df8283",
            "15afcc4331b146d4a932342d59a285ae",
            "d0c803b79bc34adb946772dd6b47b420",
            "1df4e1ad96274adf993a8b190690f519",
            "a6927040841d45b0b4702637ee70a20f",
            "a605da6fad7445e19c777fe5c218e57e",
            "8d22f6fa3ccf4ae5b2a5fb3df0bf9f21",
            "402f06c28e9945c6bcae965c1846ab05",
            "ca922dbc53ab4c0ea47f682d2b7fdc2d",
            "279b76fb09b341489ee444398ea58a61",
            "1dc3c26d6bd94a41a6a9bfa71a894208",
            "0b529fcf05284b2681fb37356e30ca6d",
            "43868c9079644f32b53047053a10f0d0",
            "ab30df7a463c43959492410bada19c05",
            "20b4b453c75546a08b1518bec2b6a577",
            "6ccbfb3b9dae4afeaa57916aba388870",
            "307b6aaaf8ae44b2a3f572c610e21e5a",
            "078722cc22cf44d586f871d279ff9588",
            "da13547cd5854c4383beb943836dd601",
            "af5febe34bf84ddbae11d2ea996d1634",
            "e73dfda02f0049a7a8f91563e1b983af",
            "a79c6fd66e2e4d7da2c5e1f565fd2440",
            "6e37ab7f2b0a4af6bbec0ad1ff01b41c",
            "f3c0e1eae7994a86bbb775cfda814e7b",
            "74e3e2549a4c44139758556082a953f1",
            "85f24e4aba3e4820bb1cb1051a834ad8",
            "758ced432af4498ca1b13e333bc3a5dc",
            "915daf3c844747bba899b41d75fb73eb",
            "9a0fe2adb4194e67ba5bc56bb654be7f",
            "5aa7999471a64086a46cdd02cc1b412d",
            "6ea54d0c06434ab5901acc225a6ecaa0",
            "542e832887c8483984e88e29e0fe7f9c",
            "9e9f92092f554d62ac2065da57271dd2",
            "e83ad391c36940a282f28a863a22095d",
            "fbd1e7d7a7704cf6ad4a19d1614be49e",
            "9fcd8d526c7043e1a401ea5451ec9833",
            "0fe71eb9e33b4cd6b5c0d377fd579b1d",
            "ec3238d053b946198098ffafc382a3ef",
            "308a8805ca9e4f10ac2893561288e832",
            "ffad32554828400babdfb2127e3709fb",
            "6c498d78247245f7a1d7cf7595f8c727",
            "7b50dbce355544bc8d63d8ecb2363a23",
            "c74917033b26445eba3bf0856c059d5a",
            "a9723c7500e64df58b690ea1d32f0bc6",
            "1c432e6eb48d4ddf99c359dee5983560",
            "fffb656ead79424ca518dd682b0dbbd2",
            "fa4c5adff2db424098b0f55c1229a042",
            "1a5ae0ef33a44397a221316e751b1a0a",
            "1fe03de424ec41e4a3da120254a9b53e",
            "56379fddb71846cdbb6b4b174e7375b9",
            "df0224f221c849278a2522b6d17f762c",
            "40d3cd2b8dd24d4a896fe504d4ed9aed",
            "788d083b277e4a4e91357cc6fac0d9fc",
            "5c7ae5e6585845bab2439b45b8b87d37",
            "153950f99f394d76b367942b9114e1cf",
            "6531f377d88a4a0ab10e6277dbd043e1",
            "9a7aaa8bd38f4d51acaab1805286fbfa",
            "4461b686785d49bf92ab1d4afdf5fd7b",
            "152e642ea173418c90e70872b6081bbf",
            "2eac3df7fada46f1a3d5ec1916b2b7f5",
            "94f03508843f4c1ca08cd1477b96c2a1",
            "00e0b269f8ba449daafd58c38937e29a",
            "861aec7a70bb4c508236add9b49d133e",
            "50192445f89d46019ef0bc61a7079b8a",
            "e7e95f79446c4f5b996c3073d1efdbda",
            "eb6cfd3facfd44209d69980d23d7a6c8",
            "2ab7767324ce42c6b1ecc1f1e83a3528",
            "f32b9c84d1d64035a7eee3c23d1bc319",
            "d4cb78badf0e4b78b6402b6dbc4701c8",
            "3e74c193557046c89a9c7b1f59156054",
            "3d9d10c1046443a5b98081d979f1c849",
            "b4d345deaaa6428ab37019f0410e800e",
            "9dc6156cfc824e7c90761856b99548dd",
            "b6421a35ea3b41e28d2ca6fbee66ad42",
            "78c685a6b8114573ba492634f5467e73",
            "ff7226f4476a4938aafabae29ce4f72a",
            "9235cf74bfb3422aace1b3c265991730",
            "c898874e6e6d4a19b78b8e848057ea07",
            "44600030d879448f857b379c944b2a14",
            "7b3cd41bbb2b4e19997a0cf18c7f4a10",
            "5574dc268509418fb3207308beaace50",
            "c7aec15d5c914821b9f4308eeb313cb6",
            "defccabb0c36441f8b826af003ae3676",
            "8f5450a0e69743eeb1efd5fe14277974",
            "89140aadeacf4cba9b81e7ebf2bba116",
            "cfa7171fe22a4029a92543e166a0f3c7",
            "66f1c54dd16541c680fce5df737ae77f",
            "d1c9ae621a6c4925bee747be22accf05",
            "77408404052745e78dd0b48471fd1ade",
            "343c06d34b7140de9d2b128b7c76259f",
            "a3858bdc02f94ac8a82b0c20e5d04222",
            "94b021e7ca234cb3a909b3eb1876daf1",
            "d9cf4006710845dc8d423670ccba8aed",
            "166f47d3ba8349be829909cdd2d355ac",
            "1367b54de3624c4498b31d853b73f6d0",
            "9bfcd51f6e7f488b8f7730c10c761b8a",
            "4f371444b4d940d49c2f34ef2f360041",
            "2006254a1772435f97babc3087d79866",
            "aa175ec534be4aa7a75d6d19c1dd4d29",
            "e825598360cb40308c737dd64f37f514",
            "dcbe5a60847041ac9683e35d0cd86cac",
            "4e07079174bc4cab9829f68eb21eebe4"
          ],
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "Nq-kyjxtL7YV",
        "outputId": "ebfddf15-0995-4e66-c181-0e6f77f0b38b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7752a9776f084d9985ee839f43874ac3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2ad0061bbeae40c893f96f85560ebb64"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/90.3k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1df4e1ad96274adf993a8b190690f519"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "20b4b453c75546a08b1518bec2b6a577"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/720 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85f24e4aba3e4820bb1cb1051a834ad8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0fe71eb9e33b4cd6b5c0d377fd579b1d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a5ae0ef33a44397a221316e751b1a0a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "152e642ea173418c90e70872b6081bbf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e74c193557046c89a9c7b1f59156054"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5574dc268509418fb3207308beaace50"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "94b021e7ca234cb3a909b3eb1876daf1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding Model Loaded..........\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pre-processing"
      ],
      "metadata": {
        "id": "5DnjTWyUL7YW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.document_loaders import DirectoryLoader"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T14:33:29.091387Z",
          "iopub.execute_input": "2024-04-03T14:33:29.092275Z",
          "iopub.status.idle": "2024-04-03T14:33:29.105448Z",
          "shell.execute_reply.started": "2024-04-03T14:33:29.092242Z",
          "shell.execute_reply": "2024-04-03T14:33:29.104452Z"
        },
        "trusted": true,
        "id": "EuHqhjLRL7YX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and process the text files\n",
        "loader = DirectoryLoader('/content/', glob=\"./*.txt\", loader_cls=TextLoader)\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T14:33:32.022964Z",
          "iopub.execute_input": "2024-04-03T14:33:32.023966Z",
          "iopub.status.idle": "2024-04-03T14:33:32.099505Z",
          "shell.execute_reply.started": "2024-04-03T14:33:32.023932Z",
          "shell.execute_reply": "2024-04-03T14:33:32.098639Z"
        },
        "trusted": true,
        "id": "b6Gaqr5YL7YX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(documents)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T14:33:35.472943Z",
          "iopub.execute_input": "2024-04-03T14:33:35.473678Z",
          "iopub.status.idle": "2024-04-03T14:33:35.480483Z",
          "shell.execute_reply.started": "2024-04-03T14:33:35.473647Z",
          "shell.execute_reply": "2024-04-03T14:33:35.479435Z"
        },
        "trusted": true,
        "id": "7Rxxb-KpL7YY",
        "outputId": "08e8c4a7-8b7e-4b4c-e4a4-8b17eeb6bf39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parent Document Retriever\n",
        "Version 1 - Smaller chunks"
      ],
      "metadata": {
        "id": "zI5QaEbBL7Yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.storage import InMemoryStore\n",
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=120, chunk_overlap=20)\n",
        "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=20)\n",
        "vectorstore = Chroma(\n",
        "    collection_name=\"full_documents\", embedding_function=hf\n",
        ")\n",
        "store = InMemoryStore()\n",
        "retriever = ParentDocumentRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        "    parent_splitter=parent_splitter\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T19:47:21.633883Z",
          "iopub.execute_input": "2024-04-01T19:47:21.634577Z",
          "iopub.status.idle": "2024-04-01T19:47:21.646584Z",
          "shell.execute_reply.started": "2024-04-01T19:47:21.634544Z",
          "shell.execute_reply": "2024-04-01T19:47:21.645722Z"
        },
        "trusted": true,
        "id": "1p7-H3CBL7Yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.add_documents(texts, ids=None)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T19:48:41.000754Z",
          "iopub.execute_input": "2024-04-01T19:48:41.001609Z",
          "iopub.status.idle": "2024-04-01T19:55:25.980891Z",
          "shell.execute_reply.started": "2024-04-01T19:48:41.001578Z",
          "shell.execute_reply": "2024-04-01T19:55:25.980016Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "04c42c0e8efb4f6cb86be653a2e16c7b"
          ]
        },
        "id": "k-By_EODL7Yd",
        "outputId": "e3c80f66-94c8-4560-9bd5-c750eca95e38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Batches:   0%|          | 0/138 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "04c42c0e8efb4f6cb86be653a2e16c7b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(store.yield_keys())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T19:55:38.816907Z",
          "iopub.execute_input": "2024-04-01T19:55:38.817288Z",
          "iopub.status.idle": "2024-04-01T19:55:38.841037Z",
          "shell.execute_reply.started": "2024-04-01T19:55:38.817260Z",
          "shell.execute_reply": "2024-04-01T19:55:38.840122Z"
        },
        "scrolled": true,
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "id": "XtziT-MBL7Ye",
        "outputId": "5f28b29e-53a2-4e4b-e228-e83a96b8f4ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 31,
          "output_type": "execute_result",
          "data": {
            "text/plain": "['c2248c5e-2681-4cd7-ac8b-8ff573af3be1',\n '8a4a6aeb-a2c6-456b-9580-f234bd8376fc',\n 'f6dc19de-f7b6-4d4f-8cbc-1726cfd8ce0a',\n '60355c21-a7c3-457b-8744-7f23c47c844b',\n '79650765-2010-4873-9714-cb0eeb64936c',\n '01ab28b7-2e20-463e-9291-bff69fe63c04',\n '9e3dad93-c7f1-4f04-b9b1-2adf8c02c55c',\n '046940db-460b-45e4-8b1d-954c2392082a',\n '00f482b4-b3a6-4145-817c-d5e8fb996e38',\n '3756a67d-a84b-4f99-9751-52eb1cf70fab',\n '1e7effac-a2fc-4ef7-b810-6babaca0c0e5',\n '9d902f85-8545-42ca-8671-87c098b113f8',\n '9c15a39f-dfd7-478b-a785-9b2fdd77b7de',\n 'c56c7843-d532-42d1-90a2-25cf6fbf1cba',\n '531d941c-5508-49fe-8508-9eae636d23da',\n 'e37c950c-6f30-4e35-a266-b6c98917cbf1',\n '47c51f89-2392-4ffd-9144-4b8583369534',\n '1c2283c5-6822-478d-baee-6d58a4103218',\n '511fd012-7a9e-4f6b-b2cd-b7947fa951bf',\n 'f24bec68-8338-4795-adf7-08db914f72ff',\n '1626169f-01c2-4252-9dcf-bb1d8ba52070',\n '175cbd3c-6c33-4b0b-a21d-f3e8616b779f',\n 'd1aaf10b-032b-4d5f-910e-4ec474261638',\n 'a86274f3-56dc-4972-b5a0-c4388d4215bb',\n '4271313e-efe6-4502-a4fe-a272c4460de4',\n '6c9fd234-9120-4df6-b170-372d06a8610d',\n 'c6f3b054-aa76-4287-85ff-84c208341fea',\n '8ca54ae5-491a-487b-9406-2ae9bef2514f',\n 'da76be7f-078b-4e9b-9245-6273d4a69c8e',\n 'e559c2d4-e0fc-4493-92fd-e10a9bfaa163',\n 'b2abf488-4098-4f22-9cbe-aa38c217f626',\n '17299b48-e370-4906-8adc-319e805d3048',\n 'd55849d9-7fbd-42ff-9282-5dbbfa424e50',\n '2ef07a63-c83b-4994-bb87-497f675ea6a2',\n 'aec55e56-b89e-4e9a-b2e5-4c569d37fcc4',\n '01880040-7bd0-4f21-a848-3d73d44a4b6c',\n '186abe8d-ab95-4c98-99bb-f415723a5550',\n '1fdd2a33-050c-44ef-92ae-0fdbae01d4cd',\n '5ecb5ca1-e15d-42bc-9bc6-9f6aad803224',\n '2c763f7e-1413-4351-b3c1-5d87461fe1de',\n '9e77460c-79d7-4f51-af70-e3fb31c8a044',\n 'bb06c1c3-7aec-46af-b630-6d52abeafc52',\n '3fc5b55a-6c17-490c-9071-b9e7acf21d44',\n '726fc235-c76e-4dab-aa93-090d83150a73',\n 'd33c289f-ede6-49e3-a58a-fd50ffa79b71',\n '26ed4c42-d329-4de2-bdde-66e2eed88c04',\n 'a498d5bb-6034-452a-9dc2-c5816983e1aa',\n '4a36838e-7a0a-4e18-b4f4-9f45531df182',\n '487457b8-36ef-4e60-a00b-624d420e8a77',\n 'dff77578-2fa3-4993-b80a-6036ecb91b43',\n '20fff65f-3606-4838-b760-e0bf5c8eeea7',\n '6c4380db-6650-42de-bca5-4f5928f7092d',\n '68d26b73-ae1d-4348-89fa-103c742386fa',\n '15893189-63bd-4cbb-ab64-b2076283b6b0',\n '42666a40-dd45-4212-a469-a72602634f4b',\n '81741182-b8bc-4deb-b4dd-aa6a131059e0',\n 'acd6fd95-0202-4f99-8c00-0af871f737e0',\n 'ecd7eec9-bf5c-451f-ab2b-c163c2c4e22f',\n '40ba8b2e-ca87-4c24-a7b1-8f41d1afa3f0',\n 'b4cd85e2-5a32-42a8-a563-24401d8b5b0f',\n '59016d1b-215c-41a0-a760-81eb35482947',\n '9a7fcf58-4ae2-4317-b516-1d6cb9257913',\n 'dfde5bc2-6722-4799-a5b2-c8ad541ca63a',\n '5bd190d0-81fc-4cc1-a067-1b3b0f639921',\n '545fdfe6-441a-4325-9514-f508456d8988',\n '367fd306-7af2-48b3-8c5c-04dd84f1218d',\n 'd991dbf9-4e27-409a-8f09-107be2a2d56e',\n '3290f374-c086-49b3-b613-2c4d32e7ff4d',\n '42d99c64-84fe-4581-b987-fa6e0be71a04',\n '93ff9bc3-727b-4530-9e36-156f29684989',\n '9cd34384-c5fb-4b4b-b89b-4a2c19adb931',\n 'f965128a-518c-45c1-8d9d-64aae20251fd',\n '95e2fec2-86d3-4efd-a000-d3c7397a75f3',\n 'b8951ce1-f0a1-4e7c-bb44-332ff0979da8',\n '73640849-f1e0-4876-86c9-bcfc97fbf5a8',\n '044337dc-f77c-40ea-ac78-9be18303458f',\n 'c8c8156c-d5af-4688-bbdd-65025c241884',\n '1cae4427-a129-46bc-aaec-809f95060c7f',\n 'd9a56c3d-4b3f-4fca-a608-1dda1ffc2931',\n '1a470994-4d44-4b86-87e2-fbe25dea9ed6',\n '7869df9e-b18c-4d74-9eb4-9e3164ae014b',\n '22535b69-c543-4aad-b66e-5372eebfb994',\n 'b1ef7e16-a8e7-42f6-93c4-2e4332a57afc',\n '5606da43-d4ff-4b0f-92a5-f15d220358fc',\n '857d82a8-3a74-42a3-9482-1d0dd654e909',\n '5930364c-f6d2-4aca-9305-c71e00414e80',\n '1ad81cf4-2aaa-402a-bac7-ddd55baa31de',\n '6d3efd32-2fdc-4311-a8d0-872762fde158',\n '5930fd61-4d49-4e9c-96b7-71d2dcaec6b8',\n '24d870a3-bf9c-4f88-9517-815c87ddcc80',\n '3c6b01a7-0a71-4ff1-a88a-797949b21da8',\n 'b53bc554-29b5-4e79-9aa9-52a5a7c9d7ef',\n 'f8551fc6-c85d-4105-a0a2-0f4aebff12fd',\n '09a36855-b2bc-4a55-8e80-ec8047ed8aff',\n '457e0bf9-7f65-44f5-921d-6a18ecc0f694',\n '316e6643-0d06-42a8-91d8-294cd97cec2b',\n '9418d465-a74a-4552-8cc0-3b4f0f701a9c',\n '66f59b7f-a6b8-43e2-9de0-6ea3cbe8bd0d',\n '07211b53-89cd-49d7-977c-bf67ac55df8e',\n '524be5cd-b665-410b-a418-7c9854f6c592',\n 'ded3336d-90b7-41b0-9c19-6d366e9c9d5e',\n 'fb64c5f0-cec3-482b-8998-596ace090ecb',\n 'ee64808b-6b46-449f-a942-ab5d728fc2d1',\n '70ab6f09-6682-4053-b35b-46a538b1a689',\n '78b8e670-262f-4ddb-b744-4846c505ff05',\n '603a6405-cd83-4a45-a4e4-033fa8192394',\n 'cc7a57ed-3c79-4be2-886b-4ea920661bc0',\n '02d0ee65-9c73-480a-8f85-fdb5a6bd4867',\n 'f1b95ce5-d9e2-4eb1-b53f-f26771fb13b7',\n '68d5fc29-6bb1-4b75-a97f-cf387121e08e',\n '85de9df3-2440-4d70-8742-ee70e990306b',\n '8d18f8f3-1142-4da9-8852-f8fe5671132b',\n '33a2d253-f11a-4bc9-ac62-9403bc2f1056',\n '6470ca93-d778-4935-8c59-f4b078e6fe69',\n 'f6f19ca2-aad2-4ae2-841a-d3675f78646d',\n '07e27084-11ec-46c4-a129-d6d4b6fd5424',\n '20c39a1f-28dc-4316-b145-0b89deb223da',\n '04cef5b1-9cd6-4e41-b28b-873a466398a9',\n 'a7b26db3-e90d-4acb-aa1b-b2ad6deef02e',\n '3a90d581-36a9-4244-99a2-ca07295d0751',\n '3ebb3b18-d6ac-40c6-a987-275c081c5e1b',\n 'd6983415-ac33-4ae3-8fa0-87b7fddd18d5',\n 'e7755463-a024-42cc-a322-585c28dcacd3',\n '542b0fe5-117e-4f38-84a8-4ab99b56b0cf',\n '9bd55e5b-12a3-4f2d-a6b2-1dcab9c66297',\n 'c3364b2e-6c43-4d3b-b300-f4e39d272e8c',\n '0bb03a56-41d6-4ba0-aee1-5e533e42caac',\n '229f1452-431b-4bec-8c0c-a6296cc26b61',\n 'e17cc46e-ef1e-4081-8c4a-610cd03510b0',\n 'e6616b20-d114-40e2-89f7-bdc463494a0a',\n '32dba0e8-b0c9-416c-b149-cf2dbef79e2c',\n 'c5b2eed7-d3a8-4915-afa4-710eaa63186d',\n '926ec797-f1cb-49e4-96f3-681d14c3cad6',\n '6e1235f0-bac9-4f81-bbc4-95e14bd72541',\n '97f4623e-c051-4279-a8bd-6401ed9a1807',\n '41f1b25e-cc73-4f22-809c-4b0b55a01efa',\n 'ccfc3d1a-96f6-4f39-a22b-f78f12f1e40d',\n '50d2997a-e842-407a-82ad-a4802d153962',\n 'b526f128-643b-4a8b-99c0-ede400e81d0a',\n 'd338263d-184e-4a28-907a-79eb0d0469cc',\n '44758285-7b32-4163-b358-2592772fb988',\n 'ecf763b1-f91c-4a50-be4c-a79c479d8296',\n 'b3cc7def-9d0b-4f37-bf75-462fe63cdc38',\n '39bf780b-85d3-468e-ac57-43b8f928f417',\n 'f06de728-08c0-490e-b51a-7b3fce58929d',\n '133b7a02-7880-4df3-a728-8217ca7716e9',\n 'a15e9aa7-648c-4eeb-8caf-e8d61132a043',\n '81a0a75a-0d87-4a3c-8304-0df76c25eafc',\n '23d6980a-39ef-4ba5-a98e-8c6f033d32b5',\n 'fa9ae287-2837-4982-b1bf-26feeba3316c',\n 'ddf8e1a8-48a6-4b99-a733-a1e34273d68b',\n 'f9f73e97-5e94-4b81-a92a-6e1badd40ae7',\n '7013a6ca-e016-479b-a0a5-04b9ce1aa9e4',\n '24b5074c-8645-4b8f-a113-3bf6af78e0d8',\n '4a4582ae-838c-493b-bf9c-efc038d80983',\n '1246c853-db74-4a00-9e72-8164f32b26e6',\n 'ca12def5-97a1-4555-b21b-5398f9eeb94f',\n '0121da60-e2a9-4a0f-bcfb-aa260eceaac7',\n '20e7493d-13ba-44cb-b3a5-20f75074935c',\n 'd1bcae6b-5149-4105-9f54-6d1e03514d71',\n 'fe43231c-4a95-47fc-8136-c768320eb028',\n '58bf30b8-7855-4532-a56c-47d1b7782562',\n 'a509066f-0f9f-4e6a-8bab-ca2a69b00f8e',\n '7a089a1a-dbef-4665-96b6-5dc7f390bfc8',\n 'af6644e1-721e-45b2-8267-dd2313083e72',\n 'b4cc1168-5d40-4a1e-a3b1-91d736a17d91',\n 'ce03fbda-1912-4256-bb59-aba86c028d9b',\n 'e1ffb46f-0e10-4b69-963c-0b5ec691a6e3',\n 'c8fef71e-89d1-4548-961c-7a480d921e2d',\n 'ed58c8c6-85b0-4c21-912c-2dd02dc10058',\n '920dedc3-e31f-4442-958f-e65c3b4ea26c',\n 'd7332245-5961-43a2-bc08-05ac9781f87f',\n '3f66303c-d6db-435c-be02-696bcd65f37b',\n 'bb1d35be-46b4-43f5-a416-fde3aa16625e',\n '920ad88d-5386-4437-b6bb-6cd4a4a12d24',\n '250f211a-611f-4d56-91cc-d4ac7313f330',\n '853ccaf5-f1ba-4271-a59c-826c0507d25c',\n '2369b850-e703-41b7-a9d9-674b895c1482',\n 'c3659ca4-0652-4b67-ae12-9c08ce960106',\n '705dbe70-87ad-42a1-a62e-1f65f002dbea',\n '6e49254f-d82f-495d-8175-ec2376e6fbe4',\n '89f673b1-f688-4abc-a775-17d85ab482cc',\n '3930fa72-19f4-4f7d-b37c-ead7a1b33af9',\n '75db89b5-5f70-4611-89f6-7b1f481e9f3a',\n '92f48ab2-128b-4c2c-b37b-f975014e33b5',\n '02c8733d-2ddf-4546-9ed8-01fb0ad5dab5',\n '1ece8c39-a319-48c9-aeef-79487b62e187',\n 'b08ccb30-71a2-4cce-b543-042762e86ab2',\n '6f721e4c-b965-4832-9e45-bcbba72470da',\n '561d44b5-baf5-4c42-936c-1c9f80a9d3c6',\n 'cae0d85c-9d1b-4123-a55c-ea8198033341',\n '7fa1a9d8-c585-475b-91b6-fa199506f0f0',\n '3ded15c3-6f4b-4c7a-923d-54b43a61ccec',\n '9389f8d1-1ca4-4111-bf2e-b16af208b654',\n '325e9426-a354-47c3-add3-3f9d077f214c',\n '1592a108-e10d-4544-a98d-1ec2ed948ff4',\n '7933e60d-c0e7-48b8-b3db-e84532763bec',\n 'ccbfccbb-de3e-4192-bf99-6a6e070bfc76',\n '9c08d75b-2443-4d55-ba1c-53ec59f36ec7',\n 'f40707c1-725c-4af7-ab2d-445bd81f25f8',\n '51dc847c-92d5-4326-bdd4-7608c6cb9067',\n '47131e11-e552-4917-91f4-7dd8b74690f5',\n '3eb28d19-027c-4e80-94fa-6961de8f102e',\n 'd03c55f5-d2a0-4997-a57c-154115ce80d6',\n 'ddb9aa9f-8f39-4607-b720-e2d63e5eacce',\n 'a64ddffc-64c5-4463-ae66-e61639e7cf62',\n '2bc5bd40-582a-42b2-9388-54b20802fd13',\n '7572b90b-f172-4961-b2c1-dc8ba4c69ac7',\n '16ca5bfd-d5e5-4205-9c39-a5ceb1a5723c',\n 'd23c6e1c-008d-4c0f-857a-b067a56d4505',\n '393600e3-a217-45f9-a70c-972aaf8e9007',\n 'b3bd822a-63ad-475d-85d8-8c61b9671dd0',\n 'c3364820-242c-4a66-a3c3-2b0ab59b893e',\n 'b00917e6-cf38-4bda-a16a-ba5628bce828',\n '04e49b62-6a29-4531-b188-5b91c3faf9d5',\n '95e6464b-644e-4525-a6f1-b2216105f877',\n '939ab0dd-7ee0-43c9-8802-d30a9bbbf16c',\n '7752c437-4cd9-4c90-afad-5b751b9b76a3',\n '97d4925d-26b5-4ba1-81cf-ef50f01ff12c',\n '22860566-3af7-44a1-9dbc-6f6cb2b01971',\n 'a72a9332-887e-4a24-a5f6-abb5a6986747',\n '9b722568-5ea1-4f92-87f2-874538b0a499',\n '512e73ce-dd57-4bda-852a-35924648333f',\n 'b3877e50-2ab6-4c87-b52f-40ab992be902',\n 'b327a2a0-0283-4e42-8cea-92c76e567c91',\n '9021191e-7ad4-42f4-90dd-63627cf0b1bb',\n 'ecee8b35-9a37-421b-aa22-99ee7ea201de',\n '056cf8a3-9274-42aa-acc2-5844faea3be5',\n '409039a3-4be2-4f1a-8016-68e2352f097d',\n '2dbbcbd4-9fcc-4750-810e-314b03cba15c',\n '605ad0b3-74b5-4b1a-9e2e-85a4384999f1',\n '32a2d4ef-5543-4706-9154-0e78cdd59bfa',\n '4bd4b040-30ba-4680-b78a-b521fb339a18',\n '8353f565-a70c-4b7d-9d7b-4ebc4f0eb46f',\n '5d80a51e-81f7-46dd-b452-52b42e6161b9',\n '2df54d51-f0b8-4416-bc93-aabf314575af',\n 'b0f09fb2-4503-4bc7-bcd1-88fc89b2dded',\n '0fd59877-d1b4-4ffd-99c8-ab318699d798',\n '930df910-60ad-4814-a809-f8e83210626c',\n '2ee5ca25-270c-4f49-8cc3-f7ef5a7e3f8c',\n '67f31beb-0e50-43a2-9186-26208b6ac45b',\n '71d2fa37-80d8-464d-909d-be35a1b8c591',\n '03bc5a96-b9b5-4840-9358-fd404d45b04d',\n '23eaaac7-5930-4617-8c83-28513e69b1b5',\n '4f81ecb2-ebce-46e1-ba9d-602a86f0b3c6',\n '5c7e6e0f-d673-459a-bc23-bb6c094ca785',\n '3b80c79e-2b7a-427e-aaca-f3bcf7e43b96',\n 'ab5be7cc-d773-4f58-b297-79b46d3eb187',\n 'a056e13f-5cac-4cd0-a539-3252a49729a6',\n 'baf3ec3b-1ca4-4a32-a9b4-c193743294b2',\n '67443ca0-ff20-4e98-b3aa-b5d4df70aff2',\n 'dc8d9ba9-d90d-4359-a434-af74647f974e',\n 'a22d85fe-8e58-4a82-9f3b-045d5be069b0',\n 'c01494c8-aeb5-4c84-b995-6ecc5e8965f6',\n 'e303b4ce-dfee-4f67-a16f-2867cd32d175',\n '4c48b47c-876f-4d34-a141-224ec272fdfb',\n '766d637d-29d8-4260-916e-9febe53cbe5b',\n 'b0a2b6e5-332e-42be-b793-30e0514e13de',\n '90df948c-2186-477b-b28e-793fed3b0335',\n 'c8458593-ae22-4f5c-8c76-9d995084432c',\n '5aa93f1b-5e97-4994-ba80-6de44bdba881',\n 'cefa431c-aaca-4053-8068-4fe11943f0e6',\n 'f6a440a1-6d00-4af2-9fdd-8c7eb596450c',\n '6df76d7e-0dc2-450e-a828-d40675ccf7af',\n 'bfc6f8e2-151a-419b-a667-66ae3b44d8f2',\n '075950b0-9341-4596-b9f7-cc90de8e19b4',\n '880265cf-1344-45ad-94e0-b3d208d1b0b4',\n '468b63ea-e08e-4952-9396-53f6e666bb1a',\n '31a08e1b-a9bb-494f-b69b-45f60bf7ac4e',\n '77070b2b-8f12-4ef9-a67d-1757803b7f39',\n '71b19adf-2a6f-44c2-b0ae-1740d34d1a6c',\n '35c08f85-8086-43de-a74d-a0da571cf335',\n '74d89993-69d6-4fd2-9e66-cc21d7954c93',\n '28255b12-0afe-49c5-b23e-6bd724594d72',\n '952e5488-7882-47ca-83a2-50b2e0d00b83',\n 'ebd00a4f-00d6-4b68-932e-d598d2aab307',\n 'eb351860-0b52-44a2-8482-5461ad70e0cb',\n '9d3747f6-e876-4986-adbe-46bb66522d39',\n 'a3f566a5-9ab1-4e90-8ed2-04013e4c41ea',\n '450f422c-a63d-4ade-ba65-59504e74621c',\n '186ae833-29a1-46b0-a571-1dd4e01c0df7',\n '3f5a0296-4610-49f5-8d63-a92bd211b8ce',\n '33b4e040-c124-403a-9dcb-cbe796a2eeb3',\n 'cad463d5-02bb-40a8-b9ef-5cbaf859c106',\n '6090c698-4ff8-4084-acbb-774aed55d19e',\n '9005f02f-e9c0-487e-a540-099f5ece8525',\n '2617a62c-841f-4d6c-9264-5cbba62cee7f',\n 'e0fdd608-3c82-4524-96d3-6cbd4b239d7b',\n '35195879-d77e-4073-ab27-cceee04913c1',\n '808678dd-ec7c-4fe2-b68c-6280b1591d8f',\n '39381815-f916-460d-ad52-76e7a2e93f90',\n '9b87edd6-0dfe-442e-84cf-6ec7c3adc5fa',\n 'b45af084-d295-4b24-b228-75b7b8ed3eec',\n '25476a6a-3283-4e13-8aa3-c2e8d2c63da6',\n 'e39a9bd3-774b-490d-8283-8a36b33d9529',\n '2adf346c-03fb-4513-ad62-060ed1eef301',\n 'ec32e80c-ee62-4d7e-b76e-a45ffe751442',\n 'cd270d27-786c-4323-8249-07e46039a708',\n '553cb785-e61f-4468-89dd-2a8d3f0fecc1',\n '69209d8a-7648-49d4-ad86-7693fbb647e6',\n '810e8974-d1ed-45e3-8728-c7ca3c4bd2f2',\n '8cd34485-2854-4e4c-a0bd-f1b9675c4b28',\n 'a805c2be-92f8-48e6-bc62-29c3246092e3',\n 'eba5635b-e704-45d3-a5a8-ac1a3f9f973b',\n '55354ef2-2c59-4e5b-932c-16313f04ee32',\n '21041307-9423-4c32-b08b-4f5a78bbae30',\n 'f90126e0-dffb-4527-aaa9-faaad5fa3c76',\n 'c6d88680-1611-4691-badb-426a7f742aa4',\n '0fb30f9a-1706-43db-b088-ffc5e54e2b99',\n 'df6c6813-fb5a-442d-b219-9edbe1dba961',\n '3ee25eb7-95b3-47e5-978e-8e4fb04f8b40',\n '64096d5d-e007-42b9-8947-a57f83ee9db9',\n 'ad6e804a-a22b-4629-b53e-d14fdcf5952b',\n '5cb092ab-6820-46e7-90c8-c07d2be9ad3f',\n '848b1981-8ba7-4089-aa76-62acc33815da',\n 'c8f31a99-17f2-4097-9f2e-3b1f3c74b1b9',\n '20dc0a26-bf36-49aa-a7ca-36e5c56c8da6',\n '94d00322-8b6e-4506-94f3-a9402c59f5b3',\n 'ebced261-0d48-4773-84d6-e0f2ac60826b',\n '1070be1a-319f-42fa-9ec4-d5a5ffb80e6f',\n '22721c55-1fbe-43da-a00e-71477b446c2d',\n '64570259-0e3f-44d9-8a9b-072bc57a34e0',\n 'c911ae41-d21f-4ea1-b4b2-b4954d47e2c9',\n 'fe45d78d-2104-42f1-a3de-02be5600ca1a',\n '1534dccb-8f39-4214-a4cc-43a069c24c0a',\n '1000a60f-f0b2-4851-9cff-595343d11cfd',\n 'b154cf45-1cda-44df-b79a-74ff9a31cb54',\n 'b937b64a-a2ce-4839-b056-0241215e138f',\n 'eea03589-b93e-4597-b1fa-dad9707ec560',\n 'ab54ce5b-ba80-40d4-a73b-f7ecb74e1ff5',\n '41e07e24-1bc9-4c5e-9f90-72068576f400',\n '7fc2e141-1af0-4872-9912-d210c1c54fce',\n 'f5a12e01-d4ec-4c40-aeb7-7407d34b41e4',\n '3d3e8887-7173-4c04-8aad-45634826fde9',\n '5b8bee38-8e8d-40ca-b269-92e4d6ed21ce',\n '16f70350-b4bb-4aea-b8f6-eda7b1275bf0',\n '773ef89b-2ade-4521-b027-225d51cce620',\n '3da650f7-7e07-4fb7-b271-a637cd21662e',\n '29ba05f1-38e5-4352-a8d5-3a89065c975a',\n 'bdb466d7-4789-4b0e-8ab7-f0cad7cea768',\n 'ce853ae6-d47d-4206-8734-f6d78405af52',\n '778b840b-45bc-4eb8-bade-f7fc1f1afa4c',\n '7fb14acf-3d94-4ffd-90cb-3bd2d396cf6d',\n '8719ed8c-914a-498c-bda2-ba468ffa9712',\n '30bd565d-7f75-4209-bd03-664e474448af',\n '991d5212-be17-4f07-a71d-04dad894d92c',\n 'dee45f98-c96d-44e9-ae33-14dbd94be83f',\n '760ee4f1-b280-4ba1-993d-ea519fe62348',\n '81f70ea4-f463-4740-ba8f-bde85701ab1e',\n 'b5b062a0-a351-4d27-8ff1-3628cd15e599',\n '7d974e2a-2a63-4dae-a836-e75376ff4953',\n '178c7b17-1653-41b3-ad62-36575fe4e80d',\n 'd88f8b30-c873-4b0f-82a3-15f0c9de1cd4',\n '96065367-78e5-40bb-b4af-bea88a0892b9',\n '384b8169-8fbb-4886-b40f-77c50b3828fd',\n '61567d68-b7c7-4d61-853e-654f4b13a985',\n '49a7a807-9c6d-4360-adaa-c8918f776668',\n '1918feb4-dcdc-4437-bf0f-9f9450af7231',\n 'a3776800-812f-46bf-a28b-da4e73b264c0',\n '7426d0b6-9660-4f2c-a2e8-50685ca79f28',\n '9a2bc133-7b02-4cf0-827d-e854b83b09fe',\n 'cdbde395-d3d6-4088-8607-ffb3451ae7ee',\n 'dda47545-1158-43d5-bf8d-7c675a9e1e8c',\n '5c280885-ed1b-4fe0-80c3-01052cf3144b',\n 'efd84784-67bd-465e-8ffd-af09fc5a1fb3',\n '60b0f735-48a4-45ba-8327-3002b021dbb8',\n '1280e6ef-c9d4-4b93-bf7b-e3527fab6c50',\n 'd729f445-a10c-479f-81ab-a281c2edafee',\n 'f90c686d-86b6-4052-b153-41bd630e22dc',\n 'c07c0388-ce1b-4ae8-8c7e-5c1a65452ba7',\n 'b2350200-c68a-4899-9757-5d1d3afafaa3',\n 'a6c1dd24-cd91-428f-90dc-d01ebf3e9378',\n '606e412a-190b-4cfb-8db1-ada188c7c490',\n 'c1180f36-1d7f-4e9f-a606-a8536a4c441a',\n '72bd2b90-ab32-49e2-b7b9-e6e955ad7310',\n 'fe91cb77-48e7-44a2-9bcb-89f2ab3db65a',\n 'f0e3c00c-cd0a-49ec-8c7a-83b744a18214',\n '4e343a0f-c103-4a8a-a846-1ad9da2ccb88',\n '2fba0e97-1787-4b83-97e9-54a0600c536c',\n '3642bef0-3568-4715-809c-6116dc2de425',\n '55c432a9-4787-49f2-853e-4117399ad1c5',\n '39e9c583-dcfd-4936-8c5c-6ffad088e0d5',\n '620b9662-5a8a-4d0d-9040-6b4cc5cfd0cb',\n '37ae1efc-44b9-41dd-aaa5-246ded6ff554',\n '59d2e378-2549-4abf-9890-81a1b13365a6',\n '44512901-a6f8-4ee0-849e-6050003d0acc',\n '68a2c886-0831-44f9-955f-2479a09e6a09',\n 'a04086f7-fd5a-41c3-ac6c-e830c0797ca1',\n '045b7e34-b5c6-4eed-a456-9a5aef476eff',\n 'f07a236e-c10a-449d-b429-542a1ac01a1f',\n '5e40e369-9fec-408b-8610-1dbab9a48795',\n '375db8cf-3861-459e-a6b7-a5690ff79098',\n '29f0701d-5987-4fce-8271-e4c43a398f3b',\n 'e68154a6-82ac-4d48-bcf3-8bbc78bfc01f',\n '84b7ac09-d7cf-4dfc-a30f-bfa00a305a58',\n '02365b3d-399b-4bc8-874f-bafbad8c23ec',\n '176242b0-9951-4339-a137-24be99017784',\n 'b42d0ba8-76b1-4f46-ad9f-e0786d63ecd1',\n '4bf5822b-fbc6-4c4d-8ae4-a3c3ded4419d',\n '372112e8-f29d-41a9-a479-909aeefe1437',\n 'eae1a20f-00d5-4b0c-94c7-b000bd7771e8',\n 'b95b4f4a-ec6f-49a2-8a45-1203bf0b64ca',\n '5f4611bd-9afa-44d5-9959-28e6c0ea743e',\n '21596a9b-94ce-4bd3-838e-876cdefa2004',\n 'cd68b3f0-6fea-4219-90f4-ea0de3081544',\n '96909098-eb03-480a-924b-f59d1e1c1d8d',\n 'dd91d563-b83f-4784-aed5-7fdd125a3310',\n 'fc931a2a-1be9-45a4-84a4-b65e85bfc895',\n '58a854e5-50c2-483c-94da-2a9a39218793',\n '2d7b2e7a-5a2d-468d-a7e3-61d4cca73ff0',\n '3b2eb2b7-c231-4b25-8491-531f03f089f3',\n '82f8fc67-1ca3-4584-83bb-8c32e3887a02',\n '330bc53b-a532-428d-a250-49175376919f',\n '039dc534-26a0-4083-9cbc-2f9f9fc60068',\n '091a9635-9485-4397-9cb5-2e978ea0d038',\n 'f9ce6a0b-fd2f-4f53-92be-3646baf45899',\n 'b61c2cd2-e32d-4c91-8d61-1c8dd957e42a',\n '96885298-d257-4cfe-bff5-4bf4234307b5',\n '651c0e4d-ca59-46ca-aa12-00189b0ccd29',\n '86952d45-9984-4795-90a3-14c9168480ac',\n 'd3a220b9-67ff-425a-b1f2-4d258ac3290a',\n 'fdc73fc2-b191-47ad-98eb-bc65bad684d1',\n '42f118b7-702a-4aac-854c-8ffabbed04b2',\n '7e6688ef-f19b-403c-8ff4-dcb7319204e7',\n 'df722120-fb98-43eb-a8c0-6ac539707ba3',\n '0bb31f0a-d2d9-441c-b00a-42ecc4b12ee7',\n '275204d6-1729-4989-8f48-1fc597ef32ba',\n 'bf9aa03b-6059-426e-8460-377f1fa9cef4',\n 'e0c19098-dc57-453d-876f-fd3a6edcf52c',\n 'f3fc951e-0a2b-4e60-bbe1-1b625c39c8e1',\n '8dc88fa8-d119-48dc-aadf-69080ffa6088',\n 'db619daa-1b10-43a8-9b00-b7c56a86e5e7',\n '38c2419f-0a6c-4287-9401-f20918ea08b3',\n 'e6e1233b-e2b1-4db5-89c9-4cfd76c2de6c',\n '8f5b5a1b-db42-4f05-97af-5918ab6c4eb9',\n '78f6b207-99b8-48fc-9b8c-31cd3c607a2a',\n '72420bc1-c1f7-47d4-aaf9-62f6a133a419',\n '790cedfe-12d7-47ea-a5c9-69079eabbf4c',\n 'd0766826-6e08-4285-94e1-cbde0a2dc316',\n '91211b35-14e4-453a-a2a8-edd88da6ba03',\n '6f503b27-c8e5-4a28-a1e7-8de7fb92d55d',\n 'f1f606d2-d86d-4276-9615-c2afcd17cfd4',\n '4e88dbe6-2889-4dcc-9162-85055437dbd7',\n '096cd752-322b-4a9e-a477-ad1ffc0b4338',\n '018cf545-8645-444d-820c-d8858914df2c',\n '76339a10-b7f1-44f9-8c6c-aafeba988ed8',\n '05bd7ec7-9a5e-4e53-802c-fcb80f9d4dc4',\n '5fdc2020-4f67-4192-aa18-6b4997a880dc',\n '988a3a56-0120-4c2e-bd7a-805b04bb5997',\n 'f8be2cc1-3e09-4f42-94e1-1f0fdfba8bb0',\n 'd42929ca-2bb6-4fcb-8e70-41b01865e77c',\n 'a4cc1e3e-d2d1-43a9-8f6d-321373cde044',\n '4d163a93-4d5f-493c-a82a-4227c8641c2e',\n '1e4dca69-05b7-4fc7-aa0c-6e3597c08d23',\n 'dfabb9ea-3ae4-461d-8a3a-64e424504893',\n '2613b3de-1e39-41b4-8dd7-0210021d03ba',\n '2629f310-4fad-4f04-9720-38c78c64ca11',\n '8273c2e1-34a1-4c23-b0c2-2dea9dc69d11',\n '2a8f05a1-d76e-4e03-b9f1-d1a1fe78fa21',\n 'ff45b179-0b4a-4c84-a9b7-64b55abc4882',\n 'cf2c8aad-88be-4c95-8cf9-e890d30a221c',\n '01216079-ab3d-4e77-a4f1-55f508fa5e8d',\n '6d54984b-e305-4c89-8fa0-5bd3b59d985a',\n '5ff027bc-85bb-4b01-8a54-39c5f7245a61',\n 'ec14e0ae-b84e-429a-b029-7cec2873cd70',\n 'da34e119-569e-439f-bc79-622f44ca4645',\n '9bd2a135-7fdd-4fa0-8eb3-9b4413edf588',\n 'df3498a2-240e-4a57-8585-5a23a6d8c053',\n '79eb8b4d-13b5-4959-a135-0241a2879b44',\n 'b7061b0a-d967-4a26-80a3-8ea68e8b3d09',\n '1d5e17cc-857a-4b8b-a665-caecb4adac8e',\n '4aa11381-5670-43d6-9a43-d10db6a1dd86',\n 'd44fe43e-a392-4bce-89e3-d7285daf3a4a',\n '604d6f9e-a0a5-47c6-a93f-b5eb59fbe7c1',\n 'badea090-dcc4-4f67-a60f-070eb6133b96',\n '366dd8ee-4285-4637-aa43-786fc56fdb49',\n 'a32010f5-9c91-4d78-a253-e79893e97a1f',\n '84c2bffd-258c-49af-8a2e-1d967ece6ddd',\n '730a5f4e-938d-4ddd-8337-f052983e24df',\n '62e2fa34-bec5-4556-a5d1-e22e7f95903f',\n '37b6b784-1237-4464-8a3b-78f0cb51effa',\n '35893b49-dd8b-46fd-b20a-0a474a11a7cc',\n '85f00f3e-bcb8-4afd-b7b5-b4b17bbdaddd',\n '5f3e8de1-45f9-42d2-bb35-e13f955db11a',\n '6982f0bc-7399-4ff3-83d2-5891a43ee531',\n '87b812e7-b44d-4f97-99c4-7221defa9c0a',\n 'd6c4f658-616c-4368-9a08-c27decacf9d8',\n 'a423dd52-452d-4165-9b10-334adb2fc630',\n '5dc8691a-70b6-43e8-aec7-b1ce2763109d',\n '72e7b89f-7d31-41e4-919a-38fa734dbbb7',\n '495dc9df-91c0-479a-89fe-b9495f7a64a6',\n '4d755ac0-a75f-46f4-bad3-ab9e09d92593',\n 'b2ddc15e-c123-451b-b7a0-95dd75aeb66c',\n '0c5e6fea-ec6b-4856-95d2-9134878eaf1b',\n '8e096dd1-88c2-4028-adce-50a6cb558c9d',\n 'dfcce392-1616-4b8e-88be-e0ad7179d082',\n '1bbae32e-4d73-410b-9df5-073b41828c09',\n 'f919f4cd-0259-4b1e-97bf-e59116bec475',\n 'a07818a2-1937-4aed-8728-28b97c8a78aa',\n '4624c159-a0f1-46ac-96b8-11b30aeb0c4b',\n '0241ffde-70a5-4ac2-8ea6-8a2f2df16e40',\n 'c708e31b-9f7d-4c2b-a9ac-a24ab356da76',\n 'aa1e7e75-8690-4155-8a99-7058b56d93c7',\n '534c0a09-711e-4c6e-911c-62e735540467',\n '3a962bd0-74d5-4960-9b65-25963f984fb1',\n '6900dd96-86bf-49fb-bffa-0059d6f4255c',\n 'db7721ef-604c-4ac4-84b0-c370fc9f90cf',\n '37d869ea-2370-4701-bbfd-b17ad3d5c378',\n '9532b21b-ad1b-4539-9231-5224e556dd28',\n '621ac6d1-9a93-42f5-a051-f09798f03ff0',\n '6a247687-ca21-45fa-90af-ff4cc9da3857',\n '386fd9b5-6f57-471f-9cff-8803013afa9b',\n '5a12fdff-f106-400d-9591-f10ebbe34271',\n '98a1f32e-ad81-41ad-96a2-3a80b2d4ee0c',\n 'caeb4640-8c81-49f9-959c-faae481fba1f',\n 'd53d99c8-f00a-4178-a737-5590f72577dd',\n '9b5e8451-7c4f-4358-9764-c79caaf673a8',\n '2dc09fbf-24ec-4fba-82f3-36b27bd7cc00',\n '8928574a-7124-415a-89ce-c1f1365a79fd',\n 'b108196b-3cbe-483d-b48b-ab03c27caa26',\n '8e3ec85c-c22d-4a43-9a09-bda4160cf995',\n '09c4e860-1486-463f-9ecd-75b71040d168',\n '58dcd119-3c3e-492d-82fb-985de1c2afef',\n '52b72d67-a364-4f5a-a294-9f136df6541d',\n 'd6305943-7abe-41d0-a326-b8aeb43fc9bc',\n 'e7daf147-3424-47c5-ac17-ed95e2b954b7',\n '734b37df-5d1f-4909-8d79-c0cf4712fe24',\n 'd43e495e-6bc9-4c5c-b331-c9c670b88218',\n 'f9733147-7c37-4122-b373-eaa346d1dd38',\n '280eae5b-9975-46ab-9c19-99f110ddbdaa',\n '62b8d715-3482-4b18-8671-3a13aafba6bd',\n '41429179-6595-46b2-bd15-c92fb31b2dd1',\n '21a39be2-c6d3-4162-b6df-836b98226d17',\n '10faf784-0335-4f7a-b518-059bea096dc4',\n '9cd8f5ec-42bf-4420-ac7e-b58894584a3f',\n '4dde9606-9bcd-45c3-ae76-ca36d6ebca89',\n 'a95086e0-f03c-4386-8fc9-148347570383',\n '67acaec1-fe20-46e8-b3a2-51f67678a630',\n '2533b3e6-93d4-46ee-9cfb-3ecd689e6133',\n 'eada7dbd-4a01-412f-8573-f3258ca40049',\n '49b7466d-92ca-421b-8521-faa8d2df15c8',\n '11f4d713-02c6-4030-a3d5-17aac2e2e903',\n '4ce7f730-d1f3-4c4e-9e4b-1afb3be611f3',\n '8ab619c2-eaf2-4c8f-be13-50c385ae5bfe',\n 'f02a8189-2471-4043-9607-8b554226e1e9',\n '8b0c83da-6899-40b2-9b5a-715c89e9d025',\n '8df405d9-c27e-44fb-93df-301e247625df',\n 'b38eeb59-dedd-45bf-9578-3c26508584f2',\n 'd22f453f-d633-4c26-8937-84d17fe6ac13',\n 'b916c641-a716-4065-a42a-b693548859d2',\n '6ad1f96f-bf5a-423d-ae42-d4dfe0539295',\n 'de2b9d4f-96b7-403e-8c36-fb2d662adc7a',\n 'abec1a02-6989-48ce-a10e-310a5da4978e',\n 'f20cd6ef-972c-4dd9-878f-10d15d36040f',\n '204c2d1d-98b5-4527-b2ce-3f319119bbff',\n '51b4f6c1-33d9-48dd-8a8f-14b6ea437e29',\n '5ccebdba-70a9-4285-82ff-87ebef8e460a',\n '3b3c79bc-b73e-4ffb-a179-92a407928b79',\n '3b09fe95-da8f-4546-85ef-7b1b840bb0bf',\n '5b3aaf33-658f-436d-abed-4541e3f50690',\n '23e4d663-75af-4a56-b0ca-31c48af00ce0',\n 'a0ddec56-78bd-4b1a-965f-82fa294a9647',\n '23d3af1e-a3d7-44bd-8b61-2df3c7a40a9e',\n '991dc632-0c68-4179-9c44-d3e368759808',\n '363ef8d3-a0f2-46c6-821f-ccdbe971b42e',\n 'f6e9cc40-9e06-4f15-b157-9e735a08d97a',\n '176612df-7575-4217-8aa3-0382b670c36f',\n 'e295bf82-55dc-4242-b884-fcc7b64a4547',\n '978d93df-122a-40f0-adae-9938ce0c234d',\n 'dd6d3ad8-e620-4dba-b618-1441e72d80d0',\n '69ed0fe4-0747-4889-8989-f17fe130c9ef',\n '1edab3d5-01e5-4d0a-979a-3df6f43c0115',\n '11521aa0-fa00-4748-92ab-dbbcddb1504c',\n 'aff8cfd5-ff6d-4b0e-bcfe-a3482af46b63',\n '262b6603-3be6-412f-822d-4d0624d86d0d',\n '8630535f-0f65-4f7b-8057-7bf493a288e0',\n 'f34bc452-52a4-4d3b-90bb-0cf9dfd94d88',\n '382f6206-5aa2-474e-8c06-cdc4fb2101b3',\n '6167d566-4304-4ee3-8e2a-b008d77ece64',\n 'e7eb783b-6f62-43c4-a46f-acc843b314d0',\n '15143807-2a2f-4bee-92ff-a902e4998970',\n '72d94337-93cc-44b9-9b1f-2659657bdf2d',\n 'df3b937b-e0ed-4805-82f5-b14ba265e3d9',\n '9d724005-acf5-4269-aec8-4f54d03d85cf',\n '8ff765c6-5598-418b-9a54-22c8d0b04c2c',\n '640d3036-68fe-482e-b1fc-49a3ca2da108',\n '5d76bb77-0332-49a6-8a99-4a3f80f6abb3',\n 'b3cbc15d-d77a-4d28-a0df-c635187e1f03',\n 'a025fde5-f76a-43eb-bd5c-23b730be615f',\n '20d505a8-eaed-4ef1-9f6c-5e092dee5076',\n 'f970bb0a-c39a-4aeb-b592-86819fdccbf4',\n '66eb211d-44c6-44c9-851e-ad2066bde9e2',\n '95571309-3620-4689-bcc0-313ba938a4a8',\n '54fafbad-4bd1-4e72-9b07-2cb79ec86729',\n '49c9f224-0506-4506-af4a-c3e480ca041e',\n 'f758b54a-f6ff-4148-9e7f-9bf2c9d70bc6',\n 'c4da48cc-9ba9-4117-899d-04a9e6cb3ce4',\n '10b34626-8827-4574-8f5a-e3a10ebfe15d',\n '2d062ec7-4077-46ec-b693-910801d11d42',\n '3837f04c-0795-4226-838c-ad27bc0de5e9',\n 'b8750b89-8b53-4c70-92b2-44a5b304026b',\n 'cca7d029-ee78-4e3a-84d8-56e389d05007',\n 'c22a655d-948a-4130-a024-4f2ac0762825',\n 'aa98b630-b634-4ad4-b1a3-da1aa670b0ff',\n '8bca221b-f664-40f7-a296-eee12012e68d',\n 'e79069a1-f572-477a-8c3e-36fe6bdf9cf3',\n '042a4dae-0077-4da4-89e7-8c70ef07ea4e',\n 'a1c292db-199d-4b68-944c-d564863a15ec',\n '44c0bf5b-9815-4fb8-99db-8a796481e97a',\n '3358b5e3-1b20-4a2a-9638-76ac355efb65',\n 'f7c2abe2-9f98-423c-9258-0452031d16e9',\n 'a8d95b6b-92d4-498b-9d62-c5d6784928b2',\n '138e87a8-499e-4bb6-93df-3585bb96e379',\n '81a16469-326a-425f-813d-9ab4f76da4ac',\n 'e944a543-0b08-4b38-8e97-79b3195fc6ef',\n 'e6e71e5a-81d0-4d97-8884-4533a626c014',\n '8f665a17-3e40-4fa6-bbbd-e59dee012df1',\n '5a9a7093-6f1a-4221-b450-8beec707007f',\n '4b6ad089-1cd9-4217-bc2e-266061ac1690',\n '056e417d-3777-46b6-a750-e03e2f13e6f1',\n '6c8ddf6e-fc82-4270-be83-ac5fc5fdb9af',\n '979b6975-d0d1-4ed9-905f-880c0271d845',\n '04181a7b-f8f7-46b7-9763-e11897d9d9ee',\n 'bb2ffc5c-ed3b-4aa8-8726-80c0b052be35',\n '4135c704-6685-4d9e-ad13-fb2b09584395',\n 'ec1b07e8-c191-47a0-83c2-397607088216',\n 'd6073ebc-5ff6-4c9d-b917-4450251733d2',\n 'edee3a5f-e453-4591-b7fc-8f6eada03063',\n '58689390-7d52-4344-9198-80ecd35d669a',\n '7de8f1db-a2ba-43de-8222-14c65961da34',\n '3ed8bb91-f653-441e-8a53-d7231c938477',\n '232d105d-5035-49c0-944f-f45f5e9be2cc',\n '08703abb-9fc2-4380-b641-3fa2d30e25fa',\n '18d0ae04-903c-4bf0-a720-2f99241e19b3',\n '9fe5f317-822d-4d3f-9a0f-ceb066dda66d',\n '160ef01e-3800-4075-a763-02c0f379e673',\n '49df3ff1-dece-4a8c-b028-32fb1a9a4c8c',\n '59544c5f-df68-4834-ab1f-8a639244af23',\n '2183ba8c-62f6-406b-8a3f-2d84de143fac',\n '4d9a9cee-dbff-4425-8718-587fc15400fa',\n '3f16d921-73b0-4998-abd7-54facd618d8d',\n '65a8fdd0-7da3-44e5-bfbe-49e11efda10d',\n '8a4e6d8d-eacb-4595-9e35-2472c3aed68f',\n '08f9a6b1-06f5-465c-8597-4da5748a9b83',\n '181c2eb1-fc39-449b-bea2-2e969b159bde',\n 'edb59b94-2b0a-4efb-93d4-4b8680ad5d39',\n '31e2a5b8-cf3d-4d9f-91f1-08646efe1e2d',\n '190dc751-0583-4eab-8fb5-870d5ab6a78d',\n '3e91a32b-fda3-4b29-ae31-02ed6f15d07a',\n '446ac8b7-e713-4cae-ad43-9e8d8afbabea',\n 'fc9c3c59-b559-48e4-82d5-e2d5efdf342e',\n '86d6d40d-fa4b-49a4-950d-7a63a199f65e',\n '2352bc52-bc5c-4497-8563-a1a86582d57e',\n '8fd3f302-e5ef-418f-95c6-31707351f72c',\n 'a02108d8-c54b-4fab-8746-c51d7d938d7a',\n 'cda59fa7-4f89-44ee-97aa-abdcc980546f',\n '6eb7f23b-1c0c-4e56-81b4-3087b288cb85',\n 'e3a829fc-9938-4017-95f5-725cf298e7ee',\n 'c7267c15-2e1a-4f6f-819d-50e3d1c4bb95',\n 'be032431-c3c0-4a00-918f-ce64c7eb5bed',\n '1330fccc-b2b4-4e2d-81b4-3b0443b499c0',\n 'f43d34e7-1c10-4a71-b3f7-b29f0c800935',\n '93e7428f-dd91-46c6-9b80-17608c04c90e',\n '15389cdf-89f4-4898-a501-c3dedb00f1a1',\n 'e0c03685-4f03-4edb-bb1e-712fe69868e9',\n 'aa3b5043-0665-48c1-ae6f-2d682231c8e9',\n 'd8a5f373-35d9-4b63-af22-3ddfd3949c57',\n 'a0c0bce2-a070-456b-8dad-cb5666c8c7c3',\n '83b66e12-ab92-46bd-852e-e6237847de1d',\n '3029fa4f-501e-4844-a22f-d652ec1204d9',\n '52346d7a-55f1-47a7-b065-88b392abd7ff',\n '41c9200c-7bd9-43e3-b933-bd502e6c1f2a',\n 'a36a6f2c-0f5d-4090-9e86-62445702a3c6',\n '0799ac9d-0040-412c-b044-4c367c232c6b',\n 'c6f05e75-0422-43aa-9f5f-ff96b7979d5b',\n 'cc509696-b67a-4ee7-a341-05304ff485fc',\n 'd4e6c968-e02a-48fd-b4fe-46cda7d239b8',\n 'd103aa3b-30cc-4e3e-8c9d-90f433bd3781',\n '86562d2a-3def-4086-a796-8f175ab4964b',\n 'df17bb20-aef3-42a3-9e26-f17a4c4e4d87',\n '1af2f5f9-700c-41c5-86b3-15f47794a7d9',\n 'cddee8a1-2434-486a-9ce7-7bd4870bb91b',\n 'bfe37c2f-36a0-4955-aa5e-b7c989f0cfcb',\n '0ccffc45-5666-4b70-bcad-647f218ad960',\n 'cb09cd7d-0acd-43e6-b0b9-6ca6b8159f25',\n '6ff4b3b9-bdef-4b71-8843-f217817168fe',\n '8b7b3a26-5cb7-477b-9cb2-a61d045e8cb1',\n 'b232493d-c956-4804-b6cb-4f745892d21b',\n '557832d1-db8b-4fe8-9756-0d9ad12baee3',\n '2b93f68e-7ede-4539-9d7b-11c665f27435',\n '7f8ec616-e392-4529-bb33-a5a04c5a188f',\n 'ebef249d-dfab-4bc1-96ea-67ba6a05f4f6',\n '7b14b118-19fe-4e26-a7c5-518152862bb9',\n 'f41712bd-540e-4cea-8577-4c121dcbd0f6',\n '2be051f3-b8fa-4b37-9663-2f092e5b8980',\n 'a562143c-dec4-4a46-86a4-c2834da2de49',\n 'b17d5c4e-3d23-4430-ac72-816627a87276',\n 'c22d54bb-b7a1-4618-bcb7-af5b449c6c54',\n '120f0400-46fc-447e-8c46-dce85d10c297',\n 'd20030ae-1c10-42dc-8e07-cc3fbb6de704',\n 'ca70247b-a2e7-4ad3-b2ea-aa287daa280a',\n 'fff3c1b6-3854-4979-b994-b9e34e29f714',\n '064f70be-fb42-49a0-a9c3-617ce595d582',\n '7ae3e7ef-c351-456a-a24d-079b76b94e05',\n 'f9eee2f7-4fb4-4193-bc6c-114531a5d761',\n 'e5c6de4d-3c81-4b02-9063-95cf84c50bea',\n '089a1041-4aec-45b5-8ede-d6a9b5bd9e01',\n 'dc64ba8f-9875-4cb7-aa7f-3e1356021ec2',\n 'ae445672-fc38-4c93-8c31-70db0c3646d3',\n '78c7a552-fc1b-4cfe-98c5-e985407e89bc',\n '0bd8e390-29c4-43b9-990e-42c30d60d0ca',\n '4f1a39ec-6248-4958-9010-b10cd3472330',\n '33895473-f812-4055-a0a5-2326d7f886ed',\n '3e013a16-5d1f-4f03-ac77-1328fd1adc9e',\n 'a54b8f73-31cb-4a47-8f2c-ca32494f1886',\n '15ce101b-247e-4426-a8e9-5fa2afab0022',\n '2d0fcf86-46a4-4f34-a61e-c22d73a095a4',\n '8561d088-6e3c-4f7c-bb7f-53d7bce1cb93',\n 'ff711b4b-0523-420f-8511-6a73498b96a4',\n 'b1ce25ec-c7af-4784-9081-4eb01fc8e45f',\n '5c6929ae-b69a-41a0-82f0-8d5b0585b8a1',\n '9f72a718-ed9f-47c3-9465-27b93250a233',\n '1e9c4863-f561-42a7-9ebc-de91272b5362',\n '3b444700-cf5d-44e5-bc87-981cb7e94677',\n '80817719-c3d6-4cb8-a510-965612d26eac',\n '4c802301-b21d-4362-bf19-cc6006b5a1ae',\n '562d38a3-12e8-410e-bd5c-0925c2631d8f',\n 'a729b506-3785-4b1b-9679-a2cc9c2ef178',\n '2c902356-ae04-4590-b27d-e06066df1cf8',\n '0787c03a-b2a5-45ce-82ec-0bda471a54e8',\n '6e7a886d-8e93-4158-b091-a30cab95f2cc',\n 'f74cc9c6-440c-4ae6-b61e-82682e10eaec',\n 'b29029f5-9402-4f9a-8e14-2d7cddc45081',\n '2eb7a8d8-36c6-454f-ad41-266c38033d52',\n 'f26fe96c-945a-42f9-9f42-c7d98fca7caf',\n 'db78508e-c816-485b-bbc3-f14369d0bf51',\n '7c83b6d4-8033-4d42-a912-41d251188eba',\n 'c37ed76b-7181-4cc4-9327-f004a07c3c93',\n 'f5e56c15-c77f-45e0-8697-bf5a5b74dd4c',\n '9da637d4-e9ba-4513-916d-058019e2d215',\n 'e298f043-fd8a-447e-9247-6cf18d9f5615',\n '3c4d38fd-4254-483e-80fb-9e33b2941f29',\n 'c78a002d-1e84-4b40-a8d5-533521bb57e5',\n 'a201f4ee-fd33-487a-997f-df15734c6527',\n 'add968da-8a24-4f0c-90c9-d74184ff99b2',\n '27325306-78cd-4e2f-945c-b22ae64f47a1',\n 'd96d268b-8711-4b8b-83df-578fe0120887',\n 'c3c93aa3-1aa3-4b64-9cb2-a9eecfb2a6e2',\n 'b86f025e-d8fe-4c0e-abc5-c4ef6c15ff17',\n 'a56c0dc9-cd94-4b46-9698-414c9b1185c0',\n 'f1a45678-11de-46a4-a1cc-491089834fdd',\n '0352a8bb-6d99-4743-bf46-416cf4f4646f',\n 'eae36150-e82d-46fc-a7de-a1763e8bcfc9',\n '665d9271-8430-4614-98ba-9e834d448f45',\n '20467a36-a9a1-474f-a1d5-a549459be8f7',\n 'c11fe70a-8dbe-427e-88cd-6cae5b248d11',\n 'ddaa6327-b6ad-4e91-9e16-0b22b21e18e0',\n 'c10be774-4139-4dc9-b902-255cbe6c1b25',\n '0610e937-b0e9-4d93-995d-0fdd509a8563',\n '013c2c23-25ba-43a7-bb97-fb825b6458fe',\n '131caa72-f695-4f05-8953-a15a5bedd954',\n '0f4b524b-9f42-4cea-83dc-ed36ae047d96',\n 'b4e85a08-c3cb-4720-b5b1-3302b15f3165',\n '8723fb9a-be95-4f51-bcec-04e847bdbdc9',\n '7dfd5be9-1411-4b8b-a3a9-2415272e91c1',\n '4ed5ea8c-db72-4eb0-a8f0-83474e5dbc26',\n '715795f9-b775-4f2d-b48e-5a65b9dc0623',\n 'e9e0e61b-b3ba-4d8f-b453-3eb8a28d830d',\n '00f3c37b-4847-4c7d-b2f9-6ee28a2954a7',\n 'a5389962-1844-481a-832a-f39cf48d3097',\n '067d2546-2f50-4907-a23b-c4f2c0bf31bc',\n 'f7438746-6dba-4023-926c-4c34c62f0bf8',\n '43ad7d93-dd8b-4881-8a4f-56a06210eba5',\n '7a0bc004-fea9-4dbe-b2eb-a77cc69de140',\n 'e129ccde-badc-45b1-9466-333c66993454',\n '1b2e4f9b-9861-4e3a-9741-041e55339098',\n '5ae9562b-3c39-418e-98e3-233668ed0da7',\n 'b75887c0-4e98-4002-b686-a9e3b180ca65',\n 'd987bcb0-b056-4a47-9d93-19892ebb698b',\n '9da83e6a-71bf-4247-8602-962f31219233',\n '0ea117d1-cdfd-4bb1-998d-9f56e53ce76d',\n '6c5cd186-3880-4aab-afb8-6058332a9149',\n '8304fe00-7e50-4073-a720-6f9b9c83e16a',\n '0e351edb-4e8e-4c54-979d-51d0c9ee0e8e',\n 'e82c659e-e400-402a-86a1-c5ab4eec9775',\n '8a6c6878-bf3c-4f2d-83e3-92dd3766e4bc',\n 'e50610de-9896-4a5c-ac76-0b1fd00b861f',\n '85fd528e-7d54-46e7-9135-de119761aa94',\n '6b04e3a6-46e4-4c1b-a0f7-f881cf9ca46c',\n 'befc20c8-b274-4e17-8e5c-5a8aaf0f9a2a',\n 'eb7d973c-32ef-4aea-9949-af2b203010fc',\n '369e1281-c512-4890-a829-35374c1c4e4d',\n '266d419c-25b7-4987-9e0b-f01532f36209',\n 'd50f1ccb-0eb4-4224-b774-98ee072ecc6d',\n '491060ae-f4e2-496b-9541-4831763e1bc6',\n 'e534a119-64c0-4c3c-a9dc-fce0a9721336',\n '2156328f-2957-4449-8a6d-feb2bde47dab',\n '45b6297b-5c1b-4741-8899-428a4037f5bb',\n '34a52e08-277d-4ff9-96fc-9f831b432b0a',\n 'b432ca1d-be8f-4cea-ab66-fdc7a614fc0d',\n 'd0bf9424-6e2e-4123-a97f-7da7b5e4a81e',\n 'e6319b31-4d41-4726-8622-7235eb8ec7ef',\n 'bb05a2c4-1112-400b-bc7c-cde363ce51ac',\n 'c672aac9-c481-4a76-9e9d-7b7ac111714b',\n '1103407c-4420-4e05-bbfa-b7052685e425',\n '1bd25f20-dc37-4267-9628-3ba3bbba4798',\n 'dd587fcf-91eb-415d-baf2-54351d68c35a',\n '0a5d44ca-7168-41ad-a947-4663fcc535a4',\n '88fafa6e-0bfc-4564-873b-ebee2fb5b1d8',\n 'b8d52e7f-5bdc-4ed8-b45f-65be41520144',\n '0ba461b3-d706-457e-9d2d-3085c93d61aa',\n '898a6887-ae3e-430f-873e-1cfebb61fb51',\n 'e4ae8adb-5cde-4756-b064-1ee17fefc83f',\n 'ce6823db-ece3-4f6f-b93a-36d699d6aaf5',\n 'bdd29721-ba1e-4feb-9caf-e9c09c81baa6',\n '8a742117-da7e-45a3-8640-31691df879c1',\n 'b35e0f6b-656b-4cb4-aa5c-35b9507fd71b',\n '682e8d48-c7ae-4583-9676-addf3e4dca55',\n '2d834c7f-1c0d-47f0-bb3c-d4e284d330a3',\n '8c1ec888-4454-454a-9cf2-99a53fd76832',\n '1bcb6121-8ae9-4fe2-95ae-a4d5a0d3f802',\n 'de79729b-3aa7-4fcf-89ad-6fdabfc37f69',\n '87a29cbb-7f65-4a58-849f-baf3b62ded83',\n '691bbe66-f007-4363-a617-ea23e5eff78c',\n '0243cd5c-31b0-403c-bf95-68e27708ec2d',\n 'd351921f-1b43-42af-bd5a-d8c6d2df8d1c',\n 'e0ef871b-254d-4589-bf4e-471f2e731783',\n '856c0afb-e4ff-42a8-8d46-0df798c1fb9b',\n '92b80f00-3450-4cf4-b2e4-c6b473cdbd5b',\n '5eda96ee-4394-468d-b786-1b784fd460d1',\n '7d2800cd-e025-4105-a29f-c22399a1d59b',\n '2e950922-bc8b-4c86-8d19-ed12b93404e2',\n 'b46838d4-59e6-4301-ab6a-71a9abfde1b4',\n '9a8ee75b-4dd3-4154-bec1-f6fff84a10a0',\n 'a5f1024e-a9fb-44a9-8604-991ab3512577',\n 'cb5efd35-cc34-482f-8034-031132e596de',\n '5e4aee1d-1012-4266-965a-38f85f4b7e5d',\n 'fa9ec70c-040b-40ee-836f-9af0bf0b0e55',\n '8593eb09-bcbc-4602-a5f9-3af8d8249790',\n '3fc4300f-f1df-458e-bff0-7eaa7a16143b',\n 'fc962679-05b1-4590-bad6-20b06c76276a',\n '60a59671-6705-4684-9876-5b7105299673',\n '0191677d-3138-44ae-83d9-4cbd8a3cf56a',\n '743ca5a4-4e1c-47ac-a794-a1e45b90c3cd',\n '040e99c7-ee26-45ee-8117-83cb88417013',\n 'd3d61e6d-eb22-4a93-a654-f38f195dac19',\n '37300c96-67df-44b5-9094-99e741432c95',\n 'db616e24-0483-435d-9b53-937cbb20a2b0',\n '8016f5e7-b0b9-49ab-9891-33b52d9e815c',\n 'a1588d05-31b9-43a1-89f0-bf2e4b6ab229',\n '1dc15b91-b1e7-45e9-b055-4d4c03a0c014',\n '6a908ed0-1b86-4bad-9dca-2c9a02864ed4',\n '9563ba4a-44a8-47af-86be-219bf5cd43dc',\n '0f970192-afcf-4526-bec4-4488767d2a55',\n '476dec04-4d82-4c54-9101-005d6851bd9d',\n 'b64584f5-f1bf-4a8b-9e5a-bc308bb3d4a9',\n '09f97315-756e-4702-9a0d-d01c6f56f753',\n '82ba1d06-f0ba-43fc-964e-5ec39d772e95',\n '5e005e6e-d975-4ae0-b344-bb220a8cf5f4',\n '858eee11-a594-4c0a-84b1-e76f06872a76',\n 'a3a9e611-7208-41be-ab18-56ef7441c0a6',\n '0676fe25-e3eb-4338-997c-114c4ae7ef21',\n '269ef95c-8099-4052-89c7-f06625a9bab8',\n '1f0e8984-9366-4fd8-afc1-8f8b50dedc49',\n 'acc1efac-afd9-44bc-a199-97668e581dc0',\n 'a5eab03f-fa48-4d5e-9003-b156042d525b',\n '0d4c9401-9937-4557-8e39-aac5af97a88d',\n 'cd5e62f0-3d19-43dd-9a54-f7f5f2c6bd54',\n 'b46fa2ba-d361-4ab6-95c4-7c84464a8e55',\n '41c0f5b7-ed7d-44cf-ab1d-102921064ef3',\n '6a6e5e35-3419-4daa-a5f4-7ee281240d13',\n '6c2fc371-ed99-48ed-af8a-9edfaf4bd795',\n 'dc42e938-260e-42c9-a230-c7109351d3a6',\n '9918d165-bfc9-4f5e-bf5e-a3822d968ad5',\n 'e7332a48-d9ca-4d8d-9cb5-6b4cbf19847d',\n 'c8516472-4be7-43ab-a6be-780323ec2128',\n 'f60c6083-75e2-4c34-9564-e5b0585c15b6',\n '9d5465c4-7f41-447e-85d0-ddf20c069deb',\n 'a8e6e64c-d2f6-43c9-a577-45d9554f1e08',\n 'd643a0fa-eed8-4fd4-af49-72cfe40dd6a6',\n '1c0d521a-9716-4c96-858d-a1cab7db7a0f',\n 'caf820b1-c5f4-4432-8d36-b9794eb5d566',\n 'e0e19f00-8994-429d-8ae5-28b6f218a9ab',\n 'e5e782f6-e818-4460-8274-cff0679fd71b',\n '6c4aa863-f76c-4f78-9302-f38501110bf2',\n '4199a8a9-c9f5-4485-9ca0-4fafafd13250',\n 'd3e5d87c-5096-46c0-8c81-b14fe9ced839',\n '7cf2c440-875f-4e23-bf70-bc35521ce857',\n 'ba9c34c0-6eb3-4eb3-9ea5-7503ce677397',\n 'bb60f231-04c1-4280-b925-8d662f7da5c5',\n 'df6fc272-6fc0-495d-9e8f-6169cffa7845',\n '64b93c58-8266-4877-980e-bb0a76ddf1e3',\n 'ee2a4f3a-074e-46c6-b957-f8456338c2de',\n 'c1a6fe72-fd89-48d7-94a6-8fcb79cd502b',\n 'fb7fc544-8ed8-496a-9933-8d3b180ee234',\n 'ac03759a-bf9b-45c2-ac2b-4ec48f6cf27e',\n '50d7ebfb-7d5c-4e01-8a53-2090a79b842f',\n '240c78dd-1310-4404-84c9-1fe58a7e8bb7',\n '333a86c3-e852-41c7-8ff4-b446275a3078',\n '752fa3be-38b2-40a4-9329-a0f94634f4ce',\n 'd4bf9aea-8641-46b9-a5c0-b1407cb9d487',\n 'f31afc5a-5ea9-4264-8520-6d2b951e2978',\n 'ef120fc3-1088-48d9-88c2-c50bed8a3a6f',\n '58577b59-f5d8-4882-8046-b871d03a55c9',\n '2a7c7ade-4b5a-480e-bfce-34caedaa2ef8',\n '9bb6aa14-a6f0-4b19-9b28-73e1768bc949',\n '7096da5f-f1a0-4df8-b43f-a651151889a7',\n '8a8b80ed-4824-4daf-8637-91a28916af1a',\n 'f729be7d-d8fd-4539-802a-ae5bebff0ad8',\n 'd335e2ba-e0ee-4c59-a076-2a3d13a16f6f',\n '92c983ad-3ff4-4522-8958-1f49be93e343',\n 'ab0342e6-40da-4ac5-a5cb-474e8e262be4',\n '606a8ff1-371a-4137-be1a-f996415bb5d6',\n 'f9469ea4-80a8-4029-a573-080aed17a0ba',\n '9a78fb87-17ed-46ae-bb8e-c5ce8e4022b1',\n '53452f64-1e6a-410c-b4a7-141dd1eab7e5',\n '59b22c8c-4da9-47f1-9d6d-035e7f43db1f',\n '92801d9f-58af-4532-9e8d-3c0dc1140c44',\n '3a29c626-5d57-4345-974a-14b53c5c0e88',\n 'fb1edcee-0161-4388-95ff-55f015f2be71',\n '72f98c7d-328b-4af1-a30a-c6cf83805f76',\n '51abaa69-078c-4060-a9f0-7d68732257ae',\n '03a79136-0b8c-42cb-b8a0-76724f18c16f',\n '614067fc-8283-4bb2-bd2d-c377852cdb08',\n 'e3b17bf6-096e-4173-bfe6-ba008ac7ab60',\n 'f1a29404-f2f6-4bf9-81fc-9f64aeb701a5',\n 'bad8ada6-dc88-4bfc-92ae-2b821da3d185',\n '0cc8f60d-9f98-468b-b5eb-46401189c060',\n '5ca8c959-c804-43bd-bed9-f4591026f7b0',\n '514bbca2-0540-4000-9844-341ee77b5439',\n '53514c23-0c44-4ca7-a039-950ccf2972f6',\n '6d70a632-ddc9-4d47-bdf9-27e4ba455bc3',\n 'c86d9472-4ebe-4f0d-b07b-e85e1eefe099',\n '6d16cefa-2ce9-4db9-bbac-d9d498895a5c',\n '7166b93b-1a99-4a16-bd96-9e3871e6d9df',\n '7efd2cd3-6efd-4141-843b-51c6a72722b0',\n 'ae1d262f-40b0-4af9-963b-520e867d101d',\n 'db68a7b4-653e-449b-8827-80e11816e7cd',\n '563a4ea0-0eb1-456a-89c1-cede28b784bc',\n 'f89669f1-ef46-47f0-8e57-7770d8f766e9',\n '6c083005-5110-44e1-a3a0-bf3f8042f41d',\n '4db042ee-945c-4db4-8610-12bb16839762',\n '90a35cfc-716f-4af5-9213-090228852734',\n 'eb5cb631-8851-49fd-8e08-b79a30e6440a',\n 'f84b9f06-72d8-42af-82c9-d7dd0bdbf3f7',\n 'e7654c24-aae5-450e-86c5-50073670fa69',\n '88fb5499-8752-4b64-b2af-a161570f9bed',\n '07e0ac0c-19b3-4305-8ae0-ba36c223d68d',\n '7c056410-ab7c-46c8-8cf1-a21fdd28ebf9',\n '1ec8241b-431e-4dd2-bfb8-53ab983ef49c',\n '6bef30ed-0e6b-4374-bfc6-1ee9a7dc1c6b',\n '73789cdc-4919-43f9-a084-b53ef68fee0d',\n '971333b9-5ea3-4313-8642-74ea660ad675',\n '05e12eb7-313b-4f83-a0fc-04524fb7d55a',\n '1aa02f72-11b8-41e2-9ddf-096da0c1ea66',\n '15ea34e7-a212-4711-934a-790ce7062e8b',\n 'ed769caa-6adb-4b19-8b2a-eafb2a01f597',\n 'dcd36759-74d7-4724-a7bf-9be02c42d02d',\n '23ecd9dd-101d-46b9-b58e-28cfd3ed54ed',\n '0da796a4-19fb-48fb-8de2-99e9288fc453',\n 'cd1269b5-2a2b-41fa-9d33-4b64dd6b1974',\n 'd82592b5-a843-48f5-9182-2a40d9bbb771',\n '83f755d7-2e33-4aeb-a57f-abeae0cc5245',\n 'e74a4379-d731-4984-a5bf-a946a6f34ec0',\n '95272e2c-e2e4-41ff-a8d3-c51e3901e0c9',\n 'bf1e29d3-2550-4cbd-b25a-dd7de2b2c36b',\n '717a096e-cf7f-4f14-954a-07cdf54b4d65',\n '401dda9b-5f6a-4031-9284-3a54a3e6e3c9',\n '49b3efb8-7872-41cc-97f6-db9f114f2e06',\n 'aa52de8b-4930-4a0d-bfca-0b4c43796890',\n 'a45ce899-3035-41e5-a601-b800268bc4c6',\n '70cefa5c-cb11-46f4-b17a-ce1c358795ee',\n 'ba2f66b9-544e-46bf-a303-fceb4f9a77f2',\n '8689080a-19fa-4898-9bcf-f753a3f6ffc0',\n '4229b4f3-882e-4148-9a2b-399cbe5794a9',\n 'acd22c5c-ddd7-4ba3-ba2a-19ede8ab88e1',\n '0be8ed11-003c-4035-bdbd-ddb51f06c196',\n '4107e01b-ee48-42c7-b041-d154c0d51a3a',\n 'ab93492f-e631-466b-bdd2-9ae34947bfc0',\n '5c58e51a-29fe-41d2-81b7-767e772e5171',\n '716cbdc1-149a-41bc-bc9b-c1c2a78b1c14',\n 'aaaa812e-f814-4e51-9f33-d15671453cb3',\n '370d7ac9-c1e5-40fc-8ea4-b07c0d85cfb1',\n 'bcab1c70-429f-4761-8ad3-e9f8f167af08',\n '9faa6da2-cc4e-4280-8e89-e096e3c64c05',\n '41943e78-c85b-4ac7-a95d-1c7505755af8',\n '1a5a250e-b2ac-4a71-a45a-92ab376a0889',\n '55bbf32d-95b6-4581-8fd4-9e05a4309778',\n 'b872bd99-1df0-4b51-96bd-03af4941d26d',\n 'd45b417d-c220-4256-91de-e3e15bdeeb47',\n '740b8d15-ff8d-497e-a8c7-44a897deef4c',\n '84cc6b33-d4f4-4393-922b-311e54fc5094',\n '3f1653e5-a443-4ce2-8ba1-dd48d2dd8fb6',\n '81f8ae1e-f666-402a-b67d-5e58f635ab8c',\n 'c9fa70bd-7552-4466-947d-abb5ff8d5355',\n 'ff92b5a5-40e0-410c-aeaa-939ad23a4aa0',\n '15c1136b-1a15-4b4b-be5c-75e2123d576c',\n 'e1fdc94c-a9f1-4166-b1a0-fa6b1c95c017',\n 'c2bd87dc-21e5-4bdd-9c6d-2b19aacff0aa',\n '674b66fe-75f1-4200-a2cd-29e19e215619',\n ...]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_1=\" What are some challenges associated with data collection?\"\n",
        "sub_docs = vectorstore.similarity_search(query_1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T19:56:37.204113Z",
          "iopub.execute_input": "2024-04-01T19:56:37.204484Z",
          "iopub.status.idle": "2024-04-01T19:56:37.476474Z",
          "shell.execute_reply.started": "2024-04-01T19:56:37.204456Z",
          "shell.execute_reply": "2024-04-01T19:56:37.475547Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "3fb2b3e0cf5e405ca1c36dfd5c51afc2"
          ]
        },
        "id": "LXVA5Mn8L7Ye",
        "outputId": "5b5a71e1-8cf7-4461-9a9c-53cd3c07c45c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3fb2b3e0cf5e405ca1c36dfd5c51afc2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sub_docs[1].page_content)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T19:57:07.959340Z",
          "iopub.execute_input": "2024-04-01T19:57:07.960014Z",
          "iopub.status.idle": "2024-04-01T19:57:07.964929Z",
          "shell.execute_reply.started": "2024-04-01T19:57:07.959980Z",
          "shell.execute_reply": "2024-04-01T19:57:07.963980Z"
        },
        "trusted": true,
        "id": "d-yoxSM1L7Yf",
        "outputId": "4c29faef-2193-493c-95a0-d816c0f85596"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "is the fact that this data has to be collected to begin with. So historically, a lot of data was collected manually\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_1 = retriever.get_relevant_documents(query_1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T19:59:49.669351Z",
          "iopub.execute_input": "2024-04-01T19:59:49.670022Z",
          "iopub.status.idle": "2024-04-01T19:59:49.951969Z",
          "shell.execute_reply.started": "2024-04-01T19:59:49.669991Z",
          "shell.execute_reply": "2024-04-01T19:59:49.951013Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "0fb8578b76e941efa07fea6333619e2a"
          ]
        },
        "id": "hBQk75vpL7Yf",
        "outputId": "a7ba4529-7513-4f64-989d-d3a60193cfd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0fb8578b76e941efa07fea6333619e2a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(retrieved_docs_1[0].page_content)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T20:04:13.228212Z",
          "iopub.execute_input": "2024-04-01T20:04:13.228585Z",
          "iopub.status.idle": "2024-04-01T20:04:13.234630Z",
          "shell.execute_reply.started": "2024-04-01T20:04:13.228555Z",
          "shell.execute_reply": "2024-04-01T20:04:13.233739Z"
        },
        "trusted": true,
        "id": "Fo1N9T7bL7Yf",
        "outputId": "d9d39bb0-d44d-4128-a078-8c38ab05e0a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 50,
          "output_type": "execute_result",
          "data": {
            "text/plain": "237"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T20:04:01.274971Z",
          "iopub.execute_input": "2024-04-01T20:04:01.275365Z",
          "iopub.status.idle": "2024-04-01T20:04:01.281375Z",
          "shell.execute_reply.started": "2024-04-01T20:04:01.275334Z",
          "shell.execute_reply": "2024-04-01T20:04:01.280416Z"
        },
        "trusted": true,
        "id": "JO-tQs6YL7Yf",
        "outputId": "c53e044c-2c07-4caf-9124-614937f32f81"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 49,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[Document(page_content='other aspects which go into this. One is the fact that this data has to be collected to begin with. So historically, a lot of data was collected manually through forms and so on. And one part of the problem would be that these forms were', metadata={'source': '/kaggle/input/audio-files/audio_1.txt'}),\n Document(page_content='is the fact that this data has to be collected to begin with. So historically, a lot of data was collected manually through forms and so on. And one part of the problem would be that these forms were manually entered by somebody and then converted to electronic forms. So that would be, there would be two levels of potential sources for errors. The person writing down the information and then the', metadata={'source': '/kaggle/input/audio-files/audio_1.txt'}),\n Document(page_content=\"some way, right? For instance, if it is some data, what I don't, let us say today's everybody's favorite data set is COVID, right? So it might be collected statewide. So it might be that though the data is anonymous and randomized and all that it's actually the first. So many tens of thousands of things are from a particular state, and then next state, and so on and so on will collect it and\", metadata={'source': '/kaggle/input/audio-files/audio_4.txt'})]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#query_1=\" What are some challenges associated with data collection?\"\n",
        "query_2=\" What are the advantages of the Apriori algorithm?\"\n",
        "\n",
        "retrieved_docs_2 = retriever.get_relevant_documents(query_2)\n",
        "retrieved_docs_2"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T20:05:47.426867Z",
          "iopub.execute_input": "2024-04-01T20:05:47.427736Z",
          "iopub.status.idle": "2024-04-01T20:05:47.717469Z",
          "shell.execute_reply.started": "2024-04-01T20:05:47.427698Z",
          "shell.execute_reply": "2024-04-01T20:05:47.716585Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "749716a869c94bdaa4b1c6086feee59f"
          ]
        },
        "id": "Y3WZwrqEL7Yg",
        "outputId": "0fd4a139-3f35-419f-f0af-a3096c520086"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "749716a869c94bdaa4b1c6086feee59f"
            }
          },
          "metadata": {}
        },
        {
          "execution_count": 51,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[Document(page_content=\"that we removed in this time, if we removed a different point, would we get a different decision tree? For that, it's not necessary. You will get a decision tree. It's different. But in general, the removing and modifying the data set in a\", metadata={'source': '/kaggle/input/audio-files/audio_5.txt'}),\n Document(page_content=\"and something turns out to be genuine, and then you pay it, you're paying double. It makes a lot of sense. If you spend some effort and time to check a fraud and you save on paying out something which you should not. So there is a trade off there. And this is, again, something where if you had an efficient system which was based on machine learning, you could possibly get some benefits. And\", metadata={'source': '/kaggle/input/audio-files/audio_1.txt'}),\n Document(page_content=\"tree will be a kind of implicit importance, ranking among the attributes, which ones are more significant. So not all columns are equally important. So these are all things that have come up. So maybe when we come to it later on we'll discuss it. But these questions that you're asking, sir, when we form this decision tree, is it important or in some sense beneficial if we know the data? Insight of\", metadata={'source': '/kaggle/input/audio-files/audio_3.txt'}),\n Document(page_content=\"template as. So I just give you a line here which says a Y equal to Mx plus c. And here I want a specific m zero and C zero, which fits the given data in the best possible way, which is also part of the description of the algorithm. What does it mean for one line to be better than another line? So that's part of it. So the other side of this picture, as I mentioned, is when you don't have this\", metadata={'source': '/kaggle/input/audio-files/audio_1.txt'})]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#query_1=\" What are some challenges associated with data collection?\"\n",
        "#query_2=\" What are the advantages of the Apriori algorithm?\"\n",
        "query_3=\" What distinguishes training data in supervised learning, and what is its purpose? \"\n",
        "\n",
        "retrieved_docs_3 = retriever.get_relevant_documents(query_3)\n",
        "retrieved_docs_3"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T20:07:41.158583Z",
          "iopub.execute_input": "2024-04-01T20:07:41.159282Z",
          "iopub.status.idle": "2024-04-01T20:07:41.427925Z",
          "shell.execute_reply.started": "2024-04-01T20:07:41.159252Z",
          "shell.execute_reply": "2024-04-01T20:07:41.427041Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "b1b0e05cdab64968b49dde5397ddd1e5"
          ]
        },
        "id": "EmPOMteKL7Yg",
        "outputId": "332af2a0-5fe7-48c5-b27f-bb171d5b1314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1b0e05cdab64968b49dde5397ddd1e5"
            }
          },
          "metadata": {}
        },
        {
          "execution_count": 52,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[Document(page_content='So training data is both the entire data set that you are given to start with. And then it is the specific choice that you make about how to split this data into a large part, which you will use for building the model, and a small part that you will reserve for validating the model. So depending on how much data you have, the usual practice is about 20% to 30% of it you keep aside. So you use', metadata={'source': '/kaggle/input/audio-files/audio_4.txt'}),\n Document(page_content='given a new row, a new item, a new combination of these attributes, which is not seen before, try to predict what would be the most appropriate category to assign. So these items which are given with the labels. So these are called labels. So these items are called training data. So we have labeled training data, which consists of some information which has been collected in the past, some', metadata={'source': '/kaggle/input/audio-files/audio_3.txt'}),\n Document(page_content=\"this is an automatic step. You just have to say, split the data into train and test, and it'll do. So, training set and test set. That's it. So this is a little bit, what should I say? Confusing, because we actually call the whole thing also training, and then we split it. And then we again call this training and test. Right? So training data is a little bit of an ambiguous term. So training data\", metadata={'source': '/kaggle/input/audio-files/audio_4.txt'}),\n Document(page_content=\"usually training data is collected in some way, right? For instance, if it is some data, what I don't, let us say today's everybody's favorite data set is COVID, right? So it might be collected statewide. So it might be that though the\", metadata={'source': '/kaggle/input/audio-files/audio_4.txt'})]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#query_1=\" What are some challenges associated with data collection?\"\n",
        "#query_2=\" What are the advantages of the Apriori algorithm?\"\n",
        "#query_3=\" What distinguishes training data in supervised learning, and what is its purpose? \"\n",
        "query_4=\" What is cross-validation?\"\n",
        "\n",
        "retrieved_docs_4 = retriever.get_relevant_documents(query_4)\n",
        "retrieved_docs_4"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T20:08:50.467638Z",
          "iopub.execute_input": "2024-04-01T20:08:50.468285Z",
          "iopub.status.idle": "2024-04-01T20:08:50.715654Z",
          "shell.execute_reply.started": "2024-04-01T20:08:50.468254Z",
          "shell.execute_reply": "2024-04-01T20:08:50.714793Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "60641fa116394904ba1da1c56d77b6e3"
          ]
        },
        "id": "D5EZUVqUL7Yg",
        "outputId": "3540e388-f287-4a7c-c945-2d19694dce95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "60641fa116394904ba1da1c56d77b6e3"
            }
          },
          "metadata": {}
        },
        {
          "execution_count": 54,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[Document(page_content=\"cross validation, cross validation. What I'm saying is that you do not make a hard and fast choice once and for all to split your data this way. So you're taking your data and you're saying, hey, first let me leave out this 20% and build one model. So I am building multiple models. I'm building m one, m two, m three, m four, and m five. Each of these models is built using 80% of the test data, but\", metadata={'source': '/kaggle/input/audio-files/audio_4.txt'}),\n Document(page_content=\"not get the same. Okay, so the question about cross validation, cross validation. What I'm saying is that you do not make a hard and fast choice once and for all to split your data this way. So you're taking your data and you're saying, hey,\", metadata={'source': '/kaggle/input/audio-files/audio_4.txt'}),\n Document(page_content='statement to start with, which is, is this method actually giving me something reasonable? And is it giving me better something than something else? So, for this one strategy is so called cross validation. So what you do is you take', metadata={'source': '/kaggle/input/audio-files/audio_4.txt'}),\n Document(page_content='is this method actually giving me something reasonable? And is it giving me better something than something else? So, for this one strategy is so called cross validation. So what you do is you take some fraction, say, in this case, it is one fifth. So 20%. So you take your data and you leave out. So you randomize it into five groups, which preserve, in some sense, the random behavior, so they', metadata={'source': '/kaggle/input/audio-files/audio_4.txt'})]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#query_1=\" What are some challenges associated with data collection?\"\n",
        "#query_2=\" What are the advantages of the Apriori algorithm?\"\n",
        "#query_3=\" What distinguishes training data in supervised learning, and what is its purpose? \"\n",
        "#query_4=\" What is cross-validation?\"\n",
        "query_5=\"What is the process of building the decision tree classifier, and how is it trained on the dataset?\"\n",
        "\n",
        "retrieved_docs_5 = retriever.get_relevant_documents(query_5)\n",
        "retrieved_docs_5"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T20:09:54.016338Z",
          "iopub.execute_input": "2024-04-01T20:09:54.016947Z",
          "iopub.status.idle": "2024-04-01T20:09:54.316383Z",
          "shell.execute_reply.started": "2024-04-01T20:09:54.016915Z",
          "shell.execute_reply": "2024-04-01T20:09:54.315450Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "5575488a698449a0a250fc6893cfa7a3"
          ]
        },
        "id": "YcDvBi6GL7Yh",
        "outputId": "f73f356d-19f2-4709-83bd-9db7674e165a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5575488a698449a0a250fc6893cfa7a3"
            }
          },
          "metadata": {}
        },
        {
          "execution_count": 55,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[Document(page_content='to build a decision tree in python. If you have the data, what you need to do manually is to probably decide. So the question is, can we specify a certain threshold? A certain threshold of what? Yes sir. So there is also stopping criterion where we can stop. Correct. So you can also specify criterion saying that, saying that stop when your improvement is less than so much and so on. All these', metadata={'source': '/kaggle/input/audio-files/audio_5.txt'}),\n Document(page_content='a three way thing, right? There are three types of irises that we saw, so it will be essentially a number, zero, one. And now to get this decision tree classifier, we have to take this function that we have imported. So we say take a decision tree', metadata={'source': '/kaggle/input/audio-files/audio_5.txt'}),\n Document(page_content=\"apparently is not that good in practice. So I want a nonlinear way of giving some impurity value to this ratio of yes or no, and then we can use either this entropy idea or this genie index, and it gives me a sharper curve, and this sharper practice turns out to be better. So that's how you build a decision tree. You build a decision tree by applying one of these two nonlinear things. If you come\", metadata={'source': '/kaggle/input/audio-files/audio_3.txt'}),\n Document(page_content='you can actually construct a decision tree for it. So this tree, which we will see later, this python library for decision trees, actually constructs from the iris data set, this decision tree. So what you do is you start with these', metadata={'source': '/kaggle/input/audio-files/audio_4.txt'})]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install cohere"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T20:10:34.824184Z",
          "iopub.execute_input": "2024-04-01T20:10:34.824584Z",
          "iopub.status.idle": "2024-04-01T20:10:49.398103Z",
          "shell.execute_reply.started": "2024-04-01T20:10:34.824552Z",
          "shell.execute_reply": "2024-04-01T20:10:49.396958Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "id": "RU3auoHYL7Yh",
        "outputId": "aaecad31-a616-45b5-d100-f4773ca2f820"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Collecting cohere\n  Downloading cohere-5.1.7-py3-none-any.whl.metadata (3.0 kB)\nCollecting fastavro<2.0.0,>=1.9.4 (from cohere)\n  Downloading fastavro-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\nRequirement already satisfied: httpx>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from cohere) (0.27.0)\nRequirement already satisfied: pydantic>=1.9.2 in /opt/conda/lib/python3.10/site-packages (from cohere) (2.5.3)\nRequirement already satisfied: requests<3.0.0,>=2.31.0 in /opt/conda/lib/python3.10/site-packages (from cohere) (2.31.0)\nCollecting types-requests<3.0.0.0,>=2.31.0.20240311 (from cohere)\n  Downloading types_requests-2.31.0.20240311-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: typing_extensions>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from cohere) (4.9.0)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.21.2->cohere) (4.2.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.21.2->cohere) (2024.2.2)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.21.2->cohere) (1.0.4)\nRequirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx>=0.21.2->cohere) (3.6)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.21.2->cohere) (1.3.0)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.14.0)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.9.2->cohere) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.9.2->cohere) (2.14.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->cohere) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->cohere) (1.26.18)\nCollecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.31.0->cohere)\n  Downloading urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx>=0.21.2->cohere) (1.2.0)\nDownloading cohere-5.1.7-py3-none-any.whl (145 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.3/145.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fastavro-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading types_requests-2.31.0.20240311-py3-none-any.whl (14 kB)\nDownloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: urllib3, fastavro, types-requests, cohere\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 1.26.18\n    Uninstalling urllib3-1.26.18:\n      Successfully uninstalled urllib3-1.26.18\n  Attempting uninstall: fastavro\n    Found existing installation: fastavro 1.9.3\n    Uninstalling fastavro-1.9.3:\n      Successfully uninstalled fastavro-1.9.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\nbotocore 1.34.51 requires urllib3<2.1,>=1.25.4; python_version >= \"3.10\", but you have urllib3 2.2.1 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires kubernetes<27,>=8.0.0, but you have kubernetes 29.0.0 which is incompatible.\nkfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.2.1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed cohere-5.1.7 fastavro-1.9.4 types-requests-2.31.0.20240311 urllib3-2.1.0\nNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cohere\n",
        "co = cohere.Client('w8CnnlzVol2aZEiirZNLUs0onAqXUUYBZCw2Oj7g')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T20:11:07.211637Z",
          "iopub.execute_input": "2024-04-01T20:11:07.212049Z",
          "iopub.status.idle": "2024-04-01T20:11:07.405822Z",
          "shell.execute_reply.started": "2024-04-01T20:11:07.212015Z",
          "shell.execute_reply": "2024-04-01T20:11:07.404896Z"
        },
        "trusted": true,
        "id": "NPAwX35sL7Yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_query1=[]\n",
        "for i in range(len(retrieved_docs_1)):\n",
        "  retrieved_docs_query1.append(retrieved_docs_1[i].page_content)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T20:12:55.957304Z",
          "iopub.execute_input": "2024-04-01T20:12:55.958812Z",
          "iopub.status.idle": "2024-04-01T20:12:55.963410Z",
          "shell.execute_reply.started": "2024-04-01T20:12:55.958774Z",
          "shell.execute_reply": "2024-04-01T20:12:55.962412Z"
        },
        "trusted": true,
        "id": "jukaV60YL7Yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = co.rerank(query=query_1, documents=retrieved_docs_query1, top_n=3, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned."
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T20:14:41.122610Z",
          "iopub.execute_input": "2024-04-01T20:14:41.123335Z",
          "iopub.status.idle": "2024-04-01T20:14:41.241940Z",
          "shell.execute_reply.started": "2024-04-01T20:14:41.123302Z",
          "shell.execute_reply": "2024-04-01T20:14:41.241190Z"
        },
        "trusted": true,
        "id": "dkE78buML7Yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T20:14:51.154201Z",
          "iopub.execute_input": "2024-04-01T20:14:51.155098Z",
          "iopub.status.idle": "2024-04-01T20:14:51.161009Z",
          "shell.execute_reply.started": "2024-04-01T20:14:51.155063Z",
          "shell.execute_reply": "2024-04-01T20:14:51.160147Z"
        },
        "trusted": true,
        "id": "vCc8AxBGL7Yo",
        "outputId": "c1a6e081-8d4b-466d-fbf3-9755a50af531"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 61,
          "output_type": "execute_result",
          "data": {
            "text/plain": "RerankResponse(id='0ed6eed8-6229-4dbe-8ec9-998cb737c76c', results=[RerankResponseResultsItem(document=None, index=1, relevance_score=0.9396924), RerankResponseResultsItem(document=None, index=0, relevance_score=0.72321177), RerankResponseResultsItem(document=None, index=2, relevance_score=0.047869008)], meta=ApiMeta(api_version=ApiMetaApiVersion(version='1', is_deprecated=None, is_experimental=None), billed_units=ApiMetaBilledUnits(input_tokens=None, output_tokens=None, search_units=1.0, classifications=None), warnings=None))"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(query_1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T20:44:48.242997Z",
          "iopub.execute_input": "2024-04-01T20:44:48.243765Z",
          "iopub.status.idle": "2024-04-01T20:44:48.248594Z",
          "shell.execute_reply.started": "2024-04-01T20:44:48.243733Z",
          "shell.execute_reply": "2024-04-01T20:44:48.247656Z"
        },
        "trusted": true,
        "id": "-8vcZKJjL7Yp",
        "outputId": "986afcc0-ad24-4c11-d0d8-744dc518a528"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": " What are some challenges associated with data collection?\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = co.rerank(query=query_1, documents=retrieved_docs_query1, top_n=3, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
        "for idx, r in enumerate(results.results):\n",
        "    print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
        "    print(f\"Source Document: {retrieved_docs_1[r.index].metadata}\")\n",
        "    print(f\"Document: {retrieved_docs_query1[r.index]}\")\n",
        "    print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T20:37:27.161937Z",
          "iopub.execute_input": "2024-04-01T20:37:27.162941Z",
          "iopub.status.idle": "2024-04-01T20:37:27.281620Z",
          "shell.execute_reply.started": "2024-04-01T20:37:27.162905Z",
          "shell.execute_reply": "2024-04-01T20:37:27.280720Z"
        },
        "trusted": true,
        "id": "CWIzNGsvL7Yp",
        "outputId": "3c424528-4829-48f4-823e-d573a272ed14"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Document Rank: 1, Document Index: 1\nSource Document: {'source': '/kaggle/input/audio-files/audio_1.txt'}\nDocument: is the fact that this data has to be collected to begin with. So historically, a lot of data was collected manually through forms and so on. And one part of the problem would be that these forms were manually entered by somebody and then converted to electronic forms. So that would be, there would be two levels of potential sources for errors. The person writing down the information and then the\nRelevance Score: 0.94\n\n\nDocument Rank: 2, Document Index: 0\nSource Document: {'source': '/kaggle/input/audio-files/audio_1.txt'}\nDocument: other aspects which go into this. One is the fact that this data has to be collected to begin with. So historically, a lot of data was collected manually through forms and so on. And one part of the problem would be that these forms were\nRelevance Score: 0.72\n\n\nDocument Rank: 3, Document Index: 2\nSource Document: {'source': '/kaggle/input/audio-files/audio_4.txt'}\nDocument: some way, right? For instance, if it is some data, what I don't, let us say today's everybody's favorite data set is COVID, right? So it might be collected statewide. So it might be that though the data is anonymous and randomized and all that it's actually the first. So many tens of thousands of things are from a particular state, and then next state, and so on and so on will collect it and\nRelevance Score: 0.05\n\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_query2=[]\n",
        "for i in range(len(retrieved_docs_2)):\n",
        "  retrieved_docs_query2.append(retrieved_docs_2[i].page_content)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T20:26:54.798106Z",
          "iopub.execute_input": "2024-04-01T20:26:54.798909Z",
          "iopub.status.idle": "2024-04-01T20:26:54.804030Z",
          "shell.execute_reply.started": "2024-04-01T20:26:54.798872Z",
          "shell.execute_reply": "2024-04-01T20:26:54.802917Z"
        },
        "trusted": true,
        "id": "bXCvbwAAL7Yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(query_2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T20:45:03.266653Z",
          "iopub.execute_input": "2024-04-01T20:45:03.267046Z",
          "iopub.status.idle": "2024-04-01T20:45:03.271865Z",
          "shell.execute_reply.started": "2024-04-01T20:45:03.267018Z",
          "shell.execute_reply": "2024-04-01T20:45:03.270886Z"
        },
        "trusted": true,
        "id": "9F5k1xHjL7Yp",
        "outputId": "827f733b-6ae3-4a2d-d09a-77e98aa426f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": " What are the advantages of the Apriori algorithm?\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = co.rerank(query=query_2, documents=retrieved_docs_query2, top_n=3, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
        "for idx, r in enumerate(results.results):\n",
        "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
        "  print(f\"Source Document: {retrieved_docs_2[r.index].metadata}\")\n",
        "  print(f\"Document: {retrieved_docs_query2[r.index]}\")\n",
        "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T20:38:00.913548Z",
          "iopub.execute_input": "2024-04-01T20:38:00.913896Z",
          "iopub.status.idle": "2024-04-01T20:38:01.047047Z",
          "shell.execute_reply.started": "2024-04-01T20:38:00.913871Z",
          "shell.execute_reply": "2024-04-01T20:38:01.046110Z"
        },
        "trusted": true,
        "id": "GBSuKGMKL7Yp",
        "outputId": "63ff8573-b494-486a-d93b-1b142811f71e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Document Rank: 1, Document Index: 1\nSource Document: {'source': '/kaggle/input/audio-files/audio_1.txt'}\nDocument: and something turns out to be genuine, and then you pay it, you're paying double. It makes a lot of sense. If you spend some effort and time to check a fraud and you save on paying out something which you should not. So there is a trade off there. And this is, again, something where if you had an efficient system which was based on machine learning, you could possibly get some benefits. And\nRelevance Score: 0.20\n\n\nDocument Rank: 2, Document Index: 3\nSource Document: {'source': '/kaggle/input/audio-files/audio_1.txt'}\nDocument: template as. So I just give you a line here which says a Y equal to Mx plus c. And here I want a specific m zero and C zero, which fits the given data in the best possible way, which is also part of the description of the algorithm. What does it mean for one line to be better than another line? So that's part of it. So the other side of this picture, as I mentioned, is when you don't have this\nRelevance Score: 0.14\n\n\nDocument Rank: 3, Document Index: 2\nSource Document: {'source': '/kaggle/input/audio-files/audio_3.txt'}\nDocument: tree will be a kind of implicit importance, ranking among the attributes, which ones are more significant. So not all columns are equally important. So these are all things that have come up. So maybe when we come to it later on we'll discuss it. But these questions that you're asking, sir, when we form this decision tree, is it important or in some sense beneficial if we know the data? Insight of\nRelevance Score: 0.13\n\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_query3=[]\n",
        "for i in range(len(retrieved_docs_3)):\n",
        "  retrieved_docs_query3.append(retrieved_docs_3[i].page_content)\n",
        "\n",
        "print(query_3,\"\\n\")\n",
        "results = co.rerank(query=query_3, documents=retrieved_docs_query3, top_n=3, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
        "for idx, r in enumerate(results.results):\n",
        "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
        "  print(f\"Source Document: {retrieved_docs_3[r.index].metadata}\")\n",
        "  print(f\"Document: {retrieved_docs_query3[r.index]}\")\n",
        "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T20:45:45.872588Z",
          "iopub.execute_input": "2024-04-01T20:45:45.872966Z",
          "iopub.status.idle": "2024-04-01T20:45:46.013463Z",
          "shell.execute_reply.started": "2024-04-01T20:45:45.872937Z",
          "shell.execute_reply": "2024-04-01T20:45:46.012609Z"
        },
        "trusted": true,
        "id": "PseqspuhL7Yq",
        "outputId": "7e5975ec-92b7-4f0c-94f8-b1318dc9d579"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": " What distinguishes training data in supervised learning, and what is its purpose?  \n\nDocument Rank: 1, Document Index: 0\nSource Document: {'source': '/kaggle/input/audio-files/audio_4.txt'}\nDocument: So training data is both the entire data set that you are given to start with. And then it is the specific choice that you make about how to split this data into a large part, which you will use for building the model, and a small part that you will reserve for validating the model. So depending on how much data you have, the usual practice is about 20% to 30% of it you keep aside. So you use\nRelevance Score: 0.96\n\n\nDocument Rank: 2, Document Index: 1\nSource Document: {'source': '/kaggle/input/audio-files/audio_3.txt'}\nDocument: given a new row, a new item, a new combination of these attributes, which is not seen before, try to predict what would be the most appropriate category to assign. So these items which are given with the labels. So these are called labels. So these items are called training data. So we have labeled training data, which consists of some information which has been collected in the past, some\nRelevance Score: 0.93\n\n\nDocument Rank: 3, Document Index: 3\nSource Document: {'source': '/kaggle/input/audio-files/audio_4.txt'}\nDocument: usually training data is collected in some way, right? For instance, if it is some data, what I don't, let us say today's everybody's favorite data set is COVID, right? So it might be collected statewide. So it might be that though the\nRelevance Score: 0.48\n\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_query4=[]\n",
        "for i in range(len(retrieved_docs_4)):\n",
        "  retrieved_docs_query4.append(retrieved_docs_4[i].page_content)\n",
        "\n",
        "print(query_4,\"\\n\")\n",
        "results = co.rerank(query=query_4, documents=retrieved_docs_query4, top_n=3, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
        "for idx, r in enumerate(results.results):\n",
        "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
        "  print(f\"Source Document: {retrieved_docs_4[r.index].metadata}\")\n",
        "  print(f\"Document: {retrieved_docs_query4[r.index]}\")\n",
        "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T20:46:08.476513Z",
          "iopub.execute_input": "2024-04-01T20:46:08.477475Z",
          "iopub.status.idle": "2024-04-01T20:46:08.611554Z",
          "shell.execute_reply.started": "2024-04-01T20:46:08.477429Z",
          "shell.execute_reply": "2024-04-01T20:46:08.610724Z"
        },
        "trusted": true,
        "id": "Wj2agSdiL7Yq",
        "outputId": "4cec0692-0370-43fd-cca6-2003f0f0d16f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": " What is cross-validation? \n\nDocument Rank: 1, Document Index: 0\nSource Document: {'source': '/kaggle/input/audio-files/audio_4.txt'}\nDocument: cross validation, cross validation. What I'm saying is that you do not make a hard and fast choice once and for all to split your data this way. So you're taking your data and you're saying, hey, first let me leave out this 20% and build one model. So I am building multiple models. I'm building m one, m two, m three, m four, and m five. Each of these models is built using 80% of the test data, but\nRelevance Score: 0.99\n\n\nDocument Rank: 2, Document Index: 3\nSource Document: {'source': '/kaggle/input/audio-files/audio_4.txt'}\nDocument: is this method actually giving me something reasonable? And is it giving me better something than something else? So, for this one strategy is so called cross validation. So what you do is you take some fraction, say, in this case, it is one fifth. So 20%. So you take your data and you leave out. So you randomize it into five groups, which preserve, in some sense, the random behavior, so they\nRelevance Score: 0.96\n\n\nDocument Rank: 3, Document Index: 2\nSource Document: {'source': '/kaggle/input/audio-files/audio_4.txt'}\nDocument: statement to start with, which is, is this method actually giving me something reasonable? And is it giving me better something than something else? So, for this one strategy is so called cross validation. So what you do is you take\nRelevance Score: 0.95\n\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_query5=[]\n",
        "for i in range(len(retrieved_docs_5)):\n",
        "  retrieved_docs_query5.append(retrieved_docs_5[i].page_content)\n",
        "\n",
        "print(query_5,\"\\n\")\n",
        "results = co.rerank(query=query_5, documents=retrieved_docs_query5, top_n=3, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
        "for idx, r in enumerate(results.results):\n",
        "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
        "  print(f\"Source Document: {retrieved_docs_5[r.index].metadata}\")\n",
        "  print(f\"Document: {retrieved_docs_query5[r.index]}\")\n",
        "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T20:46:19.827764Z",
          "iopub.execute_input": "2024-04-01T20:46:19.828731Z",
          "iopub.status.idle": "2024-04-01T20:46:19.926275Z",
          "shell.execute_reply.started": "2024-04-01T20:46:19.828697Z",
          "shell.execute_reply": "2024-04-01T20:46:19.925411Z"
        },
        "trusted": true,
        "id": "SHHhMN5lL7Yq",
        "outputId": "dd60b6c8-c313-4d5d-ec45-7f147c155cef"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "What is the process of building the decision tree classifier, and how is it trained on the dataset? \n\nDocument Rank: 1, Document Index: 0\nSource Document: {'source': '/kaggle/input/audio-files/audio_5.txt'}\nDocument: to build a decision tree in python. If you have the data, what you need to do manually is to probably decide. So the question is, can we specify a certain threshold? A certain threshold of what? Yes sir. So there is also stopping criterion where we can stop. Correct. So you can also specify criterion saying that, saying that stop when your improvement is less than so much and so on. All these\nRelevance Score: 0.96\n\n\nDocument Rank: 2, Document Index: 1\nSource Document: {'source': '/kaggle/input/audio-files/audio_5.txt'}\nDocument: a three way thing, right? There are three types of irises that we saw, so it will be essentially a number, zero, one. And now to get this decision tree classifier, we have to take this function that we have imported. So we say take a decision tree\nRelevance Score: 0.95\n\n\nDocument Rank: 3, Document Index: 3\nSource Document: {'source': '/kaggle/input/audio-files/audio_4.txt'}\nDocument: you can actually construct a decision tree for it. So this tree, which we will see later, this python library for decision trees, actually constructs from the iris data set, this decision tree. So what you do is you start with these\nRelevance Score: 0.77\n\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parent Document Retriever\n",
        "Version 2 - Larger chunks"
      ],
      "metadata": {
        "id": "jJ1wjLRDL7Yr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.storage import InMemoryStore\n",
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=50)\n",
        "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
        "vectorstore = Chroma(\n",
        "    collection_name=\"full_documents\", embedding_function=hf\n",
        ")\n",
        "store = InMemoryStore()\n",
        "retriever = ParentDocumentRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        "    parent_splitter=parent_splitter\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T14:41:04.646115Z",
          "iopub.execute_input": "2024-04-03T14:41:04.646990Z",
          "iopub.status.idle": "2024-04-03T14:41:06.772816Z",
          "shell.execute_reply.started": "2024-04-03T14:41:04.646959Z",
          "shell.execute_reply": "2024-04-03T14:41:06.771678Z"
        },
        "trusted": true,
        "id": "FJWaVW_GL7Yr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adding original documents before splitting or chunking as the retriever already does these\n",
        "retriever.add_documents(documents, ids=None)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T14:41:21.011415Z",
          "iopub.execute_input": "2024-04-03T14:41:21.011988Z",
          "iopub.status.idle": "2024-04-03T14:47:28.587446Z",
          "shell.execute_reply.started": "2024-04-03T14:41:21.011959Z",
          "shell.execute_reply": "2024-04-03T14:47:28.586587Z"
        },
        "trusted": true,
        "id": "mdXssW4lL7Yr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(store.yield_keys())"
      ],
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2024-04-03T14:47:33.856030Z",
          "iopub.execute_input": "2024-04-03T14:47:33.856795Z",
          "iopub.status.idle": "2024-04-03T14:47:33.866727Z",
          "shell.execute_reply.started": "2024-04-03T14:47:33.856764Z",
          "shell.execute_reply": "2024-04-03T14:47:33.865854Z"
        },
        "trusted": true,
        "id": "-wrGMOd5L7Yr",
        "outputId": "04230ec1-f8e9-4fd4-bf45-f9f749ea639f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['7da65afb-a10d-4ee6-9774-c8bbf99be0ed',\n",
              " '3b8b7902-f8a3-442e-89a0-2e6ed94ccb94',\n",
              " 'a912eadc-4e4c-4264-9146-d3072d9f03ed',\n",
              " 'bcc54842-dc9c-4b2a-949c-8d38061e6cb2',\n",
              " '4e94fd45-2c71-4953-a6b5-135ef9166f7a',\n",
              " '54e377b5-3dfc-4c54-955c-e605517643b3',\n",
              " 'ed5ae530-464b-47dc-b06d-4cf467a3c9c5',\n",
              " '9cc8a3db-dad6-4bea-86ec-c6f49243eb1e',\n",
              " 'c084f9a3-4c61-45e3-9937-0fb2df63cf87',\n",
              " 'd4f89984-8034-4127-8fc2-1c5838d2d407',\n",
              " 'a8368a2a-5d1b-4c8e-aa50-2f327c54c920',\n",
              " 'c0edd486-28a7-4346-8827-a139254753a4',\n",
              " 'ac123263-f42d-4b15-a8a8-4d61dbc14a59',\n",
              " '71cba8df-1ff9-4bca-ba86-e3c0e90a1a65',\n",
              " '760a98d8-726c-41bd-a0a9-cd87998a4c2f',\n",
              " '18c9f62a-4468-42af-8b0f-80eedca4b2c9',\n",
              " '7cd1d4d5-3d06-4f36-b501-9180b24f8d60',\n",
              " 'f8f590e2-bc62-4f5e-9852-add9c2b901d8',\n",
              " '64c92887-244d-4449-978c-5501aa56cf72',\n",
              " '8f9c7ed7-d59c-4388-a845-ca1f882f0bf8',\n",
              " '9df1d3bd-7edc-4304-b43e-77e3726dd65b',\n",
              " '0b5fe507-1070-43ea-8a56-1c5095b1f50a',\n",
              " '126efadf-822a-4f11-97a7-b64e580a3564',\n",
              " 'a3983444-5343-4d03-9265-35a50b925dc7',\n",
              " '04ad72ba-c9b0-4373-8b31-a6d25fbb7b65',\n",
              " '8f5ce2aa-999a-467f-a897-b035137c5f2a',\n",
              " 'ee888093-7469-4eb4-b3ab-26c39d638881',\n",
              " '99d720c6-fd46-4e7d-8b16-7555f1df3446',\n",
              " '04c3f42a-78be-4ed4-b9e9-c277d6846ce6',\n",
              " '878a358a-5f5a-4774-8529-cc94de23d817',\n",
              " 'bd810c77-0a74-4c64-8722-43043255cbbb',\n",
              " 'e2599dcf-8e28-4931-bf6a-23944e25dd79',\n",
              " 'f24cb291-f9b2-4f82-8eb2-3889b7ae2dd1',\n",
              " '6d36ff18-2c62-4209-afbf-1e0974c4d20a',\n",
              " '452a35ca-d272-40d5-b0b8-398c2967eb00',\n",
              " '1aeddcc1-6c16-453e-83df-e5ff5828b825',\n",
              " 'eaffb8fa-d452-467d-88fb-0025b3dcb4a7',\n",
              " '8f93ca8b-229e-42ca-a8bd-a2af8862683e',\n",
              " 'c19c6003-f326-4ed1-8b92-dd473ea0eeeb',\n",
              " 'd7c8ae80-f839-4aa7-8ab1-fdd20acb05aa',\n",
              " '6170f9c4-3af8-4f58-95cf-032c942458c9',\n",
              " '427e745b-f7b5-40b8-8d69-03b7ebecd4e0',\n",
              " '9cde8d4d-1fa5-4a98-9f68-d33d5081535b',\n",
              " '07454430-9223-44f7-ac56-d46546dbefb9',\n",
              " '9910b14b-a0ff-4225-9b8d-e7eb22bee614',\n",
              " '68c46c8c-3cf3-4669-b335-09997adc245b',\n",
              " '8823dab1-4793-4157-8b5c-bf9dc5ee7e16',\n",
              " '7b764209-6d46-4687-aaa5-e1321e82c4f0',\n",
              " '591d2bee-0f53-4842-ac44-d5619ff9c708',\n",
              " '7cb91d11-d219-4490-a88b-210688371564',\n",
              " 'fe87835d-b571-46f6-9da4-f30f49b002fe',\n",
              " '6c42cc8a-3ac6-450a-8190-55fde66c6aea',\n",
              " '6ba77e2f-29ee-4635-a824-1e3153bea74d',\n",
              " '66dca61b-2a48-4239-a947-f3b34c52573e',\n",
              " '0d851b6e-1552-452f-b3c0-cbf6e8a88082',\n",
              " '6648f70d-4f8b-4cfe-b42f-ef08ac850987',\n",
              " '5b334945-3235-47e0-a76f-6e37ca8a0946',\n",
              " '3850cee8-8c30-4d3e-ac1e-1b26db23a90a',\n",
              " 'c3b837a4-0fc4-45ff-a033-b2b40d6443a1',\n",
              " 'a0b878b9-52b2-4914-b6cf-363ba53319b4',\n",
              " '6c9c9972-3228-494d-a015-576c7bebddd3',\n",
              " '8c633258-d2ec-4049-9ac1-57d41ca6c156',\n",
              " '576c09ca-908d-4563-bf6a-4f35a13cc03b',\n",
              " 'd00b25f1-d4e7-47e5-8e9c-d404babdaec8',\n",
              " '222824be-7e07-48c5-9dc4-37a4a565a6d9',\n",
              " '01293805-1238-4a1b-ab1a-7d882aef25fc',\n",
              " '47ab35b4-26b8-48ec-ac57-547fc4349097',\n",
              " '0397f960-ac78-41e7-98b5-a25aff2da00e',\n",
              " '2a3de078-7a51-40ce-b25a-2a5408be8534',\n",
              " '0b7ac8f3-68fa-4f94-ba23-77bc6455bf80',\n",
              " '173ddccc-8252-435c-a394-127281578668',\n",
              " '2aa084c1-22f9-4573-8e66-c96c449fb0e2',\n",
              " 'adfdec10-15d5-4f03-a00d-edd3b2ef3b11',\n",
              " '8f933bd7-26f8-4db6-adac-176907952e5d',\n",
              " 'fb2121f6-22ec-4313-a883-82aed53fc32c',\n",
              " '272abc98-88a2-44e2-b0f6-961752756b1b',\n",
              " '6738ef18-278a-4217-9c44-d5594904f6dc',\n",
              " '43e1d02c-3c77-413f-9cae-fefdfe7bd193',\n",
              " 'f60694ab-753b-4007-a290-7fbca51b1e07',\n",
              " 'e3ad6cec-cbca-4d84-891a-8cb82c4c9fd0',\n",
              " '79e8432d-cd68-4857-b70a-0731bad635ca',\n",
              " '2d0501a0-28d6-41d1-b518-cad7b4fbb3ee',\n",
              " '00574ac7-52c9-4e69-993d-a9bb5de41c26',\n",
              " '65f21fc0-45a1-45fd-a93a-6075d6fa104f',\n",
              " '83991a99-f11d-4885-bed8-ce04e334219d',\n",
              " '2e0e9462-740b-4438-bc42-eeedb3b35da7',\n",
              " 'e0258570-5308-4a32-9ea4-919388606ec9',\n",
              " '4ff0b5cc-f4ff-4c05-98f6-7e1d61ea644c',\n",
              " '1fa998e7-28fc-4a82-aa4b-3004415d2663',\n",
              " '33eec1fb-e8e3-47c2-b2c1-6181006e7a2e',\n",
              " 'b709a6d8-8590-4239-8703-aae4f3f09958',\n",
              " '8dace65f-e67a-40c2-8234-3097f969886a',\n",
              " '6f7415fd-375e-476d-9887-e7390c23136d',\n",
              " 'c6e1ae20-b16c-4ff6-8d0f-121dfde6285c',\n",
              " '38a4e657-1d42-4432-b40a-88504f1c82ff',\n",
              " '38615017-a84b-4846-bdfa-9c02a4b621ea',\n",
              " '41ade90b-7e41-4988-9dbe-c3bc7928215d',\n",
              " '3e2629e3-bc72-4746-81cc-d7a97829b1cb',\n",
              " 'ff998b71-be09-420c-8e8c-67dd76e1c93e',\n",
              " 'e425cd5a-12d8-4ad5-9df0-648b5998240b',\n",
              " 'd18d3bb9-9b0c-479d-8d89-da19d394c6dd',\n",
              " 'bfe718e4-c4a9-4bb0-8c9a-aa27eea724dd',\n",
              " 'ddbf1b2a-58c5-4cd5-ac83-852aa0f9214b',\n",
              " '106fb91d-8a0b-4379-ab26-b3b06cfe3355',\n",
              " '84fa4e7e-9fc0-48db-bbaf-6c400260cefa',\n",
              " '152b00ac-b319-4562-ac2c-295e6e17daa6',\n",
              " '7937c499-8640-4d9e-822d-79a2b4c1800e',\n",
              " '5875ffd2-ff6c-48e7-9804-08e1e61658a6',\n",
              " 'd3e3abf8-5bea-4516-ab3c-c7c1afc79ab8',\n",
              " 'd0cf8a6b-fc24-46d5-84e1-6ae5e434d043',\n",
              " 'b281042d-e0f7-4f48-9c48-601b963926a1',\n",
              " '67a4354f-3950-4853-b7fc-6d2dec420571',\n",
              " 'e3a8e2d7-1022-4dfe-9b31-6e9d54d3dcb4',\n",
              " '33400992-f0c7-477b-accc-7c7a7f2f1bf7',\n",
              " '8d43a72e-0d63-450a-b8d0-14344eecb7f2',\n",
              " '858c96f2-5d9e-42c6-9d67-03291fa2059d',\n",
              " '0fb01870-a342-4257-881f-4fe33da81527',\n",
              " 'bf72430f-1582-41aa-acd2-393d1149393a',\n",
              " '0ebc622d-66e1-41f4-b4ab-827626bd75a1',\n",
              " 'c2969bd1-f4b5-4e47-8153-58a4f1f2d0d7',\n",
              " 'ec7bddf8-5ab0-44ba-b4b9-521afe933775',\n",
              " '25974fd7-810d-4279-97e0-dac2962bc7fa',\n",
              " '78804e6e-6d9c-43ed-8623-4dce5dab695e',\n",
              " 'dd4f9683-7559-44bc-8d30-7bab15f7d690',\n",
              " 'eb4317f9-27fd-4bd3-af88-fcd7e72123aa',\n",
              " 'a4cad609-2d91-49a3-a1ad-ff0179f8f484',\n",
              " 'c3aebf9e-bb9f-4c7b-9dfc-68b35d7912c2',\n",
              " '6d52c2b1-b541-4a8d-aaca-1df4fac7b1dc',\n",
              " 'b8e1ebd2-80f5-47a9-85fe-cec3a0ec0243',\n",
              " 'f710a047-11ea-4cf6-bc20-de7545c61ee7',\n",
              " 'c34c3e6d-471a-495b-8229-28b6b7a74bbb',\n",
              " 'bfc0736c-2107-40fe-aaf3-6097653a1c59',\n",
              " '349b5110-6bb9-4570-8e75-2dcdc62093a8',\n",
              " '8723e776-4d65-4f89-a0c8-d7c1ff26ccb3',\n",
              " 'e677096d-ec2e-4ed9-b2e6-cf16987e6b3e',\n",
              " 'd5694b4c-17c2-4195-ae7d-51d230e3a790',\n",
              " 'ec9f486f-e5fe-4833-bb85-d07f27593387',\n",
              " 'bf12cb97-a0bc-4bf1-859d-8e98bac59867',\n",
              " '27b06d6b-02ea-4501-a0e3-34ae71df6ae0',\n",
              " '2e2b579d-35d7-405a-80de-b935b56c1feb',\n",
              " '4aee7a2b-6287-4a97-b67c-2f414ef7a7af',\n",
              " 'a990bb34-6973-4950-af74-a5887d808c44',\n",
              " 'a1f0464c-5fc8-42c6-b979-af32c8291d46',\n",
              " '49ccac4e-fb47-4851-bc55-aa4c726b45e5',\n",
              " '8cfd2836-64d2-477c-a615-8f0d02af0f55',\n",
              " '31c83552-6dda-462d-b431-546dc2ed2a05',\n",
              " '00a27351-8380-4a02-b3cf-3663350fe0e1',\n",
              " '86613e56-98a0-4c0e-8f41-fb3f9b6907c3',\n",
              " '83aacd47-bf50-4001-8660-ef62f206aed4',\n",
              " '94701c38-4e40-4ec2-bbfc-75025e6624d6',\n",
              " '381bd95d-7f8e-4b8e-a054-4063d82c3712',\n",
              " 'a3e529ae-409d-4a26-8d42-c45213e84cb1',\n",
              " 'ce62ff34-5408-4bda-a615-170b76ae4a3e',\n",
              " '921a48d7-794b-401e-b227-14168f8810c6',\n",
              " '81b39ca5-0e6d-49d4-8d7c-37b9be5443b0',\n",
              " 'a95e341d-238f-45db-bd26-07a41ac50755',\n",
              " 'c24b373e-806f-4601-a26f-79952a689fd1',\n",
              " '9b346973-d8ee-404a-bcc4-50fe3729dcb1',\n",
              " 'f2fe2607-04c8-40d1-a4be-1e78861bf09a',\n",
              " 'ed487086-9f4b-4aa6-aa5b-b2ef07ad1b2b',\n",
              " 'c100003b-1377-40b1-aa09-700feff23568',\n",
              " '8114e68f-c5d6-4bd2-bbc1-d6f5d043793e',\n",
              " '44c30f8f-de9a-4e77-8b98-4d9d8ee1c39a',\n",
              " '6660edd4-8ea6-4baa-b1bd-d227dd0ed8d2',\n",
              " '17431a21-fd93-48ac-9c21-ece6f5198e1d',\n",
              " '47f4760c-61c6-48cf-a28f-30589fcc0032',\n",
              " '742b880f-c877-48d0-9160-a01435c69f0e',\n",
              " '46f67f8b-6a21-4573-84f1-d3a7f22b4b41',\n",
              " '1a1ea472-87aa-44fd-aa3e-ecbce00057f4',\n",
              " 'c428c278-d8ce-47c2-a63e-952a0829cf22',\n",
              " 'a8134516-9cac-440b-a381-76c3988e9451',\n",
              " '197a4035-a992-4ba9-802e-4ce44b3f9d4d',\n",
              " '8cfc00db-f1a2-444f-a915-f24c9c864b01',\n",
              " '00054508-0975-4130-85be-23cc330bf029',\n",
              " '3ead565d-ff3d-455b-9606-f3ab0caac064',\n",
              " '85162332-ea50-462a-9218-99f6b4d86930',\n",
              " '9344f3f7-1e30-4625-91e9-baa6289e2bb8',\n",
              " '3444cfc8-241e-48d2-871a-c00abaaa15fb',\n",
              " '35845eea-fc2b-4613-a651-be324bd96843',\n",
              " 'efdc66e1-adf9-466f-b454-8368acbc765e',\n",
              " '01b19b48-877f-45ca-a347-9b3c5187c283',\n",
              " '3bafb7b0-b442-479d-880f-ce9e53713e8c',\n",
              " '52276596-35ea-451b-a39c-99d73d5b51c9',\n",
              " '122e443e-1a07-45b7-8d3d-8873a254adf1',\n",
              " '09511fe2-5723-46bb-b943-51d9e4671e00',\n",
              " '629e3e0d-212c-418a-a8bd-c93ebbec17c6',\n",
              " '5b0cda4a-fe13-497f-a4ec-82b579c32069',\n",
              " '32172484-3ce7-4845-b887-e85fde324b7a',\n",
              " '1789410f-339c-4a7a-8656-cd2f281c965f',\n",
              " '2fb0cc34-0d10-4db1-a326-722b2fc6f4f8',\n",
              " 'b8cb580e-b9ed-40d5-9f73-1662788c48fd',\n",
              " '902e5105-3705-4550-81a6-2817a0ea5b8f',\n",
              " '70cee8e8-4090-47a8-a2a7-6a6fb17dc0af',\n",
              " 'b28f5930-afa1-48a9-9cdf-2623f389a403',\n",
              " 'ce84a657-388f-4894-b7bd-1bb6a16ed7ce',\n",
              " 'fd104dbd-bb2c-4cd4-94f2-a0a989d2c172',\n",
              " 'd552d4d0-1def-48e4-b425-a85d5f392a32',\n",
              " '3aa63d85-16da-4821-9918-4e802f8c6090',\n",
              " '9a597669-c4ba-4226-9017-75f03be54ca6',\n",
              " '75b04d96-afa9-4b89-9b4e-0be086500bea']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 1"
      ],
      "metadata": {
        "id": "jLNgcjFMYFTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_1=\" What are some challenges associated with data collection?\"\n",
        "sub_docs_1 = vectorstore.similarity_search(query_1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T14:50:06.540964Z",
          "iopub.execute_input": "2024-04-03T14:50:06.541812Z",
          "iopub.status.idle": "2024-04-03T14:50:06.837309Z",
          "shell.execute_reply.started": "2024-04-03T14:50:06.541780Z",
          "shell.execute_reply": "2024-04-03T14:50:06.836289Z"
        },
        "trusted": true,
        "id": "CAck-DYnL7Ys"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sub_docs_1[0].page_content)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T14:50:23.676593Z",
          "iopub.execute_input": "2024-04-03T14:50:23.677445Z",
          "iopub.status.idle": "2024-04-03T14:50:23.682059Z",
          "shell.execute_reply.started": "2024-04-03T14:50:23.677411Z",
          "shell.execute_reply": "2024-04-03T14:50:23.681156Z"
        },
        "trusted": true,
        "id": "udWLdebML7Ys",
        "outputId": "642b5b85-d8b1-44d4-889a-907c6398d306",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "entered by somebody and then converted to electronic forms. So that would be, there would be two levels of potential sources for errors. The person writing down the information and then the person typing in the information. Now, gradually these kind of electronic forms are spilled in directly, so at least the source of the error is reduced to one step. But still, people mistype things. I mean, there are any number of situations where people type their email address wrong and so notifications don't reach them and so on. So there is this data collection. How do you collect the data and how do you clean it? And the third thing is, how do you make it uniform? So when data is being collected by\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sub_docs_1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T14:50:28.267195Z",
          "iopub.execute_input": "2024-04-03T14:50:28.267934Z",
          "iopub.status.idle": "2024-04-03T14:50:28.274269Z",
          "shell.execute_reply.started": "2024-04-03T14:50:28.267903Z",
          "shell.execute_reply": "2024-04-03T14:50:28.273294Z"
        },
        "trusted": true,
        "id": "jScYTbOhL7Yt",
        "outputId": "fcdde94d-6950-4764-b9f7-78553f8f836a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"entered by somebody and then converted to electronic forms. So that would be, there would be two levels of potential sources for errors. The person writing down the information and then the person typing in the information. Now, gradually these kind of electronic forms are spilled in directly, so at least the source of the error is reduced to one step. But still, people mistype things. I mean, there are any number of situations where people type their email address wrong and so notifications don't reach them and so on. So there is this data collection. How do you collect the data and how do you clean it? And the third thing is, how do you make it uniform? So when data is being collected by\", metadata={'doc_id': '3b8b7902-f8a3-442e-89a0-2e6ed94ccb94', 'source': '/content/audio_1.txt'}),\n",
              " Document(page_content='it uniform? So when data is being collected by different people, they may collect different things. And for instance, if you look at the government, typically the government collects data in different forms. For instance, there is a public distribution system which the ration shops, so they collect some information about who is collecting ration and who is income levels and so on. Then on the other side, you have, of course, things like electoral information. Then you have property information. Who owns property, who pays electricity bills, who owns vehicles, vehicle registration, driving licenses. Not necessarily the same thing. Not everybody who has a driving license owns a vehicle. So', metadata={'doc_id': '3b8b7902-f8a3-442e-89a0-2e6ed94ccb94', 'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"So clearly there is something going on in the background, and that background in our case is revealed through a test. Even tests have errors and all that. So it's really a problem that we have to grapple with in real life. And we will just assume for now that when we reach such a situation, we just take a majority decision. But realistically, if we get too many of these situations, and it's very difficult to decide, we have to ask whether the data has actually been collected correctly. So as we said, the main issue is really how to ask the questions and what this means. Sorry, sir, I have a question. Yeah. Sir, once we know that the data is young, we are making a prediction and it come out\", metadata={'doc_id': '3850cee8-8c30-4d3e-ac1e-1b26db23a90a', 'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"is itself a kind of computationally intensive task. How do you provide a lot of well trained examples? Because the assumption is that the labels that these training samples have are somehow correct so they can be trusted. Because if you can't trust those labels, and the model that you build will obviously not work. So if you want to write generalization of the labels that people have given you, you have to trust those labels. So that means somebody reasonably conversant with the domain has to be willing to put in the time to label it. Now, if it happens, as I said, because historically you're collecting data, it's fine. But more and more, we are finding that this is not the only source of\", metadata={'doc_id': 'c0edd486-28a7-4346-8827-a139254753a4', 'source': '/content/audio_1.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_1 = retriever.get_relevant_documents(query_1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T14:48:21.683452Z",
          "iopub.execute_input": "2024-04-03T14:48:21.683839Z",
          "iopub.status.idle": "2024-04-03T14:48:21.984653Z",
          "shell.execute_reply.started": "2024-04-03T14:48:21.683812Z",
          "shell.execute_reply": "2024-04-03T14:48:21.983661Z"
        },
        "trusted": true,
        "id": "Q-yenRdOL7Yt"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(retrieved_docs_1[0].page_content)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T14:48:25.892231Z",
          "iopub.execute_input": "2024-04-03T14:48:25.892757Z",
          "iopub.status.idle": "2024-04-03T14:48:25.899429Z",
          "shell.execute_reply.started": "2024-04-03T14:48:25.892721Z",
          "shell.execute_reply": "2024-04-03T14:48:25.898343Z"
        },
        "trusted": true,
        "id": "5N3ujf_pL7Yt",
        "outputId": "21d32dc9-b234-4552-ab24-3cfb977388da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1996"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T14:48:30.670475Z",
          "iopub.execute_input": "2024-04-03T14:48:30.670893Z",
          "iopub.status.idle": "2024-04-03T14:48:30.678068Z",
          "shell.execute_reply.started": "2024-04-03T14:48:30.670867Z",
          "shell.execute_reply": "2024-04-03T14:48:30.677009Z"
        },
        "trusted": true,
        "id": "ihfwSuJXL7Yt",
        "outputId": "3bef9c23-bc14-4250-cb0a-ffd1a5eee5ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"entered by somebody and then converted to electronic forms. So that would be, there would be two levels of potential sources for errors. The person writing down the information and then the person typing in the information. Now, gradually these kind of electronic forms are spilled in directly, so at least the source of the error is reduced to one step. But still, people mistype things. I mean, there are any number of situations where people type their email address wrong and so notifications don't reach them and so on. So there is this data collection. How do you collect the data and how do you clean it? And the third thing is, how do you make it uniform? So when data is being collected by different people, they may collect different things. And for instance, if you look at the government, typically the government collects data in different forms. For instance, there is a public distribution system which the ration shops, so they collect some information about who is collecting ration and who is income levels and so on. Then on the other side, you have, of course, things like electoral information. Then you have property information. Who owns property, who pays electricity bills, who owns vehicles, vehicle registration, driving licenses. Not necessarily the same thing. Not everybody who has a driving license owns a vehicle. So you have all these different sources of information. Now, is it possible really to find out same person in all these? Well, a lot depends on how the information is collected. You have a common format in which you record information about people. Even names in India tend to be spelt in different ways, written in different ways. Sometimes we expand initials, sometimes we suppress them, sometimes we write middle names, sometimes we don't. Sometimes we invert the order, sometimes we don't. Addresses, of course, are written in a million different ways. So there are all kinds of issues with just getting the data to a format where you can work on\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"So clearly there is something going on in the background, and that background in our case is revealed through a test. Even tests have errors and all that. So it's really a problem that we have to grapple with in real life. And we will just assume for now that when we reach such a situation, we just take a majority decision. But realistically, if we get too many of these situations, and it's very difficult to decide, we have to ask whether the data has actually been collected correctly. So as we said, the main issue is really how to ask the questions and what this means. Sorry, sir, I have a question. Yeah. Sir, once we know that the data is young, we are making a prediction and it come out that young. So like other attribute is not relevant other than it has job or not, and then we are not going to look at other attributes. Yeah, but remember that which attributes to look like vary from one path to another. So maybe in this case, right, so what we are saying is that if they are young, we only need the job, right? But if they are middle aged, we need to know the house. So for different combinations, so they may not occur together. I agree, in any decision, but each of these plays a role somewhere. Now, if there is another attribute which never plays a role, as we will see the next tree as that example. Right? So if you look at this, this is a different tree for the same data set. So instead of first asking the age, we ask whether you own a house or not. And it actually turns out that everybody in this data set who owns a house, six. Out of those, remember we said there were nine yeses and six no's overall, but out of those nine yeses, six of them own a house and all six of them get yes. And nobody owns a house who's told no. And if you don't own a house, then if you have a job, you get it. If you don't have a job. So in some sense, if you don't have a job and you don't own a house, you're not going to get a loan. If you have either of the other two, you're going to\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"is itself a kind of computationally intensive task. How do you provide a lot of well trained examples? Because the assumption is that the labels that these training samples have are somehow correct so they can be trusted. Because if you can't trust those labels, and the model that you build will obviously not work. So if you want to write generalization of the labels that people have given you, you have to trust those labels. So that means somebody reasonably conversant with the domain has to be willing to put in the time to label it. Now, if it happens, as I said, because historically you're collecting data, it's fine. But more and more, we are finding that this is not the only source of data. A lot of data now actually comes out of automatic things like we have systems which are producing. For example, if you run a computerized device like a network switch or something like that, all these things generate a lot of diagnostic data. Even things like cars and planes and vehicles, which run with a lot of electronic components, generate a lot of diagnostic data. That's why whenever there's a crash, for instance, people are looking for the same as black box. Now, if you're trying to build a model based on that, then who is going to sit through and go through all this diagnostic data and tell you that, oh, this part of the data suggests that there's a fault here, and that part of the data suggests that there's a fault there.\", metadata={'source': '/content/audio_1.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 2"
      ],
      "metadata": {
        "id": "tKYFDtpAYKdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#query_1=\" What are some challenges associated with data collection?\"\n",
        "query_2=\" What are the advantages of the Apriori algorithm?\"\n",
        "\n",
        "sub_docs_2 = vectorstore.similarity_search(query_2)\n",
        "retrieved_docs_2 = retriever.get_relevant_documents(query_2)\n",
        "retrieved_docs_2"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T14:51:50.853460Z",
          "iopub.execute_input": "2024-04-03T14:51:50.853845Z",
          "iopub.status.idle": "2024-04-03T14:51:51.444674Z",
          "shell.execute_reply.started": "2024-04-03T14:51:50.853816Z",
          "shell.execute_reply": "2024-04-03T14:51:51.443651Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqLlRo3cL7Yu",
        "outputId": "c24f23eb-544d-43e7-df15-fae4269885b2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"on some attributes. You might, for example, if it is, say, some medical diagnosis, then you may not be very concerned about the date of birth. I mean the year of birth, yes, but you may not need to know whether it's January 27 or May 30. It may not make much difference. So there are some things which may be less or more relevant, or even place of birth may not be so relevant, and so on. So you might automatically throw out some. But in general, we don't know. So in general, we are running all these things. So the algorithm works just looking at the values without asking what does a one mean and what does a five mean. So this is something to keep in mind. So our strategy is to maximize the reduction in the impurity. And this terminology comes from that entropy interpretation. So in entropy information theory, entropy is loss of information. If you have more entropy, you have less information. If you have more information, your entropy reduces. Right. I discussed that if you know the message for certain, you have full information about what the message contains in advance. You don't even need to send the contents, you just need to say there is a message, like a missed call, and you would know what the message means. So, information is entropy going down is the same as information going up. So this is sometimes called maximizing. Reduction in entropy is sometimes also referred to as maximizing the information gain. But because this is an uninterpreted kind of algorithm, that is, we are just looking at the attributes and comparing the information gain without asking what the attributes really signify, we are just taking the one that works best. So the problem is that there could be some attributes which give information gain but have no significance. And a typical example of this is something which is like a serial number, right? So imagine that you have something like Aadhaar number or passport number, or in a class, you have distinct roll numbers. Every student has a\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"I'm not going to formalize this anymore, but it's very similar in idea to the prior year. So you can formalize this algorithm for yourself, but it's essentially saying the same thing, except here the thing is constraint, because we know that z is fixed. So once z is fixed, then we just have to start with each individual element in z as a possible right hand side, and then work forwards to that. We don't have to look at all. So for each z, we do this calculation within that set. So that's how you generate association rules once you have a frequent set. So within that set, you play this a priority game with respect to the size of the right hand side, and then you generate all the rules. Is this clear? Yes. So it's exactly the same as the previous one, just a slightly different presentation of the idea. Okay, so now, as I said, this whole thing so far has been largely a kind of algorithmic problem, right? In some sense, if I fix the set of items and the set of transactions and the set of the two thresholds, the support and the confidence threshold, then for any x, comma y, either it is true or it is not true. So there is a fixed answer. So the answer is known. So this is something where I can actually validate that you have either enumerated all the association rules or you have missed out something. So technically there is no learning involved. It's more an algorithmic question. So how does one connect this to learning? Well, in specific context, you can interpret these rules as classification rules. So, remember we said supervised learning says, if this is there, then it is a fraud. If this thing is not there, then it is not a fraud, and so on. So, let's look at a typical example. So, supposing we have some question which is about labeling the topic of a document. So our items now are words, and our baskets are collections of words, sets of words. So for each document, we have one word which describes its topic. So here is an example. On the right. So the topic\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"When I write in canonical order, it has one fixed order, I one to ik. So if I drop ik, I get one set of size, k minus one. If I drop I k minus one, the second last element, I get a different subset, and if I merge those two, I get back this set. So if I have this set which y which is supposed to be in Ck, then it will actually be got by merging back these two subsets of y. So for every y that should be in Ck, it's going to be in Ck prime. Now I'm also going to get things in Ck prime which are not in Ck because they merge correctly. But there may be some other k minus one subset which is not frequent and which I'm not checking. So there will be some spurious entries in Ck prime, some things which should not be frequent because there is actually a known k minus one set which is not frequent. But I don't bother. I just am interested in saying that Ck prime is good enough for me. So the main advantage of this is that it's relatively speaking, given that our assumption is at every level, this fks are small. This is much faster than going to the bigger set and then filtering down, building up the bigger set from the smaller set. Even if we over approximate is going to be much faster and it's not going to be too much, because we know that the smaller sets are actually quite small. So, so the, if you want to think about how to do this algorithmically. So what we have is we have our set f k minus one. So supposing we just sort it in dictionary order, right? We think of these as k minus one tuples, and we sort it in dictionary order. Meaning that if the first position where two entries differ, if one is smaller than the other one, then that tuple is smaller. The first letter in the word which differs is smaller. So that's a dictionary. So what I will have is I will have a bunch of things where I have the same I one, two, I k minus one, and then I have a different j one. Jk, jk. So I will have a block of things at the beginning of my sorted order, where the first k minus one\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"ratio. It's not just the information gain alone, which is what we had initially. It's information gain divided by the entropy of the attribute. So is this clear what role this is playing? We are just saying that if you have a scattered attribute, then you discount the gain that you get from it by the scattering value. So more scattered attributes will, the net effect will be less because of the denominator. So in terms of our heuristics, what we said is that nonlinear functions work better in practice. And the genie index is in fact the one that is used in most libraries. But we should not blindly use this because of this problem of these scattered attributes. So we will actually use this ratio of business to correct. So we have this information. So now let's get back to this question that we initially observed. That is that something like this age, it would be much more natural if this age were described in terms of numbers. So somehow this data has been arranged so that we get this data in terms of categories. But who does the arranging? So how do we realize that in this context? Sir, I have a question, sir. In this loan example, if we add attribute like is an indian citizen or not, then the attribute has only one class, that is Indians. Suppose a bank has, in this case, the impurity is low because we have only one attribute, but it does not give us. What will happen is if you use such a category, then it will split. As what? Because if everybody is indian, for example. Yes, sir. Then after I text, if I ask the question is the person indian? Then I will get two groups. One group will have zero rows and one group will have the same set. Yes, sir. So therefore the same set will have the same entropy, right. And so the entropy will actually not change. That question will be totally redundant. Right. So I will get the one times the entropy of the table plus zero times the entropy of zero, which is just the entropy of the table that I started with. So the difference\", metadata={'source': '/content/audio_4.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sub_docs_2"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T14:52:24.711898Z",
          "iopub.execute_input": "2024-04-03T14:52:24.712569Z",
          "iopub.status.idle": "2024-04-03T14:52:24.718862Z",
          "shell.execute_reply.started": "2024-04-03T14:52:24.712539Z",
          "shell.execute_reply": "2024-04-03T14:52:24.717953Z"
        },
        "trusted": true,
        "id": "-mVfsk6zL7Yu",
        "outputId": "9dccf338-ab66-4915-cbea-b99736193aff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"on some attributes. You might, for example, if it is, say, some medical diagnosis, then you may not be very concerned about the date of birth. I mean the year of birth, yes, but you may not need to know whether it's January 27 or May 30. It may not make much difference. So there are some things which may be less or more relevant, or even place of birth may not be so relevant, and so on. So you might automatically throw out some. But in general, we don't know. So in general, we are running all these things. So the algorithm works just looking at the values without asking what does a one mean and what does a five mean. So this is something to keep in mind. So our strategy is to maximize the\", metadata={'doc_id': '00574ac7-52c9-4e69-993d-a9bb5de41c26', 'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"I'm not going to formalize this anymore, but it's very similar in idea to the prior year. So you can formalize this algorithm for yourself, but it's essentially saying the same thing, except here the thing is constraint, because we know that z is fixed. So once z is fixed, then we just have to start with each individual element in z as a possible right hand side, and then work forwards to that. We don't have to look at all. So for each z, we do this calculation within that set. So that's how you generate association rules once you have a frequent set. So within that set, you play this a priority game with respect to the size of the right hand side, and then you generate all the rules. Is\", metadata={'doc_id': '902e5105-3705-4550-81a6-2817a0ea5b8f', 'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"be some spurious entries in Ck prime, some things which should not be frequent because there is actually a known k minus one set which is not frequent. But I don't bother. I just am interested in saying that Ck prime is good enough for me. So the main advantage of this is that it's relatively speaking, given that our assumption is at every level, this fks are small. This is much faster than going to the bigger set and then filtering down, building up the bigger set from the smaller set. Even if we over approximate is going to be much faster and it's not going to be too much, because we know that the smaller sets are actually quite small. So, so the, if you want to think about how to do this\", metadata={'doc_id': '3444cfc8-241e-48d2-871a-c00abaaa15fb', 'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"ratio. It's not just the information gain alone, which is what we had initially. It's information gain divided by the entropy of the attribute. So is this clear what role this is playing? We are just saying that if you have a scattered attribute, then you discount the gain that you get from it by the scattering value. So more scattered attributes will, the net effect will be less because of the denominator. So in terms of our heuristics, what we said is that nonlinear functions work better in practice. And the genie index is in fact the one that is used in most libraries. But we should not blindly use this because of this problem of these scattered attributes. So we will actually use this\", metadata={'doc_id': 'b709a6d8-8590-4239-8703-aae4f3f09958', 'source': '/content/audio_4.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 3"
      ],
      "metadata": {
        "id": "_d7zKFnOYzAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#query_1=\" What are some challenges associated with data collection?\"\n",
        "#query_2=\" What are the advantages of the Apriori algorithm?\"\n",
        "query_3=\" What distinguishes training data in supervised learning, and what is its purpose? \"\n",
        "\n",
        "sub_docs_3 = vectorstore.similarity_search(query_3)\n",
        "retrieved_docs_3 = retriever.get_relevant_documents(query_3)\n",
        "retrieved_docs_3"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T14:53:54.183686Z",
          "iopub.execute_input": "2024-04-03T14:53:54.184045Z",
          "iopub.status.idle": "2024-04-03T14:53:54.756723Z",
          "shell.execute_reply.started": "2024-04-03T14:53:54.184018Z",
          "shell.execute_reply": "2024-04-03T14:53:54.755836Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxGuL1RkL7Yu",
        "outputId": "8bcc6a13-dbff-43f8-fcf2-2638bd0187b2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"to the way the data is available. So what we are going to be doing is agnostic to the domain. So we are not going to believe anything or assume anything about the domain, except that we will want to validate at the end somehow that what we are doing is right. But in practice what you're saying makes sense that really to do supervised learning, it's much more realistic to do it in collaboration with somebody who understands a domain rather than just doing it blindly. Because also when you do it blindly, you might come up with solutions which make no sense. You might actually identify features which contribute to a decision purely by statistical coincidence, and no human would actually do that. So there's another issue which comes up of interpretability or explainability of your answers. So there are various factors which get into this. It's only not only accuracy which matters, but also whether your model is sensible in something, so that you can justify it. So these will all come as we go along. So let's proceed with this. So the first thing is that because the choice of attribute to question will determine the structure of the tree. So here are two different trees. So this is the tree we started with, and in a sense this tree, if you remember that table, consists of asking the first column first. So the first column in our table was age. So we asked age, and then maybe we adapted our question based on age, whereas here we have directly jumped to own a house. The question to ask is which of these trees intuitively is better secondary? What is the definition of better over here, a shorter tree or a tree which predicts better? So here both trees are predicting. Exactly. So there is no mistake, at least with respect to the training data. Of course we have not seen what it does with the unknown data, but if you are just looking at its behavior on the training data, all of these things reach nodes in which I don't have any confusion, any majority to choose. So in that\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content='So we were looking at this market basket analysis, and towards the end of the discussion on association rule rules, if everybody who buys X is also likely to buy y. We said that one application of this association rule idea is to have these so called class association rules. So the right hand side was a category. We looked at an example with documents and we said, if a document has these words, it is also likely to belong to this category. So that brings us to the topic that is one of the central ones in machine learning, which is the idea of supervised learning. So in supervised learning, we have a set of items, and each item belongs to some category or class, and we distinguish these items in terms of some attributes. So you can think about this, as we will see shortly, you can think about these attributes as columns, right? So the attributes will be things like name, age, date of birth, whatever, income, stuff like that. So each row in the table will be one item, and there will be a special column which indicates what category, depending on the classification problem that we are dealing with, what category it is. And the goal of supervised learning is to now construct, given a new row, a new item, a new combination of these attributes, which is not seen before, try to predict what would be the most appropriate category to assign. So these items which are given with the labels. So these are called labels. So these items are called training data. So we have labeled training data, which consists of some information which has been collected in the past, some historical information about items and their classification. And we want to find a model that generalizes the training data. So since normally this categorization corresponds to some kind of classification, this is typically called a classification problem. So classification problem could be a binary problem. It could be, yes. No. This mail is junk. This mail is not junk. It could also be multi way, especially', metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"we have is clearly to find the best line. So let's assume we have a simple case of only one variable. So we have x one, x two, and so on. So we have n points. In reality, each of these x size is a vector of k, and the y, by convention, is always the label. So in the earlier thing, when we looked at that scikitlearn decision tree thing, we used capital x to denote the input matrix and y to denote the column matrix of output. So this is a normal standard convention in supervised learning that x's denote the inputs of the training data and y denotes the output label. In this case, it's not a label, but it's an output value. So in order to compute the best fit, we need to measure in some sense how far we are away from the answers that we want. So how far is our prediction away from the true answer? So of course, the usual problem is there that we don't know the true answer. But here, notice that unlike our decision tree, we are not likely to actually, because these values are all not quite lying on a line. It's not that we are actually going to get a line that passes through all the points that we started with. So in a decision tree, we are more or less tailoring our decision tree to give the correct answer for the given. So for overwhelmingly large values, large fraction of the training data, the decision tree will give the correct answer. So measuring the accuracy with respect to the training data will typically not give us very much information about how good the tree is on unseen data, whereas here, because we are kind of assuming that the data is slightly scattered and we will formalize this a little later on, we can measure our prediction with respect to the actual training data. So for each xi, we can first compute what our current model tells us and subtract what I know is the answer for that, which is yi. And now, this could be positive or negative, but I want to measure the error. So I could take absolute value, or in this case, I'm going to square it so I\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"and make the best line out of it. So it's not quite generating a program from a specification, but generating a concrete program from a generic program you can think of a model template as. So I just give you a line here which says a Y equal to Mx plus c. And here I want a specific m zero and C zero, which fits the given data in the best possible way, which is also part of the description of the algorithm. What does it mean for one line to be better than another line? So that's part of it. So the other side of this picture, as I mentioned, is when you don't have this training data, you don't have any examples that people have already labeled, but you are looking for some kind of patterns. So there is no guidance. So the thing about supervised learning is somebody should have gone through and given you this information before. Now, this information could have been collected over time. Maybe it's not manually collected, like the school marks. The school marks over time. Everybody has. I mean, the school keeps a record of what happens in the model exams, what happens in the board exam. So it comes automatically. The bank will be keeping records of all the loans that it bought and what happened to the customers who bought loans. So this historical information is accumulated over time. On the other hand, in some situations, like the example I mentioned about classifying paintings, now, when you go into a museum or you go into some art gallery or something, the painting doesn't have a label on it, saying, this is a painting of this type and this is a painting of that type. So somebody, if you want to fit this training model to it, somebody has to sit and actually label these things for the algorithm. So there is a lot of work in it. So generating a lot of valid training data is itself a kind of computationally intensive task. How do you provide a lot of well trained examples? Because the assumption is that the labels that these training samples have are somehow correct\", metadata={'source': '/content/audio_1.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 4"
      ],
      "metadata": {
        "id": "DBuRkJ1QY7D_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#query_1=\" What are some challenges associated with data collection?\"\n",
        "#query_2=\" What are the advantages of the Apriori algorithm?\"\n",
        "#query_3=\" What distinguishes training data in supervised learning, and what is its purpose? \"\n",
        "query_4=\" What is cross-validation?\"\n",
        "\n",
        "sub_docs_4 = vectorstore.similarity_search(query_4)\n",
        "retrieved_docs_4 = retriever.get_relevant_documents(query_4)\n",
        "retrieved_docs_4"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T14:54:16.161127Z",
          "iopub.execute_input": "2024-04-03T14:54:16.161501Z",
          "iopub.status.idle": "2024-04-03T14:54:16.687222Z",
          "shell.execute_reply.started": "2024-04-03T14:54:16.161471Z",
          "shell.execute_reply": "2024-04-03T14:54:16.686219Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPHtVrgaL7Yu",
        "outputId": "fff0beec-03ab-4257-ed2f-222ddcb7b32a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"is important is remember that we talked about this cross validation. So what do we do in cross validation or even in normal validation? In normal validation or in cross validation. In either case, you pull out some random data, set data points from your input and build a tree on the rest. Now the question is, the model that you built from the rest, is it a stable structure in the sense that is it sensitive to how you chose your training and test data? And this example suggests that you are likely to get different models from different subsets of training data. So you then have to worry about what is your final model going to be. So if you test it on a particular test set, then maybe if you test it on a different test set, you might get an equivalent model, but it might have a very different structure, and maybe for other reasons, that structure might be more natural. So this is a generic problem with this kind of tree model, that it is very sensitive to perturbations in the data. We will look at a more familiar problem, which you are generally aware of called regression. Right? So in regression, what do we do? We take a bunch of points, we try to fit a line to it. Now, if I take one point in that set and remove it, or if I shift it a little bit, the line will perhaps change, but it will change in a very minor way, right? The slope will change slightly. So small changes in input produce small changes in output. So that is a kind of stable kind of situation. So this is something called variance. So how much variance is there in your model? So, variance is basically a property which says, is it the case? So if you have low variance, then small changes in input produce small changes in output. But decision tree, by definition, is a kind of discrete kind of thing. There's no way to slightly change a tree. You change a question and the tree changes drastically. So it has high variance, whereas a line fitting kind of algorithm will have a low variance, because if you\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"the blue flowers, we are eliminating the. We know there is one which is 1.8 and that will get eliminated. Among the green flowers. It's possible that there is something which is 1.8, but because y is equal to two, it will get retaked all the blue flowers which have 1.8, but I believe there's only one in the data set. So suppose instead of the point that we removed in this time, if we removed a different point, would we get a different decision tree? For that, it's not necessary. You will get a decision tree. It's different. But in general, the removing and modifying the data set in a very small way can produce it. So it's not guaranteed. It will produce. So you can experiment with it. So you could, for instance, in the same data set, probably that's a good experiment for you to use, which is that. Notice that we have two types of discrepancies here, right? So we have this horizontal line and we have this one blue thing which is above the horizontal. So what if you remove the one or two of the bottom green things which come below the line, these are also wrong. Right? So the blue thing above the line was wrong in some sense. The green triangles below the line are also wrong. So instead of removing the widest blue, you can try to remove the narrowest green or the two narrowest greens or something and see what happens. So you just have to copy paste this code and just change the condition and all the visualization. You can just run it as it is and see, I have not done it, but I think you might get something different. So the predict outcome is not predictable. I cannot guarantee you that one or the other will produce a different output, but it can happen. And this is generally something which one has to be mindful of when you're dealing with decision. One reason that this is important is remember that we talked about this cross validation. So what do we do in cross validation or even in normal validation? In normal validation or in cross validation. In either case,\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"So what you're really asking at some higher level is that machine learning approach. So, remember, we have seen only decision trees, but we are going to see many models. So there are clearly many ways to build models. And the reason, whenever you see that there are many ways to do something, it's only because there is no guarantee that a given way is the best one. So more or less what you want to validate is, I have, say, seven different strategies to solve this problem using machine learning. And I want to check which one of them is good. So I need to compare. So once I compare, then I'm stuck with whatever I have. It may be that this gives me only 70% accuracy or some such metric, but the other models give me even less. So it's more really a comparative statement to start with, which is, is this method actually giving me something reasonable? And is it giving me better something than something else? So, for this one strategy is so called cross validation. So what you do is you take some fraction, say, in this case, it is one fifth. So 20%. So you take your data and you leave out. So you randomize it into five groups, which preserve, in some sense, the random behavior, so they partition it five ways. So if I leave out any one of these groups, it's as though I have taken the remaining 80% for training and this 20% for test. But because I don't want to actually think of it this way, I will actually keep doing this for each one. So I will leave out the first 20%, then I will leave out the second 20%. So I will do this. If I have like k chunks, I will do this k times in each chunk. Each model I will build using all but one of these chunks. So this then will solve this problem of many problems. It will solve the problem where you don't have enough data to begin with to afford to not look at some of it, because the interesting things may be in that sum of it. Now, what will you do with this whole thing? Well, you have five. Typically, each of these models are going to\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"to not look at some of it, because the interesting things may be in that sum of it. Now, what will you do with this whole thing? Well, you have five. Typically, each of these models are going to be different because we saw with the decision tree, it's kind of the questions that you ask depend on the distribution of the different roles. And if the distribution changes slightly, one attribute might get better entropy than another attribute. And you will choose that question first. So the shape of the tree might vary a lot between these, so you may not get the same. Okay, so the question about cross validation, cross validation. What I'm saying is that you do not make a hard and fast choice once and for all to split your data this way. So you're taking your data and you're saying, hey, first let me leave out this 20% and build one model. So I am building multiple models. I'm building m one, m two, m three, m four, and m five. Each of these models is built using 80% of the test data, but a different 80%. So each of them leaves out some 20%. So I'm building five different models and comparing what I see in all these five. So the first thing is that if this approach is good, then in general, these five models should behave in a similar way with respect to the outcome. So if they all behave similarly, then I can decide what to do. So there are two strategies. I can, I can somehow combine these by saying, build a composite model, or I can say, okay, this strategy is working for me. So this approach that I have chosen seems to be a good way to build a model for this data. So now let me go back and take the 100% data and build from this whole thing, build a new model M, which will be hopefully better than any of these. M one, m two, m three, m four, m five. But I built those five models to allow myself to know whether this strategy I'm using is good or not. So once I've decided I have a good strategy, then I can do something to make the best possible use of the data to build\", metadata={'source': '/content/audio_4.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sub_docs_4"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T14:54:28.741356Z",
          "iopub.execute_input": "2024-04-03T14:54:28.741737Z",
          "iopub.status.idle": "2024-04-03T14:54:28.748385Z",
          "shell.execute_reply.started": "2024-04-03T14:54:28.741700Z",
          "shell.execute_reply": "2024-04-03T14:54:28.747510Z"
        },
        "trusted": true,
        "id": "MuixB7BtL7Yv",
        "outputId": "9347d52d-5dac-439e-fb77-cb0272747717",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='is important is remember that we talked about this cross validation. So what do we do in cross validation or even in normal validation? In normal validation or in cross validation. In either case, you pull out some random data, set data points from your input and build a tree on the rest. Now the question is, the model that you built from the rest, is it a stable structure in the sense that is it sensitive to how you chose your training and test data? And this example suggests that you are likely to get different models from different subsets of training data. So you then have to worry about what is your final model going to be. So if you test it on a particular test set, then maybe if you', metadata={'doc_id': 'd5694b4c-17c2-4195-ae7d-51d230e3a790', 'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"or something and see what happens. So you just have to copy paste this code and just change the condition and all the visualization. You can just run it as it is and see, I have not done it, but I think you might get something different. So the predict outcome is not predictable. I cannot guarantee you that one or the other will produce a different output, but it can happen. And this is generally something which one has to be mindful of when you're dealing with decision. One reason that this is important is remember that we talked about this cross validation. So what do we do in cross validation or even in normal validation? In normal validation or in cross validation. In either case,\", metadata={'doc_id': 'e677096d-ec2e-4ed9-b2e6-cf16987e6b3e', 'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"me only 70% accuracy or some such metric, but the other models give me even less. So it's more really a comparative statement to start with, which is, is this method actually giving me something reasonable? And is it giving me better something than something else? So, for this one strategy is so called cross validation. So what you do is you take some fraction, say, in this case, it is one fifth. So 20%. So you take your data and you leave out. So you randomize it into five groups, which preserve, in some sense, the random behavior, so they partition it five ways. So if I leave out any one of these groups, it's as though I have taken the remaining 80% for training and this 20% for test. But\", metadata={'doc_id': '5875ffd2-ff6c-48e7-9804-08e1e61658a6', 'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"to not look at some of it, because the interesting things may be in that sum of it. Now, what will you do with this whole thing? Well, you have five. Typically, each of these models are going to be different because we saw with the decision tree, it's kind of the questions that you ask depend on the distribution of the different roles. And if the distribution changes slightly, one attribute might get better entropy than another attribute. And you will choose that question first. So the shape of the tree might vary a lot between these, so you may not get the same. Okay, so the question about cross validation, cross validation. What I'm saying is that you do not make a hard and fast choice\", metadata={'doc_id': 'd3e3abf8-5bea-4516-ab3c-c7c1afc79ab8', 'source': '/content/audio_4.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 5"
      ],
      "metadata": {
        "id": "-svmIHR_Y9bi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#query_1=\" What are some challenges associated with data collection?\"\n",
        "#query_2=\" What are the advantages of the Apriori algorithm?\"\n",
        "#query_3=\" What distinguishes training data in supervised learning, and what is its purpose? \"\n",
        "#query_4=\" What is cross-validation?\"\n",
        "query_5=\"What is the process of building the decision tree classifier, and how is it trained on the dataset?\"\n",
        "\n",
        "sub_docs_5 = vectorstore.similarity_search(query_5)\n",
        "retrieved_docs_5 = retriever.get_relevant_documents(query_5)\n",
        "retrieved_docs_5"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T14:54:43.314973Z",
          "iopub.execute_input": "2024-04-03T14:54:43.315343Z",
          "iopub.status.idle": "2024-04-03T14:54:43.946157Z",
          "shell.execute_reply.started": "2024-04-03T14:54:43.315313Z",
          "shell.execute_reply": "2024-04-03T14:54:43.945170Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTasMU4WL7Yv",
        "outputId": "b0ce8312-1b50-4fad-d074-d01a826abc52"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"build a decision tree normally until we either achieve some pure leaf nodes where the classification is uniform, or we run out of attributes. Now this is a numerical case, so we are not going to run out of attributes because we are going to run out eventually when we run out of data points. But we will end up with these very long slicing, the width between 1.5 and 1.71.5 and 1.6 and so on. So what you can do is you can tell the decision C classifier not to expand beyond a certain depth. And here it has fixed it to be depth too. So we will come back to this at a later lecture, which is, this is related to this generalization question. That is, if you build a very detailed tree that asks a lot of questions, then you start asking questions which are very specific to the training data and you end up with what is called an overfitted model, a model which is following your training data too closely and is therefore picking up some peculiarities which don't necessarily exist within. So one of the things we mentioned in passing was that we like short trees for two reasons. One is because they are easier to explain. The second thing, which I claim without any justification, is that they generalize better. So here, priority, they are saying, let us not construct deep trees. So whatever tree we construct, we are going to stop it when it reaches. So it's a two step process, right? So what you do is you first say what are the parameters for the decision tree classifier? So he says, I want a decision tree classifier to be set up with this random state 42 and which will not grow to more than depth two. And then I have to actually construct the classifier for a particular data set. So that's the next thing. So I use this fit function, right? In some sense this creates a decision tree object with certain operating parameters, and then you pass it the training data in the form of the input and the output x and y, and say fit. So the important thing to see is that this is all you\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"and with 9% or whatever probability is going to be the minority class. And that's just that. 49 by 54 and five by 54. That's okay, good. So it's not really a probability, it's just a frequency count. If I come there, then 49 out of 54 times I have observed this and five out of 54 times I've observed that. And then of course it has to give an answer and that's what basically the prediction is. So this is really the other side of it. So fit was a function that we saw earlier. So fit created the model. We passed it an x and a Y. So X was a table of inputs and Y was a vector of output predictions which were learned. So that is the training data, and it produced a model which we visualized there. Now, when I want to actually run this model, what I have to do is I have to give it an input and ask it what do you think is the output? So things, effectively, the output will be ambiguous because there will always be some degree of mixing between the classes. So what it will do is it'll first compute according to what. In this decision tree, it will basically follow the input to the leaf. And in that leaf it will look at the distribution of nodes and it will assign a number to each one so that they are normalized to add up to one. And after I normalize and add up to one, I take the highest probability one as the answer. So if you now insist that this tree should tell you one of the three classes is obviously going to predict the class with the highest probability. So it gives me one. The reason it gives me one is that the three classes are encoded as zero, one, two. So one is really the second class. The reason it's an array is because normally I predict not one value, but I predict a collection of values. So I pass it an array of inputs and I get back an array of outputs. So here I pass it a trivial array. So I pass it an array. There's an outer square bracket and the inner thing is an array of inputs, right? It's a two dimensional object. So normally I would two column\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"so now this is the real part. So, so Scikitlearn has some built in data sets and one of them is this iris library. So the way to get the iris library into your data set is to import this function called load iris. And Scikitlearn has a bunch of models predefined. So what we have been looking at is decision trees as classifiers. So we are going to import from the subset of models of form tree. We are going to import this model called decision tree classifier. So the first thing that we do is we actually load this data set. So we call this load iris function, and then we are going to remember, the iris data set had two measures. It had the petal and the sepal, length and width. So it had four quantities. But we decided we will only use two quantities. So this is what this is doing. It's taking this iris data set and it is looking at essentially columns, indices. Two and three are the siple and two, three are the petal. So it's taking all the rows and it's taking the third and fourth columns. So we're throwing away two columns. So we are making it, instead of a four column array, we are making it into a two column array. And in this data. So when you load the data set, it automatically produces these two subparts, iris data and iris target. So target is the classification variable. Remember, in this case, the classification variable is a three way thing, right? There are three types of irises that we saw, so it will be essentially a number, zero, one. And now to get this decision tree classifier, we have to take this function that we have imported. So we say take a decision tree classifier. We have to provide it with, again some random state because it uses some randomization inside it. And now this is something which we have not talked about. So we discussed that we will build a decision tree normally until we either achieve some pure leaf nodes where the classification is uniform, or we run out of attributes. Now this is a numerical case, so we are not going to run\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"So last time we started with supervised learning in earnest, and we were looking at this decision tree model. So, in a decision tree model, you have essentially a table in which the columns are the. So each item, a row in the table and one special column denotes the class that you are trying to classify. And we assumed it's a binary classification. So the class is zero, one, or yes, no, or whatever you want to interpret. The two categories are. So the algorithm was very simple. At a high level, what you want to do is choose an attribute based on the answer to the question that you're asking about that attribute. The data splits, and then you take each partition, ask another question. So that's the thing. And you want to come down and keep asking questions until either you have reached, like these situations here, a partition in which all the values of the class variable are the same. So you have a uniform class level or you have run out of questions, because we can only ask in any path one question per attribute. And if in case we have the second case where we run out of questions, but we have not reached a so called uniform node, then we just use the majority as a prediction. So we will see an example of this later as we go. Yeah, so what we also said was that, as we can see here, we have two different trees, and the trees are not the same shape. And we argued that a smaller tree is probably a more desirable one. It's more explainable. And we also claim, without at the moment justifying, that it also has a better generalization property. That is, it will move from the specific training data to the unknown data in a better way. So how do we build small trees? Well, computationally, we said, I claimed at least that it was infeasible to build the smallest tree. So you would have to do a search through all possible trees. So instead we wanted to use some heuristic. So the main question that we have to ask is, which attribute do we query next at any stage. So our goal\", metadata={'source': '/content/audio_4.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sub_docs_5"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T14:54:56.017950Z",
          "iopub.execute_input": "2024-04-03T14:54:56.018940Z",
          "iopub.status.idle": "2024-04-03T14:54:56.025286Z",
          "shell.execute_reply.started": "2024-04-03T14:54:56.018906Z",
          "shell.execute_reply": "2024-04-03T14:54:56.024212Z"
        },
        "trusted": true,
        "id": "3oqeSnpiL7Yw",
        "outputId": "93b56d11-4b7b-450f-e856-327aa4341776",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"we construct, we are going to stop it when it reaches. So it's a two step process, right? So what you do is you first say what are the parameters for the decision tree classifier? So he says, I want a decision tree classifier to be set up with this random state 42 and which will not grow to more than depth two. And then I have to actually construct the classifier for a particular data set. So that's the next thing. So I use this fit function, right? In some sense this creates a decision tree object with certain operating parameters, and then you pass it the training data in the form of the input and the output x and y, and say fit. So the important thing to see is that this is all you\", metadata={'doc_id': '78804e6e-6d9c-43ed-8623-4dce5dab695e', 'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"it produced a model which we visualized there. Now, when I want to actually run this model, what I have to do is I have to give it an input and ask it what do you think is the output? So things, effectively, the output will be ambiguous because there will always be some degree of mixing between the classes. So what it will do is it'll first compute according to what. In this decision tree, it will basically follow the input to the leaf. And in that leaf it will look at the distribution of nodes and it will assign a number to each one so that they are normalized to add up to one. And after I normalize and add up to one, I take the highest probability one as the answer. So if you now insist\", metadata={'doc_id': 'f710a047-11ea-4cf6-bc20-de7545c61ee7', 'source': '/content/audio_5.txt'}),\n",
              " Document(page_content='in this case, the classification variable is a three way thing, right? There are three types of irises that we saw, so it will be essentially a number, zero, one. And now to get this decision tree classifier, we have to take this function that we have imported. So we say take a decision tree classifier. We have to provide it with, again some random state because it uses some randomization inside it. And now this is something which we have not talked about. So we discussed that we will build a decision tree normally until we either achieve some pure leaf nodes where the classification is uniform, or we run out of attributes. Now this is a numerical case, so we are not going to run', metadata={'doc_id': '25974fd7-810d-4279-97e0-dac2962bc7fa', 'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"So last time we started with supervised learning in earnest, and we were looking at this decision tree model. So, in a decision tree model, you have essentially a table in which the columns are the. So each item, a row in the table and one special column denotes the class that you are trying to classify. And we assumed it's a binary classification. So the class is zero, one, or yes, no, or whatever you want to interpret. The two categories are. So the algorithm was very simple. At a high level, what you want to do is choose an attribute based on the answer to the question that you're asking about that attribute. The data splits, and then you take each partition, ask another question. So\", metadata={'doc_id': '43e1d02c-3c77-413f-9cae-fefdfe7bd193', 'source': '/content/audio_4.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 6"
      ],
      "metadata": {
        "id": "XK5dZy6MjeI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_6=\"What is the challenge associated with supervised learning?\"\n",
        "\n",
        "sub_docs_6 = vectorstore.similarity_search(query_6)\n",
        "retrieved_docs_6 = retriever.get_relevant_documents(query_6)\n",
        "retrieved_docs_6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjX_lVTqjdZV",
        "outputId": "5a01ae6f-dbc2-44cf-8f1e-ae674d18dd1b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"to the way the data is available. So what we are going to be doing is agnostic to the domain. So we are not going to believe anything or assume anything about the domain, except that we will want to validate at the end somehow that what we are doing is right. But in practice what you're saying makes sense that really to do supervised learning, it's much more realistic to do it in collaboration with somebody who understands a domain rather than just doing it blindly. Because also when you do it blindly, you might come up with solutions which make no sense. You might actually identify features which contribute to a decision purely by statistical coincidence, and no human would actually do that. So there's another issue which comes up of interpretability or explainability of your answers. So there are various factors which get into this. It's only not only accuracy which matters, but also whether your model is sensible in something, so that you can justify it. So these will all come as we go along. So let's proceed with this. So the first thing is that because the choice of attribute to question will determine the structure of the tree. So here are two different trees. So this is the tree we started with, and in a sense this tree, if you remember that table, consists of asking the first column first. So the first column in our table was age. So we asked age, and then maybe we adapted our question based on age, whereas here we have directly jumped to own a house. The question to ask is which of these trees intuitively is better secondary? What is the definition of better over here, a shorter tree or a tree which predicts better? So here both trees are predicting. Exactly. So there is no mistake, at least with respect to the training data. Of course we have not seen what it does with the unknown data, but if you are just looking at its behavior on the training data, all of these things reach nodes in which I don't have any confusion, any majority to choose. So in that\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"I have to decide between which bands it is, green, and which bands it is, this yellow color. So the peril of supervised unsupervised learning is that you don't know that the step that you are taking in advance, you don't know is going to be good or bad because it's unsupervised. You're just taking a guess in some sense, but then you have to have some way of evaluating whether the guess was good or not. And then going back and doing it again may be a different. So this is kind of a quick overview, a summary of the main things that we will be looking at. So, supervised learning and unsupervised learning. So we are looking either to build a predictive model, which is broadly a classification model, or a numerical prediction model. As we said, in case you're trying to predict a number or category, the category is more like a classification problem, right? I give you something and I want you to classify it as yes, no, good, bad, or we have unsupervised learning where we are looking for some kind of structure, and usually this is in the form of clustering, but we also have to deal with these outliers, as I said. And then sometimes you want to simplify the problem by knocking off certain dimensions or unrolling the object or so. So if you look at the popular press and the popular literature, all the focus of machine learning is actually on the first one.\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"have their income, you have their previous year's income, you have some, whether they own a house, what is the value of the house, and so on. So you have this high dimensional data in which you're trying to do something, and very often it's very hard to work because it just multiplies out into too many possibilities. So one of the things that one can do sometimes is to actually reduce the dimensions. And sometimes when you reduce the dimension, the problem becomes easier. So here is an example. So you have this kind of, imagine this is a kind of a carpet, the thing on the right, like an exercise mat or whatever, you take this carpet, which is colored like this, and then you roll it up. Now, if I give you this rolled up version on the left, then if I ask you to decide, given a point, whether it's green or yellow, it's kind of difficult, because there is a three dimensional twisting of this data, which doesn't make the picture very obvious. Whereas if I'm able to unroll it and produce this two dimensional thing, then there is a very simple line which separates the green from the other. So this is a situation where by reducing the dimension, somehow, you make the problem easier to tackle. But this is an unsupervised thing. Reducing the dimension is something that you do blindly in the hope that it gets you a benefit. So I can take another example of the same type, and I can start with something which actually, in the original thing, is separated by a simple plane, right? So I have this plane here which separates the green from the yellow. But then, believing that it'll be better to work in two dimensions, I actually unroll it. And then I find I have to deal with these multiple bands. So now it's not so obvious anymore because I do have lines, but I have many of them, and I have to decide between which bands it is, green, and which bands it is, this yellow color. So the peril of supervised unsupervised learning is that you don't know that the step that you are taking\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"is itself a kind of computationally intensive task. How do you provide a lot of well trained examples? Because the assumption is that the labels that these training samples have are somehow correct so they can be trusted. Because if you can't trust those labels, and the model that you build will obviously not work. So if you want to write generalization of the labels that people have given you, you have to trust those labels. So that means somebody reasonably conversant with the domain has to be willing to put in the time to label it. Now, if it happens, as I said, because historically you're collecting data, it's fine. But more and more, we are finding that this is not the only source of data. A lot of data now actually comes out of automatic things like we have systems which are producing. For example, if you run a computerized device like a network switch or something like that, all these things generate a lot of diagnostic data. Even things like cars and planes and vehicles, which run with a lot of electronic components, generate a lot of diagnostic data. That's why whenever there's a crash, for instance, people are looking for the same as black box. Now, if you're trying to build a model based on that, then who is going to sit through and go through all this diagnostic data and tell you that, oh, this part of the data suggests that there's a fault here, and that part of the data suggests that there's a fault there.\", metadata={'source': '/content/audio_1.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 7"
      ],
      "metadata": {
        "id": "WrWR6zzGjjS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_7=\"What considerations should businesses keep in mind when applying market basket analysis to understand customer behavior?\"\n",
        "\n",
        "sub_docs_7 = vectorstore.similarity_search(query_7)\n",
        "retrieved_docs_7 = retriever.get_relevant_documents(query_7)\n",
        "retrieved_docs_7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfLs--TtjjTA",
        "outputId": "8af7a48f-b1df-42ff-822b-3d13f73f06c0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"to say, as a rule, consumers who do this also do that and so on. So these are all refinements. And there are many situations where these are important and interesting, but this model as such, this kind of market basket analysis approach is very naive. And so that's why these things will come up in different contexts. But we are not going to explore it anymore in this context, because in this context, the kind of model that you build, this association rule model is rather simplistic. So we are going to look at more sophisticated models as we go along. And there you can ask the same question, but it'll take a slightly different format. Does this also include tendencies like, if there is one person who tends to buy all his groceries once a month, then they will have a bigger basket size. But for people who buy their groceries, say like per week or every other day, they will have smaller basket size, but their transactions would be spread out. So that will be treated as a different transaction every time. But for somebody who's buying everything together, doesn't that give rise to a lot of. Yeah, so there are all these situations, I agree, which are not directly addressed in this. There are many different variations of this that you could ask. So there's no doubt about that. So some of them people have looked at because they have kind of natural solutions in this. Some of them maybe you cannot do. So that's another thing about this whole model building thing is that the same model may or may not be capable of tackling every different question that you ask. So this model, some of these things maybe you can segregate and answer. But if, as you're saying, you want to compare behaviors across customers of different types, you cannot, as you said, sensibly aggregate them into a single market basket model because they all have different profiles. So you will have to, in the example that you gave, you would have to first separate out the data for these, say, the people who\", metadata={'source': '/content/audio_2.txt'}),\n",
              " Document(page_content=\"Maybe it is true that at certain times, people who buy diapers also buy beer, but it at least gave some confidence to the idea that you can use this kind of information to make decisions. So from that perspective, it possibly had a role to play. But anyway, so these are the origins of this problem, and it's called market basket analysis because it's really talking in this context about shopping baskets. I have a shopping basket when I reach the counter to pay. So question is, what is in that basket? If it has x, does it also have y? So this is a very literal interpretation of markets and baskets. But the notion of markets and baskets can be a little bit more abstract. So, for instance, if you are thinking about, especially nowadays, given the number of elearning platforms, there's a lot of emphasis on trying to structure topics in such a way that students who find difficulty, you can identify why they are facing difficulties. So you want to think of these concepts that you're teaching as these items. And basket is now a set of concepts. And if you look at a concept which is difficult to learn, maybe there is a connection to another concept which is also difficult to learn. So you might want to group this. So people who misunderstand a also misunderstand b. And therefore maybe the problems has to start by fixing b. And another place where this is used is in kind of similarities of documents. So if two documents share words, then you can think they are similar. So you can think of documents. Or if two words appear rather in many documents, you can think they are similar. So if you say that wherever x appears, y also appears, then it may mean that x and y are actually related concepts. So this is the context. So the real question is this, people who buy x also tend to buy y. So this is what we want to formalize and try to do a preliminary round of calculations to see how one might determine. So everything, as we said, is going to be done by first abstracting out the\", metadata={'source': '/content/audio_1.txt'}),\n",
              " Document(page_content=\"hand side of the rule is always a singleton, and it corresponds to a special type of item which is a category, in this case a topic. So when you have a table like this, the rows may have different number of entries. Or if you want to think about it as a table, then some columns may be blank. So not all columns are filled, but there's one last column which is filled. And now you want to now construct these rules corresponding to the first k minus one columns, implying the last column. And this now becomes something where these rules that you get out of this give you a kind of classifier. So this is a kind of a simplistic way of doing supervised learning. So that kind of brings me to the end of this basic treatment of classes, this market basket analysis. So what you're really looking for is correlations between sets of items across transactions, which we formalize in terms of these association groups. X implies y. And what we saw is that as such, combinatorially, this is a very complicated thing to calculate, given the sizes of the sets involved. But we can use this appropriate principle both to identify the sets which are interesting, those which are frequent enough to be considered, and also to decompose them into rules. So you use the approoriate twice, once to find the frequent z, and second to decompose each frequent z into possible x implies y. And using this, we can build our first simple supervised learning model. So these rules now become a very simple way of prediction. Now, if you give me a document, I will look at what words are there in the document in this previous example. And if it contains the word student and school, I will predict education. If it contains game and team, I'll predict sports.\", metadata={'source': '/content/audio_2.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 8"
      ],
      "metadata": {
        "id": "bfnVogJJjlwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_8=\"How does the genie index contribute to decision tree construction, and why is it favored in practice, particularly in Python libraries?\"\n",
        "\n",
        "sub_docs_8 = vectorstore.similarity_search(query_8)\n",
        "retrieved_docs_8 = retriever.get_relevant_documents(query_8)\n",
        "retrieved_docs_8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfoMCTdZjlwX",
        "outputId": "5181c36d-f647-46c1-bf98-773483a00853"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"squares of numbers. So genie index is actually easier to compute. And if you look at, as we will see, libraries in Python and all that, typically they use genie index. So although there is a very famous system built using entropy, which was one of the early decision tree systems, the ones that we will practically encounter in Python, libraries actually use the genie index. Basically. I'll stop with this right now. So what we are saying now is that we want to choose the next question to evaluate the next question to ask. So for the next question to ask, we have to say something about the quality of the impurity before and after. So if I just look at the raw ratio of the impurity, I have a motion of just that impurity percentage. But that gives me this linear relationship, which apparently is not that good in practice. So I want a nonlinear way of giving some impurity value to this ratio of yes or no, and then we can use either this entropy idea or this genie index, and it gives me a sharper curve, and this sharper practice turns out to be better. So that's how you build a decision tree. You build a decision tree by applying one of these two nonlinear things. If you come up to the third thing, it's also fine. It's just that these two happen to be standard things. They are both borrowed from other areas where they have a long and established history. So, as I said, genie index comes from economics and entropy comes from information. So we'll stop here. So, any questions? It okay, if there are no questions, then we'll continue with this discussion of decision trees on Thursday when we meet next.\", metadata={'source': '/content/audio_3.txt'}),\n",
              " Document(page_content=\"ratio. It's not just the information gain alone, which is what we had initially. It's information gain divided by the entropy of the attribute. So is this clear what role this is playing? We are just saying that if you have a scattered attribute, then you discount the gain that you get from it by the scattering value. So more scattered attributes will, the net effect will be less because of the denominator. So in terms of our heuristics, what we said is that nonlinear functions work better in practice. And the genie index is in fact the one that is used in most libraries. But we should not blindly use this because of this problem of these scattered attributes. So we will actually use this ratio of business to correct. So we have this information. So now let's get back to this question that we initially observed. That is that something like this age, it would be much more natural if this age were described in terms of numbers. So somehow this data has been arranged so that we get this data in terms of categories. But who does the arranging? So how do we realize that in this context? Sir, I have a question, sir. In this loan example, if we add attribute like is an indian citizen or not, then the attribute has only one class, that is Indians. Suppose a bank has, in this case, the impurity is low because we have only one attribute, but it does not give us. What will happen is if you use such a category, then it will split. As what? Because if everybody is indian, for example. Yes, sir. Then after I text, if I ask the question is the person indian? Then I will get two groups. One group will have zero rows and one group will have the same set. Yes, sir. So therefore the same set will have the same entropy, right. And so the entropy will actually not change. That question will be totally redundant. Right. So I will get the one times the entropy of the table plus zero times the entropy of zero, which is just the entropy of the table that I started with. So the difference\", metadata={'source': '/content/audio_4.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 9"
      ],
      "metadata": {
        "id": "DYYd0a-LjoJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_9=\"What is the harmonic mean, and how does it enable the combination of precision and recall into a single metric?\"\n",
        "\n",
        "sub_docs_9 = vectorstore.similarity_search(query_9)\n",
        "retrieved_docs_9 = retriever.get_relevant_documents(query_9)\n",
        "retrieved_docs_9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wuU5NmMjoJa",
        "outputId": "2daca168-d7d6-4ed7-c63a-6af6f7e49b15"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"If you want the other one go up, the first one will come down. So now I just mentioned that these are not the only things so people also talk about. So there is, for instance, the corresponding thing to precision for the negative part is specificity. Right? So what is the ratio of correct negative results compared to all negative results as a true negative? So that's with respect to this column, then accuracy, of course, is just saying how many of the correct things are there with respect to the whole thing. So that also can be measured easily from this. And then there's something called thread score and so on. So I'm not going to talk about it. And just for completeness, I will mention that sometimes people want to say that we want a single number. It's not good to give me a precision separately and a recall separately and let me choose, I want a single number. There is this thing called harmonic mean. So the harmonic mean is one upon p plus one upon R. So remember, p is precision one and it's one upon that. So it's the reciprocal of the mean of the reciprocals. So if you work this out, you get two PR on two p plus. So this is sometimes called the f score. Right? So this is a way of combining the precision score and the recall score into a single number. But as I said, in most cases, it really depends on the application. So either you want something based on the context which has higher precision or higher recall, and you choose. So these are some of the ways in which we can actually. So, remember, our starting point was that we wanted to evaluate classifiers. So we said first we have a strategy for evaluating classifiers, so we can test them by using this test set and so on. But what is it that you want to measure on the test set? So that we have this wide variety of things, but most commonly this precision and recall are the ones that people are interested in, and they will choose which one to emphasize. Depending on the context. Depending on the context, you\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"of things, but most commonly this precision and recall are the ones that people are interested in, and they will choose which one to emphasize. Depending on the context. Depending on the context, you may want precision predominant. You may want recall to be predominant. So there, it really becomes a domain specific. So I'll stop there. Any questions, sir? Like, given a precision or recall value, what we need to change in the algorithm so that we could come to the certain value of precision and recall. So that is a harder question. Right. So what is it that you need to change? So that's a very difficult question. All you can say is that given what we are doing now, this is what, then it becomes really an experimental issue. So this is the hardest question in machine learning, which is, how do I improve the quality of the classifier? So there is no answer to that question, which is obvious. So you have to start experimenting. You have to do something. So there is something called feature engineering. You might want to come out with some combination of features. So some of you may have heard of this thing, for example, which is used in medicine called body mass index. So, body mass index is some combination of some formula involving height and weight. So you might have a data set in which you have something like height and weight usually classify using height and weight. It doesn't work. But if you combine it in this strange way using body mass index, then you actually get a better class. So this is an example of what is called feature engineering. You make new features out of the old one, or you might need to change your model, maybe decision tree is not good, you need to use another model. So these are all very open questions, and I don't think there is any. If there were a clear answer to that, then life would be a lot simpler. But unfortunately, that is the real challenge of machine learning. What is it? If I am given a certain measure of performance, what is it\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"You don't mind losing out on some good candidates for whatever reason. So depending on the nature of your screening, you might want to do again, similarly, in medical diagnosis, if you want to make everybody immunized against something or if you want to give them. So if you have a disease for which there is a simple medication, then you may not be very specific about the diagnosis. Anybody who comes with certain symptoms, this is more or less what's happening now with this omicron, right? Anybody who has a cough, cold and fever is assumed to have omicron, and you just tell them to isolate. So this is something which is high recall. You want to catch everybody who's sick and make them less of a risk to people around them. On the other hand, if it's some really critical diagnosis, it's like pancreatic cancer or something where you're going to die in six months or three months. You don't want to tell somebody that unless you're very sure, because that whole person's life will be and their family's life will be uprooted by this. So then you will have a second opinion, third opinion before you actually make the decision. So you will not just go on an initial suspicion that, oh, this looks like pancreatic cancer, and blurt it out to the patient. You'll make them both. So in many situations, it's not clear, but what is generally clear is that these two things are usually not aligned. You cannot normally do both together. You cannot have normally. If you have to trade off either you want recall or you want precision. And one will go up.\", metadata={'source': '/content/audio_4.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 10"
      ],
      "metadata": {
        "id": "X_1vrGm9jrJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_10=\"What are the implications of choosing a large or small learning rate?\"\n",
        "\n",
        "sub_docs_10 = vectorstore.similarity_search(query_10)\n",
        "retrieved_docs_10 = retriever.get_relevant_documents(query_10)\n",
        "retrieved_docs_10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j855JPegjrJu",
        "outputId": "c9ae53ea-8f4a-4d48-d04b-ca063c760dcc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"loss and all. But you don't do it predictably, you do it randomly. So you pick up a random data point, make an adjustment based on the value at that data point and so on. So this is called stochastic gradient descent. And another way in between, which we will see later, is you take small batches. So here we batch gradient descent. You take the full. So if you have, say, 1 million inputs, you will evaluate your function on all 1 million inputs before you make one step of adjustment. In stochastic gradient descent, you will look at one input and then do it. Now, the problem with that is that that one input that you get might be one, which is kind of very extreme, and so it might give you a wild swing, and then you might oscillate a lot more. So it might take you longer to converge, but faster to compute. So there's a trade off between doing each iteration. You are making a lot of adjustments fast, but those adjustments are less controlled. The other way in between is take small batches. If you have 1 million things, you break it up into 1000 batches. So you do 1000 points. So you get some averaging out of the behavior of the function, make an adjustment, then you do another 1000, make an adjustment and so on. So all these things are actually used, we will see later in neural networks and all that. But in this linear prediction also, that's where they arise. I mean, that's the origin of this. So this is, of course, as you have all probably read. So this strategy is conventionally called linear regression, but it's basically this iterative linear prediction, which you can do theoretically by a matrix operation, but computationally, it's more feasible to do it in this. So I'll stop here, and we'll continue next time to discuss why we are using some square error and also what to do when you have nonlinear things, how to deal with nonlinear approximations. Having done decision trees, I think at the next class, at the end of the class, the last 15 have a small moodle quiz\", metadata={'source': '/content/audio_5.txt'}),\n",
              " Document(page_content=\"way to fix this apparent bug in our choice of attributes, where we blindly take the best one. So these increases and increases. So what we do really is we in some sense normalize. So what we want, we want to take. So the numerator is the one we were calculating before. That is, if I choose attribute a, how does the data split in terms of the categories? So that's the entropy or genie index of the old one that is in each group, how many s's, how many nodes, that's what display. But the denominator is telling me how good or bad a itself is. What is the scattering of a? So the argument here that I was trying to make is that as a gets more scattered, there are more values and the ratio of each value is smaller, then that denominator will become larger. So whatever gain you get on the top. So remember the problem was that these give us high gain on the top, but we are dividing that gain by the scattering of that attribute. So if we have high gain divided by high scattering, then the net normalized effect is lower. So you might choose an attribute which actually gives you a lower information gain overall, because the attribute itself is less static. And that's the thing that we want to achieve. Just to emphasize that when you have a machine learning algorithm, see generally, what is a machine learning algorithm doing? It's trying to describe the shape of the model based on the training data. So in this case it's literally the shape, because you have many possible trees you could construct, and the actual tree you construct depends on the data. And because you don't have any preconceptions about the data, you have to make sure that if there are these kind of patterns which disturb your algorithm, you will be able to counteract. So that's the purpose of this information gain ratio. It's not just the information gain alone, which is what we had initially. It's information gain divided by the entropy of the attribute. So is this clear what role this is playing? We are just\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"data is both the entire data set that you are given to start with. And then it is the specific choice that you make about how to split this data into a large part, which you will use for building the model, and a small part that you will reserve for validating the model. So depending on how much data you have, the usual practice is about 20% to 30% of it you keep aside. So you use about 70% to 80% of the data that you are given to build a model, and you keep a random subset of 20% to 30% away from the model building process. So it is unknown as far as the model or unknown data. It's the same as getting new data from the real world in some sense, and you can use it to validate. So, of course, this is a little bit of a disadvantage, because one of the features I mentioned right at the beginning is that typically machine learning expects that the model improves in quality with the amount of training data you give it. So you are in some sense artificially, or because you have no choice, more or less. You are building a suboptimal model, you have 100% data, but you are forcing yourself to use only 70% to 80% of the data available to build the model, because you need to validate it against some unknown data. So, one other strategy where you don't want to, I mean, one of the other disadvantages of doing this is that maybe that there are some minority anomalies in this data, and the choice of test data that you have made might have hidden all these anomalies, so you never see them, and maybe it's important to see them. So there may be many situations, or maybe you just don't have enough data as a whole to build a good model by only looking at 80%. So another strategy is to systematically do this with different subsets.\", metadata={'source': '/content/audio_4.txt'}),\n",
              " Document(page_content=\"you to take a battery of tests. So you come back, I'm sure many of you have done this. So you get some medical report and you say, this is this. So you will have some, depending on how many tests you have done, you will have anything ranging from five to 15 to 20 numbers on that piece of paper which the lab has produced. Right? Now, if the doctor were to say, okay, looking at this piece of paper, I declare that you have XYZ condition. It will be very hard for you to expect to believe that. What the doctor will say is that, look at this number. Look at that number. So those two numbers are outside the range that I would normally expect, and they typically indicate something. And therefore, I believe you have. So you narrow down the suspects. So it's the same as saying that you have a complex explanation which says, test whatever. Number one is this and number two is this, and number three is this, and number 15 is this, and therefore. Or you say, number two is this and number seven is that. And therefore, in any context, generally, a smaller model is usually better, but everything is. And this is a common feature of anything we do, machine learning. Nothing is. You cannot say it universally, as I said, like newtonian mechanics and quantum mechanics, you could always have a situation where the more complicated model is required. So, yes, certainly small trees are more explainable. It'll turn out that small trees also solve the other problem that we mentioned earlier about generalization. So, question is whether or not this model is better for the data you have not seen compared to the data that you are training on. So is it too specific for the training data that you are building. So if you ask more questions, then you are really focusing on more individual eto synchronicity or complications in the data that you see. That might be an isolated case, and it may not appear in general, so maybe you should ignore it. I will show you later on an example of this. But the\", metadata={'source': '/content/audio_3.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install cohere"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T14:55:01.089370Z",
          "iopub.execute_input": "2024-04-03T14:55:01.090248Z",
          "iopub.status.idle": "2024-04-03T14:55:16.228663Z",
          "shell.execute_reply.started": "2024-04-03T14:55:01.090213Z",
          "shell.execute_reply": "2024-04-03T14:55:16.227517Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "id": "sF4HW1f1L7Yw",
        "outputId": "ea846203-a145-440a-e699-dcb219635464",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cohere\n",
            "  Downloading cohere-5.2.2-py3-none-any.whl (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.3/144.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
            "  Downloading fastavro-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx>=0.21.2 (from cohere)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.6.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.16.0,>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.15.2)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
            "  Downloading types_requests-2.31.0.20240406-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (4.10.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx>=0.21.2->cohere)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.2->cohere) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.2->cohere) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (2.0.7)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<0.16.0,>=0.15.2->cohere) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<0.16.0,>=0.15.2->cohere) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<0.16.0,>=0.15.2->cohere) (2023.6.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<0.16.0,>=0.15.2->cohere) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<0.16.0,>=0.15.2->cohere) (6.0.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<0.16.0,>=0.15.2->cohere) (23.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.2.0)\n",
            "Installing collected packages: types-requests, httpcore, fastavro, httpx, cohere\n",
            "Successfully installed cohere-5.2.2 fastavro-1.9.4 httpcore-1.0.5 httpx-0.27.0 types-requests-2.31.0.20240406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Co-here Re-ranking"
      ],
      "metadata": {
        "id": "Xand7DRVcgX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cohere\n",
        "co = cohere.Client('w8CnnlzVol2aZEiirZNLUs0onAqXUUYBZCw2Oj7g')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T14:57:02.730508Z",
          "iopub.execute_input": "2024-04-03T14:57:02.731273Z",
          "iopub.status.idle": "2024-04-03T14:57:03.104304Z",
          "shell.execute_reply.started": "2024-04-03T14:57:02.731241Z",
          "shell.execute_reply": "2024-04-03T14:57:03.103506Z"
        },
        "trusted": true,
        "id": "fCgmNQTGL7Yx"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 1"
      ],
      "metadata": {
        "id": "Ety4_Dgfktc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_query1=[]\n",
        "for i in range(len(retrieved_docs_1)):\n",
        "  retrieved_docs_query1.append(retrieved_docs_1[i].page_content)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T14:57:16.166250Z",
          "iopub.execute_input": "2024-04-03T14:57:16.167948Z",
          "iopub.status.idle": "2024-04-03T14:57:16.172962Z",
          "shell.execute_reply.started": "2024-04-03T14:57:16.167902Z",
          "shell.execute_reply": "2024-04-03T14:57:16.171893Z"
        },
        "trusted": true,
        "id": "Oj5eGFxVL7Yx"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = co.rerank(query=query_1, documents=retrieved_docs_query1, top_n=3, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned."
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T14:57:41.443099Z",
          "iopub.execute_input": "2024-04-03T14:57:41.443926Z",
          "iopub.status.idle": "2024-04-03T14:57:41.600292Z",
          "shell.execute_reply.started": "2024-04-03T14:57:41.443894Z",
          "shell.execute_reply": "2024-04-03T14:57:41.599175Z"
        },
        "trusted": true,
        "id": "GFrqPbGFL7Yx"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T14:57:47.792230Z",
          "iopub.execute_input": "2024-04-03T14:57:47.793139Z",
          "iopub.status.idle": "2024-04-03T14:57:47.798928Z",
          "shell.execute_reply.started": "2024-04-03T14:57:47.793104Z",
          "shell.execute_reply": "2024-04-03T14:57:47.798015Z"
        },
        "trusted": true,
        "id": "laY6TA7ML7Yy",
        "outputId": "1e25bd01-bf8f-40a9-8543-3852b243ec61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RerankResponse(id='e6421dd2-e3de-4e48-9277-857f3e322034', results=[RerankResponseResultsItem(document=None, index=0, relevance_score=0.9518632), RerankResponseResultsItem(document=None, index=2, relevance_score=0.50803304), RerankResponseResultsItem(document=None, index=1, relevance_score=0.31827322)], meta=ApiMeta(api_version=ApiMetaApiVersion(version='1', is_deprecated=None, is_experimental=None), billed_units=ApiMetaBilledUnits(input_tokens=None, output_tokens=None, search_units=1.0, classifications=None), warnings=None))"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(query_1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T14:57:54.106773Z",
          "iopub.execute_input": "2024-04-03T14:57:54.107136Z",
          "iopub.status.idle": "2024-04-03T14:57:54.112443Z",
          "shell.execute_reply.started": "2024-04-03T14:57:54.107108Z",
          "shell.execute_reply": "2024-04-03T14:57:54.111505Z"
        },
        "trusted": true,
        "id": "n0g82kFKL7Yy",
        "outputId": "99f34086-d432-4197-d0c7-e7e696b54740",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " What are some challenges associated with data collection?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#results = co.rerank(query=query_1, documents=retrieved_docs_query1, top_n=3, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
        "print(query_1,\"\\n\")\n",
        "for idx, r in enumerate(results.results):\n",
        "    print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
        "    print(f\"Source Document: {retrieved_docs_1[r.index].metadata}\")\n",
        "    print(f\"Document: {retrieved_docs_query1[r.index]}\")\n",
        "    print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T14:59:36.535441Z",
          "iopub.execute_input": "2024-04-03T14:59:36.535792Z",
          "iopub.status.idle": "2024-04-03T14:59:36.542021Z",
          "shell.execute_reply.started": "2024-04-03T14:59:36.535767Z",
          "shell.execute_reply": "2024-04-03T14:59:36.541101Z"
        },
        "trusted": true,
        "id": "VEaO16UdL7Yz",
        "outputId": "55a9d004-7f24-4240-cc3b-2cf82e6dcf5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " What are some challenges associated with data collection? \n",
            "\n",
            "Document Rank: 1, Document Index: 0\n",
            "Source Document: {'source': '/content/audio_1.txt'}\n",
            "Document: entered by somebody and then converted to electronic forms. So that would be, there would be two levels of potential sources for errors. The person writing down the information and then the person typing in the information. Now, gradually these kind of electronic forms are spilled in directly, so at least the source of the error is reduced to one step. But still, people mistype things. I mean, there are any number of situations where people type their email address wrong and so notifications don't reach them and so on. So there is this data collection. How do you collect the data and how do you clean it? And the third thing is, how do you make it uniform? So when data is being collected by different people, they may collect different things. And for instance, if you look at the government, typically the government collects data in different forms. For instance, there is a public distribution system which the ration shops, so they collect some information about who is collecting ration and who is income levels and so on. Then on the other side, you have, of course, things like electoral information. Then you have property information. Who owns property, who pays electricity bills, who owns vehicles, vehicle registration, driving licenses. Not necessarily the same thing. Not everybody who has a driving license owns a vehicle. So you have all these different sources of information. Now, is it possible really to find out same person in all these? Well, a lot depends on how the information is collected. You have a common format in which you record information about people. Even names in India tend to be spelt in different ways, written in different ways. Sometimes we expand initials, sometimes we suppress them, sometimes we write middle names, sometimes we don't. Sometimes we invert the order, sometimes we don't. Addresses, of course, are written in a million different ways. So there are all kinds of issues with just getting the data to a format where you can work on\n",
            "Relevance Score: 0.95\n",
            "\n",
            "\n",
            "Document Rank: 2, Document Index: 2\n",
            "Source Document: {'source': '/content/audio_1.txt'}\n",
            "Document: is itself a kind of computationally intensive task. How do you provide a lot of well trained examples? Because the assumption is that the labels that these training samples have are somehow correct so they can be trusted. Because if you can't trust those labels, and the model that you build will obviously not work. So if you want to write generalization of the labels that people have given you, you have to trust those labels. So that means somebody reasonably conversant with the domain has to be willing to put in the time to label it. Now, if it happens, as I said, because historically you're collecting data, it's fine. But more and more, we are finding that this is not the only source of data. A lot of data now actually comes out of automatic things like we have systems which are producing. For example, if you run a computerized device like a network switch or something like that, all these things generate a lot of diagnostic data. Even things like cars and planes and vehicles, which run with a lot of electronic components, generate a lot of diagnostic data. That's why whenever there's a crash, for instance, people are looking for the same as black box. Now, if you're trying to build a model based on that, then who is going to sit through and go through all this diagnostic data and tell you that, oh, this part of the data suggests that there's a fault here, and that part of the data suggests that there's a fault there.\n",
            "Relevance Score: 0.51\n",
            "\n",
            "\n",
            "Document Rank: 3, Document Index: 1\n",
            "Source Document: {'source': '/content/audio_3.txt'}\n",
            "Document: So clearly there is something going on in the background, and that background in our case is revealed through a test. Even tests have errors and all that. So it's really a problem that we have to grapple with in real life. And we will just assume for now that when we reach such a situation, we just take a majority decision. But realistically, if we get too many of these situations, and it's very difficult to decide, we have to ask whether the data has actually been collected correctly. So as we said, the main issue is really how to ask the questions and what this means. Sorry, sir, I have a question. Yeah. Sir, once we know that the data is young, we are making a prediction and it come out that young. So like other attribute is not relevant other than it has job or not, and then we are not going to look at other attributes. Yeah, but remember that which attributes to look like vary from one path to another. So maybe in this case, right, so what we are saying is that if they are young, we only need the job, right? But if they are middle aged, we need to know the house. So for different combinations, so they may not occur together. I agree, in any decision, but each of these plays a role somewhere. Now, if there is another attribute which never plays a role, as we will see the next tree as that example. Right? So if you look at this, this is a different tree for the same data set. So instead of first asking the age, we ask whether you own a house or not. And it actually turns out that everybody in this data set who owns a house, six. Out of those, remember we said there were nine yeses and six no's overall, but out of those nine yeses, six of them own a house and all six of them get yes. And nobody owns a house who's told no. And if you don't own a house, then if you have a job, you get it. If you don't have a job. So in some sense, if you don't have a job and you don't own a house, you're not going to get a loan. If you have either of the other two, you're going to\n",
            "Relevance Score: 0.32\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 2"
      ],
      "metadata": {
        "id": "NpvCAW1bk0Ay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_query2=[]\n",
        "for i in range(len(retrieved_docs_2)):\n",
        "  retrieved_docs_query2.append(retrieved_docs_2[i].page_content)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T15:00:08.730939Z",
          "iopub.execute_input": "2024-04-03T15:00:08.732032Z",
          "iopub.status.idle": "2024-04-03T15:00:08.737307Z",
          "shell.execute_reply.started": "2024-04-03T15:00:08.731982Z",
          "shell.execute_reply": "2024-04-03T15:00:08.736223Z"
        },
        "trusted": true,
        "id": "oVI5i5C2L7Yz"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(query_2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T20:45:03.266653Z",
          "iopub.execute_input": "2024-04-01T20:45:03.267046Z",
          "iopub.status.idle": "2024-04-01T20:45:03.271865Z",
          "shell.execute_reply.started": "2024-04-01T20:45:03.267018Z",
          "shell.execute_reply": "2024-04-01T20:45:03.270886Z"
        },
        "trusted": true,
        "id": "9Ck7LvhLL7Y0",
        "outputId": "b8a5fda4-80d2-4e40-dae9-9f66bab9d76f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " What are the advantages of the Apriori algorithm?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = co.rerank(query=query_2, documents=retrieved_docs_query2, top_n=3, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
        "print(query_2,\"\\n\")\n",
        "for idx, r in enumerate(results.results):\n",
        "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
        "  print(f\"Source Document: {retrieved_docs_2[r.index].metadata}\")\n",
        "  print(f\"Document: {retrieved_docs_query2[r.index]}\")\n",
        "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T15:00:28.736467Z",
          "iopub.execute_input": "2024-04-03T15:00:28.736837Z",
          "iopub.status.idle": "2024-04-03T15:00:28.890754Z",
          "shell.execute_reply.started": "2024-04-03T15:00:28.736808Z",
          "shell.execute_reply": "2024-04-03T15:00:28.889857Z"
        },
        "trusted": true,
        "id": "YO-yBxajL7Y0",
        "outputId": "b9c6dec9-c13f-4356-aca8-286933ca2144",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " What are the advantages of the Apriori algorithm? \n",
            "\n",
            "Document Rank: 1, Document Index: 0\n",
            "Source Document: {'source': '/content/audio_4.txt'}\n",
            "Document: on some attributes. You might, for example, if it is, say, some medical diagnosis, then you may not be very concerned about the date of birth. I mean the year of birth, yes, but you may not need to know whether it's January 27 or May 30. It may not make much difference. So there are some things which may be less or more relevant, or even place of birth may not be so relevant, and so on. So you might automatically throw out some. But in general, we don't know. So in general, we are running all these things. So the algorithm works just looking at the values without asking what does a one mean and what does a five mean. So this is something to keep in mind. So our strategy is to maximize the reduction in the impurity. And this terminology comes from that entropy interpretation. So in entropy information theory, entropy is loss of information. If you have more entropy, you have less information. If you have more information, your entropy reduces. Right. I discussed that if you know the message for certain, you have full information about what the message contains in advance. You don't even need to send the contents, you just need to say there is a message, like a missed call, and you would know what the message means. So, information is entropy going down is the same as information going up. So this is sometimes called maximizing. Reduction in entropy is sometimes also referred to as maximizing the information gain. But because this is an uninterpreted kind of algorithm, that is, we are just looking at the attributes and comparing the information gain without asking what the attributes really signify, we are just taking the one that works best. So the problem is that there could be some attributes which give information gain but have no significance. And a typical example of this is something which is like a serial number, right? So imagine that you have something like Aadhaar number or passport number, or in a class, you have distinct roll numbers. Every student has a\n",
            "Relevance Score: 0.51\n",
            "\n",
            "\n",
            "Document Rank: 2, Document Index: 2\n",
            "Source Document: {'source': '/content/audio_2.txt'}\n",
            "Document: When I write in canonical order, it has one fixed order, I one to ik. So if I drop ik, I get one set of size, k minus one. If I drop I k minus one, the second last element, I get a different subset, and if I merge those two, I get back this set. So if I have this set which y which is supposed to be in Ck, then it will actually be got by merging back these two subsets of y. So for every y that should be in Ck, it's going to be in Ck prime. Now I'm also going to get things in Ck prime which are not in Ck because they merge correctly. But there may be some other k minus one subset which is not frequent and which I'm not checking. So there will be some spurious entries in Ck prime, some things which should not be frequent because there is actually a known k minus one set which is not frequent. But I don't bother. I just am interested in saying that Ck prime is good enough for me. So the main advantage of this is that it's relatively speaking, given that our assumption is at every level, this fks are small. This is much faster than going to the bigger set and then filtering down, building up the bigger set from the smaller set. Even if we over approximate is going to be much faster and it's not going to be too much, because we know that the smaller sets are actually quite small. So, so the, if you want to think about how to do this algorithmically. So what we have is we have our set f k minus one. So supposing we just sort it in dictionary order, right? We think of these as k minus one tuples, and we sort it in dictionary order. Meaning that if the first position where two entries differ, if one is smaller than the other one, then that tuple is smaller. The first letter in the word which differs is smaller. So that's a dictionary. So what I will have is I will have a bunch of things where I have the same I one, two, I k minus one, and then I have a different j one. Jk, jk. So I will have a block of things at the beginning of my sorted order, where the first k minus one\n",
            "Relevance Score: 0.35\n",
            "\n",
            "\n",
            "Document Rank: 3, Document Index: 1\n",
            "Source Document: {'source': '/content/audio_2.txt'}\n",
            "Document: I'm not going to formalize this anymore, but it's very similar in idea to the prior year. So you can formalize this algorithm for yourself, but it's essentially saying the same thing, except here the thing is constraint, because we know that z is fixed. So once z is fixed, then we just have to start with each individual element in z as a possible right hand side, and then work forwards to that. We don't have to look at all. So for each z, we do this calculation within that set. So that's how you generate association rules once you have a frequent set. So within that set, you play this a priority game with respect to the size of the right hand side, and then you generate all the rules. Is this clear? Yes. So it's exactly the same as the previous one, just a slightly different presentation of the idea. Okay, so now, as I said, this whole thing so far has been largely a kind of algorithmic problem, right? In some sense, if I fix the set of items and the set of transactions and the set of the two thresholds, the support and the confidence threshold, then for any x, comma y, either it is true or it is not true. So there is a fixed answer. So the answer is known. So this is something where I can actually validate that you have either enumerated all the association rules or you have missed out something. So technically there is no learning involved. It's more an algorithmic question. So how does one connect this to learning? Well, in specific context, you can interpret these rules as classification rules. So, remember we said supervised learning says, if this is there, then it is a fraud. If this thing is not there, then it is not a fraud, and so on. So, let's look at a typical example. So, supposing we have some question which is about labeling the topic of a document. So our items now are words, and our baskets are collections of words, sets of words. So for each document, we have one word which describes its topic. So here is an example. On the right. So the topic\n",
            "Relevance Score: 0.20\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 3"
      ],
      "metadata": {
        "id": "uNICmSrkk5gB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_query3=[]\n",
        "for i in range(len(retrieved_docs_3)):\n",
        "  retrieved_docs_query3.append(retrieved_docs_3[i].page_content)\n",
        "\n",
        "print(query_3,\"\\n\")\n",
        "results = co.rerank(query=query_3, documents=retrieved_docs_query3, top_n=3, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
        "for idx, r in enumerate(results.results):\n",
        "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
        "  print(f\"Source Document: {retrieved_docs_3[r.index].metadata}\")\n",
        "  print(f\"Document: {retrieved_docs_query3[r.index]}\")\n",
        "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T15:00:56.234251Z",
          "iopub.execute_input": "2024-04-03T15:00:56.234905Z",
          "iopub.status.idle": "2024-04-03T15:00:56.379265Z",
          "shell.execute_reply.started": "2024-04-03T15:00:56.234873Z",
          "shell.execute_reply": "2024-04-03T15:00:56.378236Z"
        },
        "trusted": true,
        "id": "muzHnafgL7Y0",
        "outputId": "1c333185-69ef-46b4-ad3f-3671fdeddb3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " What distinguishes training data in supervised learning, and what is its purpose?  \n",
            "\n",
            "Document Rank: 1, Document Index: 1\n",
            "Source Document: {'source': '/content/audio_3.txt'}\n",
            "Document: So we were looking at this market basket analysis, and towards the end of the discussion on association rule rules, if everybody who buys X is also likely to buy y. We said that one application of this association rule idea is to have these so called class association rules. So the right hand side was a category. We looked at an example with documents and we said, if a document has these words, it is also likely to belong to this category. So that brings us to the topic that is one of the central ones in machine learning, which is the idea of supervised learning. So in supervised learning, we have a set of items, and each item belongs to some category or class, and we distinguish these items in terms of some attributes. So you can think about this, as we will see shortly, you can think about these attributes as columns, right? So the attributes will be things like name, age, date of birth, whatever, income, stuff like that. So each row in the table will be one item, and there will be a special column which indicates what category, depending on the classification problem that we are dealing with, what category it is. And the goal of supervised learning is to now construct, given a new row, a new item, a new combination of these attributes, which is not seen before, try to predict what would be the most appropriate category to assign. So these items which are given with the labels. So these are called labels. So these items are called training data. So we have labeled training data, which consists of some information which has been collected in the past, some historical information about items and their classification. And we want to find a model that generalizes the training data. So since normally this categorization corresponds to some kind of classification, this is typically called a classification problem. So classification problem could be a binary problem. It could be, yes. No. This mail is junk. This mail is not junk. It could also be multi way, especially\n",
            "Relevance Score: 0.98\n",
            "\n",
            "\n",
            "Document Rank: 2, Document Index: 3\n",
            "Source Document: {'source': '/content/audio_1.txt'}\n",
            "Document: and make the best line out of it. So it's not quite generating a program from a specification, but generating a concrete program from a generic program you can think of a model template as. So I just give you a line here which says a Y equal to Mx plus c. And here I want a specific m zero and C zero, which fits the given data in the best possible way, which is also part of the description of the algorithm. What does it mean for one line to be better than another line? So that's part of it. So the other side of this picture, as I mentioned, is when you don't have this training data, you don't have any examples that people have already labeled, but you are looking for some kind of patterns. So there is no guidance. So the thing about supervised learning is somebody should have gone through and given you this information before. Now, this information could have been collected over time. Maybe it's not manually collected, like the school marks. The school marks over time. Everybody has. I mean, the school keeps a record of what happens in the model exams, what happens in the board exam. So it comes automatically. The bank will be keeping records of all the loans that it bought and what happened to the customers who bought loans. So this historical information is accumulated over time. On the other hand, in some situations, like the example I mentioned about classifying paintings, now, when you go into a museum or you go into some art gallery or something, the painting doesn't have a label on it, saying, this is a painting of this type and this is a painting of that type. So somebody, if you want to fit this training model to it, somebody has to sit and actually label these things for the algorithm. So there is a lot of work in it. So generating a lot of valid training data is itself a kind of computationally intensive task. How do you provide a lot of well trained examples? Because the assumption is that the labels that these training samples have are somehow correct\n",
            "Relevance Score: 0.96\n",
            "\n",
            "\n",
            "Document Rank: 3, Document Index: 0\n",
            "Source Document: {'source': '/content/audio_3.txt'}\n",
            "Document: to the way the data is available. So what we are going to be doing is agnostic to the domain. So we are not going to believe anything or assume anything about the domain, except that we will want to validate at the end somehow that what we are doing is right. But in practice what you're saying makes sense that really to do supervised learning, it's much more realistic to do it in collaboration with somebody who understands a domain rather than just doing it blindly. Because also when you do it blindly, you might come up with solutions which make no sense. You might actually identify features which contribute to a decision purely by statistical coincidence, and no human would actually do that. So there's another issue which comes up of interpretability or explainability of your answers. So there are various factors which get into this. It's only not only accuracy which matters, but also whether your model is sensible in something, so that you can justify it. So these will all come as we go along. So let's proceed with this. So the first thing is that because the choice of attribute to question will determine the structure of the tree. So here are two different trees. So this is the tree we started with, and in a sense this tree, if you remember that table, consists of asking the first column first. So the first column in our table was age. So we asked age, and then maybe we adapted our question based on age, whereas here we have directly jumped to own a house. The question to ask is which of these trees intuitively is better secondary? What is the definition of better over here, a shorter tree or a tree which predicts better? So here both trees are predicting. Exactly. So there is no mistake, at least with respect to the training data. Of course we have not seen what it does with the unknown data, but if you are just looking at its behavior on the training data, all of these things reach nodes in which I don't have any confusion, any majority to choose. So in that\n",
            "Relevance Score: 0.80\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 4"
      ],
      "metadata": {
        "id": "y6ngiuAKk-L1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_query4=[]\n",
        "for i in range(len(retrieved_docs_4)):\n",
        "  retrieved_docs_query4.append(retrieved_docs_4[i].page_content)\n",
        "\n",
        "print(query_4,\"\\n\")\n",
        "results = co.rerank(query=query_4, documents=retrieved_docs_query4, top_n=3, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
        "for idx, r in enumerate(results.results):\n",
        "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
        "  print(f\"Source Document: {retrieved_docs_4[r.index].metadata}\")\n",
        "  print(f\"Document: {retrieved_docs_query4[r.index]}\")\n",
        "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T15:01:05.873434Z",
          "iopub.execute_input": "2024-04-03T15:01:05.874046Z",
          "iopub.status.idle": "2024-04-03T15:01:05.997986Z",
          "shell.execute_reply.started": "2024-04-03T15:01:05.874014Z",
          "shell.execute_reply": "2024-04-03T15:01:05.997199Z"
        },
        "trusted": true,
        "id": "edbCuQieL7Y1",
        "outputId": "5fc55749-860f-4f5b-ed56-9196c474fb19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " What is cross-validation? \n",
            "\n",
            "Document Rank: 1, Document Index: 2\n",
            "Source Document: {'source': '/content/audio_4.txt'}\n",
            "Document: So what you're really asking at some higher level is that machine learning approach. So, remember, we have seen only decision trees, but we are going to see many models. So there are clearly many ways to build models. And the reason, whenever you see that there are many ways to do something, it's only because there is no guarantee that a given way is the best one. So more or less what you want to validate is, I have, say, seven different strategies to solve this problem using machine learning. And I want to check which one of them is good. So I need to compare. So once I compare, then I'm stuck with whatever I have. It may be that this gives me only 70% accuracy or some such metric, but the other models give me even less. So it's more really a comparative statement to start with, which is, is this method actually giving me something reasonable? And is it giving me better something than something else? So, for this one strategy is so called cross validation. So what you do is you take some fraction, say, in this case, it is one fifth. So 20%. So you take your data and you leave out. So you randomize it into five groups, which preserve, in some sense, the random behavior, so they partition it five ways. So if I leave out any one of these groups, it's as though I have taken the remaining 80% for training and this 20% for test. But because I don't want to actually think of it this way, I will actually keep doing this for each one. So I will leave out the first 20%, then I will leave out the second 20%. So I will do this. If I have like k chunks, I will do this k times in each chunk. Each model I will build using all but one of these chunks. So this then will solve this problem of many problems. It will solve the problem where you don't have enough data to begin with to afford to not look at some of it, because the interesting things may be in that sum of it. Now, what will you do with this whole thing? Well, you have five. Typically, each of these models are going to\n",
            "Relevance Score: 0.96\n",
            "\n",
            "\n",
            "Document Rank: 2, Document Index: 3\n",
            "Source Document: {'source': '/content/audio_4.txt'}\n",
            "Document: to not look at some of it, because the interesting things may be in that sum of it. Now, what will you do with this whole thing? Well, you have five. Typically, each of these models are going to be different because we saw with the decision tree, it's kind of the questions that you ask depend on the distribution of the different roles. And if the distribution changes slightly, one attribute might get better entropy than another attribute. And you will choose that question first. So the shape of the tree might vary a lot between these, so you may not get the same. Okay, so the question about cross validation, cross validation. What I'm saying is that you do not make a hard and fast choice once and for all to split your data this way. So you're taking your data and you're saying, hey, first let me leave out this 20% and build one model. So I am building multiple models. I'm building m one, m two, m three, m four, and m five. Each of these models is built using 80% of the test data, but a different 80%. So each of them leaves out some 20%. So I'm building five different models and comparing what I see in all these five. So the first thing is that if this approach is good, then in general, these five models should behave in a similar way with respect to the outcome. So if they all behave similarly, then I can decide what to do. So there are two strategies. I can, I can somehow combine these by saying, build a composite model, or I can say, okay, this strategy is working for me. So this approach that I have chosen seems to be a good way to build a model for this data. So now let me go back and take the 100% data and build from this whole thing, build a new model M, which will be hopefully better than any of these. M one, m two, m three, m four, m five. But I built those five models to allow myself to know whether this strategy I'm using is good or not. So once I've decided I have a good strategy, then I can do something to make the best possible use of the data to build\n",
            "Relevance Score: 0.93\n",
            "\n",
            "\n",
            "Document Rank: 3, Document Index: 0\n",
            "Source Document: {'source': '/content/audio_5.txt'}\n",
            "Document: is important is remember that we talked about this cross validation. So what do we do in cross validation or even in normal validation? In normal validation or in cross validation. In either case, you pull out some random data, set data points from your input and build a tree on the rest. Now the question is, the model that you built from the rest, is it a stable structure in the sense that is it sensitive to how you chose your training and test data? And this example suggests that you are likely to get different models from different subsets of training data. So you then have to worry about what is your final model going to be. So if you test it on a particular test set, then maybe if you test it on a different test set, you might get an equivalent model, but it might have a very different structure, and maybe for other reasons, that structure might be more natural. So this is a generic problem with this kind of tree model, that it is very sensitive to perturbations in the data. We will look at a more familiar problem, which you are generally aware of called regression. Right? So in regression, what do we do? We take a bunch of points, we try to fit a line to it. Now, if I take one point in that set and remove it, or if I shift it a little bit, the line will perhaps change, but it will change in a very minor way, right? The slope will change slightly. So small changes in input produce small changes in output. So that is a kind of stable kind of situation. So this is something called variance. So how much variance is there in your model? So, variance is basically a property which says, is it the case? So if you have low variance, then small changes in input produce small changes in output. But decision tree, by definition, is a kind of discrete kind of thing. There's no way to slightly change a tree. You change a question and the tree changes drastically. So it has high variance, whereas a line fitting kind of algorithm will have a low variance, because if you\n",
            "Relevance Score: 0.89\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 5"
      ],
      "metadata": {
        "id": "hdxOk36plB3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_query5=[]\n",
        "for i in range(len(retrieved_docs_5)):\n",
        "  retrieved_docs_query5.append(retrieved_docs_5[i].page_content)\n",
        "\n",
        "print(query_5,\"\\n\")\n",
        "results = co.rerank(query=query_5, documents=retrieved_docs_query5, top_n=3, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
        "for idx, r in enumerate(results.results):\n",
        "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
        "  print(f\"Source Document: {retrieved_docs_5[r.index].metadata}\")\n",
        "  print(f\"Document: {retrieved_docs_query5[r.index]}\")\n",
        "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T15:01:17.034860Z",
          "iopub.execute_input": "2024-04-03T15:01:17.035533Z",
          "iopub.status.idle": "2024-04-03T15:01:17.192243Z",
          "shell.execute_reply.started": "2024-04-03T15:01:17.035502Z",
          "shell.execute_reply": "2024-04-03T15:01:17.191300Z"
        },
        "trusted": true,
        "id": "7-sHgtpoL7Y2",
        "outputId": "28e67859-641a-4a22-af8b-885864d8cbe0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the process of building the decision tree classifier, and how is it trained on the dataset? \n",
            "\n",
            "Document Rank: 1, Document Index: 0\n",
            "Source Document: {'source': '/content/audio_5.txt'}\n",
            "Document: build a decision tree normally until we either achieve some pure leaf nodes where the classification is uniform, or we run out of attributes. Now this is a numerical case, so we are not going to run out of attributes because we are going to run out eventually when we run out of data points. But we will end up with these very long slicing, the width between 1.5 and 1.71.5 and 1.6 and so on. So what you can do is you can tell the decision C classifier not to expand beyond a certain depth. And here it has fixed it to be depth too. So we will come back to this at a later lecture, which is, this is related to this generalization question. That is, if you build a very detailed tree that asks a lot of questions, then you start asking questions which are very specific to the training data and you end up with what is called an overfitted model, a model which is following your training data too closely and is therefore picking up some peculiarities which don't necessarily exist within. So one of the things we mentioned in passing was that we like short trees for two reasons. One is because they are easier to explain. The second thing, which I claim without any justification, is that they generalize better. So here, priority, they are saying, let us not construct deep trees. So whatever tree we construct, we are going to stop it when it reaches. So it's a two step process, right? So what you do is you first say what are the parameters for the decision tree classifier? So he says, I want a decision tree classifier to be set up with this random state 42 and which will not grow to more than depth two. And then I have to actually construct the classifier for a particular data set. So that's the next thing. So I use this fit function, right? In some sense this creates a decision tree object with certain operating parameters, and then you pass it the training data in the form of the input and the output x and y, and say fit. So the important thing to see is that this is all you\n",
            "Relevance Score: 0.99\n",
            "\n",
            "\n",
            "Document Rank: 2, Document Index: 2\n",
            "Source Document: {'source': '/content/audio_5.txt'}\n",
            "Document: so now this is the real part. So, so Scikitlearn has some built in data sets and one of them is this iris library. So the way to get the iris library into your data set is to import this function called load iris. And Scikitlearn has a bunch of models predefined. So what we have been looking at is decision trees as classifiers. So we are going to import from the subset of models of form tree. We are going to import this model called decision tree classifier. So the first thing that we do is we actually load this data set. So we call this load iris function, and then we are going to remember, the iris data set had two measures. It had the petal and the sepal, length and width. So it had four quantities. But we decided we will only use two quantities. So this is what this is doing. It's taking this iris data set and it is looking at essentially columns, indices. Two and three are the siple and two, three are the petal. So it's taking all the rows and it's taking the third and fourth columns. So we're throwing away two columns. So we are making it, instead of a four column array, we are making it into a two column array. And in this data. So when you load the data set, it automatically produces these two subparts, iris data and iris target. So target is the classification variable. Remember, in this case, the classification variable is a three way thing, right? There are three types of irises that we saw, so it will be essentially a number, zero, one. And now to get this decision tree classifier, we have to take this function that we have imported. So we say take a decision tree classifier. We have to provide it with, again some random state because it uses some randomization inside it. And now this is something which we have not talked about. So we discussed that we will build a decision tree normally until we either achieve some pure leaf nodes where the classification is uniform, or we run out of attributes. Now this is a numerical case, so we are not going to run\n",
            "Relevance Score: 0.99\n",
            "\n",
            "\n",
            "Document Rank: 3, Document Index: 3\n",
            "Source Document: {'source': '/content/audio_4.txt'}\n",
            "Document: So last time we started with supervised learning in earnest, and we were looking at this decision tree model. So, in a decision tree model, you have essentially a table in which the columns are the. So each item, a row in the table and one special column denotes the class that you are trying to classify. And we assumed it's a binary classification. So the class is zero, one, or yes, no, or whatever you want to interpret. The two categories are. So the algorithm was very simple. At a high level, what you want to do is choose an attribute based on the answer to the question that you're asking about that attribute. The data splits, and then you take each partition, ask another question. So that's the thing. And you want to come down and keep asking questions until either you have reached, like these situations here, a partition in which all the values of the class variable are the same. So you have a uniform class level or you have run out of questions, because we can only ask in any path one question per attribute. And if in case we have the second case where we run out of questions, but we have not reached a so called uniform node, then we just use the majority as a prediction. So we will see an example of this later as we go. Yeah, so what we also said was that, as we can see here, we have two different trees, and the trees are not the same shape. And we argued that a smaller tree is probably a more desirable one. It's more explainable. And we also claim, without at the moment justifying, that it also has a better generalization property. That is, it will move from the specific training data to the unknown data in a better way. So how do we build small trees? Well, computationally, we said, I claimed at least that it was infeasible to build the smallest tree. So you would have to do a search through all possible trees. So instead we wanted to use some heuristic. So the main question that we have to ask is, which attribute do we query next at any stage. So our goal\n",
            "Relevance Score: 0.94\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 6"
      ],
      "metadata": {
        "id": "nfXPH-yulIWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_query6=[]\n",
        "for i in range(len(retrieved_docs_6)):\n",
        "  retrieved_docs_query6.append(retrieved_docs_6[i].page_content)\n",
        "\n",
        "print(query_6,\"\\n\")\n",
        "results = co.rerank(query=query_6, documents=retrieved_docs_query6, top_n=3, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
        "for idx, r in enumerate(results.results):\n",
        "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
        "  print(f\"Source Document: {retrieved_docs_6[r.index].metadata}\")\n",
        "  print(f\"Document: {retrieved_docs_query6[r.index]}\")\n",
        "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T15:01:17.034860Z",
          "iopub.execute_input": "2024-04-03T15:01:17.035533Z",
          "iopub.status.idle": "2024-04-03T15:01:17.192243Z",
          "shell.execute_reply.started": "2024-04-03T15:01:17.035502Z",
          "shell.execute_reply": "2024-04-03T15:01:17.191300Z"
        },
        "trusted": true,
        "outputId": "06f52e10-c6f4-4246-b5bf-59aa8cc1ba0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whnTycKtlIWb"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the challenge associated with supervised learning? \n",
            "\n",
            "Document Rank: 1, Document Index: 0\n",
            "Source Document: {'source': '/content/audio_3.txt'}\n",
            "Document: to the way the data is available. So what we are going to be doing is agnostic to the domain. So we are not going to believe anything or assume anything about the domain, except that we will want to validate at the end somehow that what we are doing is right. But in practice what you're saying makes sense that really to do supervised learning, it's much more realistic to do it in collaboration with somebody who understands a domain rather than just doing it blindly. Because also when you do it blindly, you might come up with solutions which make no sense. You might actually identify features which contribute to a decision purely by statistical coincidence, and no human would actually do that. So there's another issue which comes up of interpretability or explainability of your answers. So there are various factors which get into this. It's only not only accuracy which matters, but also whether your model is sensible in something, so that you can justify it. So these will all come as we go along. So let's proceed with this. So the first thing is that because the choice of attribute to question will determine the structure of the tree. So here are two different trees. So this is the tree we started with, and in a sense this tree, if you remember that table, consists of asking the first column first. So the first column in our table was age. So we asked age, and then maybe we adapted our question based on age, whereas here we have directly jumped to own a house. The question to ask is which of these trees intuitively is better secondary? What is the definition of better over here, a shorter tree or a tree which predicts better? So here both trees are predicting. Exactly. So there is no mistake, at least with respect to the training data. Of course we have not seen what it does with the unknown data, but if you are just looking at its behavior on the training data, all of these things reach nodes in which I don't have any confusion, any majority to choose. So in that\n",
            "Relevance Score: 0.89\n",
            "\n",
            "\n",
            "Document Rank: 2, Document Index: 1\n",
            "Source Document: {'source': '/content/audio_1.txt'}\n",
            "Document: I have to decide between which bands it is, green, and which bands it is, this yellow color. So the peril of supervised unsupervised learning is that you don't know that the step that you are taking in advance, you don't know is going to be good or bad because it's unsupervised. You're just taking a guess in some sense, but then you have to have some way of evaluating whether the guess was good or not. And then going back and doing it again may be a different. So this is kind of a quick overview, a summary of the main things that we will be looking at. So, supervised learning and unsupervised learning. So we are looking either to build a predictive model, which is broadly a classification model, or a numerical prediction model. As we said, in case you're trying to predict a number or category, the category is more like a classification problem, right? I give you something and I want you to classify it as yes, no, good, bad, or we have unsupervised learning where we are looking for some kind of structure, and usually this is in the form of clustering, but we also have to deal with these outliers, as I said. And then sometimes you want to simplify the problem by knocking off certain dimensions or unrolling the object or so. So if you look at the popular press and the popular literature, all the focus of machine learning is actually on the first one.\n",
            "Relevance Score: 0.81\n",
            "\n",
            "\n",
            "Document Rank: 3, Document Index: 2\n",
            "Source Document: {'source': '/content/audio_1.txt'}\n",
            "Document: have their income, you have their previous year's income, you have some, whether they own a house, what is the value of the house, and so on. So you have this high dimensional data in which you're trying to do something, and very often it's very hard to work because it just multiplies out into too many possibilities. So one of the things that one can do sometimes is to actually reduce the dimensions. And sometimes when you reduce the dimension, the problem becomes easier. So here is an example. So you have this kind of, imagine this is a kind of a carpet, the thing on the right, like an exercise mat or whatever, you take this carpet, which is colored like this, and then you roll it up. Now, if I give you this rolled up version on the left, then if I ask you to decide, given a point, whether it's green or yellow, it's kind of difficult, because there is a three dimensional twisting of this data, which doesn't make the picture very obvious. Whereas if I'm able to unroll it and produce this two dimensional thing, then there is a very simple line which separates the green from the other. So this is a situation where by reducing the dimension, somehow, you make the problem easier to tackle. But this is an unsupervised thing. Reducing the dimension is something that you do blindly in the hope that it gets you a benefit. So I can take another example of the same type, and I can start with something which actually, in the original thing, is separated by a simple plane, right? So I have this plane here which separates the green from the yellow. But then, believing that it'll be better to work in two dimensions, I actually unroll it. And then I find I have to deal with these multiple bands. So now it's not so obvious anymore because I do have lines, but I have many of them, and I have to decide between which bands it is, green, and which bands it is, this yellow color. So the peril of supervised unsupervised learning is that you don't know that the step that you are taking\n",
            "Relevance Score: 0.74\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 7"
      ],
      "metadata": {
        "id": "In7tbtVOlQin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_query7=[]\n",
        "for i in range(len(retrieved_docs_7)):\n",
        "  retrieved_docs_query7.append(retrieved_docs_7[i].page_content)\n",
        "\n",
        "print(query_7,\"\\n\")\n",
        "results = co.rerank(query=query_7, documents=retrieved_docs_query7, top_n=3, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
        "for idx, r in enumerate(results.results):\n",
        "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
        "  print(f\"Source Document: {retrieved_docs_7[r.index].metadata}\")\n",
        "  print(f\"Document: {retrieved_docs_query7[r.index]}\")\n",
        "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T15:01:17.034860Z",
          "iopub.execute_input": "2024-04-03T15:01:17.035533Z",
          "iopub.status.idle": "2024-04-03T15:01:17.192243Z",
          "shell.execute_reply.started": "2024-04-03T15:01:17.035502Z",
          "shell.execute_reply": "2024-04-03T15:01:17.191300Z"
        },
        "trusted": true,
        "outputId": "1e01083b-9fb0-4bb6-ee55-6d236044dee9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qH14OaclQin"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What considerations should businesses keep in mind when applying market basket analysis to understand customer behavior? \n",
            "\n",
            "Document Rank: 1, Document Index: 0\n",
            "Source Document: {'source': '/content/audio_2.txt'}\n",
            "Document: to say, as a rule, consumers who do this also do that and so on. So these are all refinements. And there are many situations where these are important and interesting, but this model as such, this kind of market basket analysis approach is very naive. And so that's why these things will come up in different contexts. But we are not going to explore it anymore in this context, because in this context, the kind of model that you build, this association rule model is rather simplistic. So we are going to look at more sophisticated models as we go along. And there you can ask the same question, but it'll take a slightly different format. Does this also include tendencies like, if there is one person who tends to buy all his groceries once a month, then they will have a bigger basket size. But for people who buy their groceries, say like per week or every other day, they will have smaller basket size, but their transactions would be spread out. So that will be treated as a different transaction every time. But for somebody who's buying everything together, doesn't that give rise to a lot of. Yeah, so there are all these situations, I agree, which are not directly addressed in this. There are many different variations of this that you could ask. So there's no doubt about that. So some of them people have looked at because they have kind of natural solutions in this. Some of them maybe you cannot do. So that's another thing about this whole model building thing is that the same model may or may not be capable of tackling every different question that you ask. So this model, some of these things maybe you can segregate and answer. But if, as you're saying, you want to compare behaviors across customers of different types, you cannot, as you said, sensibly aggregate them into a single market basket model because they all have different profiles. So you will have to, in the example that you gave, you would have to first separate out the data for these, say, the people who\n",
            "Relevance Score: 0.96\n",
            "\n",
            "\n",
            "Document Rank: 2, Document Index: 1\n",
            "Source Document: {'source': '/content/audio_1.txt'}\n",
            "Document: Maybe it is true that at certain times, people who buy diapers also buy beer, but it at least gave some confidence to the idea that you can use this kind of information to make decisions. So from that perspective, it possibly had a role to play. But anyway, so these are the origins of this problem, and it's called market basket analysis because it's really talking in this context about shopping baskets. I have a shopping basket when I reach the counter to pay. So question is, what is in that basket? If it has x, does it also have y? So this is a very literal interpretation of markets and baskets. But the notion of markets and baskets can be a little bit more abstract. So, for instance, if you are thinking about, especially nowadays, given the number of elearning platforms, there's a lot of emphasis on trying to structure topics in such a way that students who find difficulty, you can identify why they are facing difficulties. So you want to think of these concepts that you're teaching as these items. And basket is now a set of concepts. And if you look at a concept which is difficult to learn, maybe there is a connection to another concept which is also difficult to learn. So you might want to group this. So people who misunderstand a also misunderstand b. And therefore maybe the problems has to start by fixing b. And another place where this is used is in kind of similarities of documents. So if two documents share words, then you can think they are similar. So you can think of documents. Or if two words appear rather in many documents, you can think they are similar. So if you say that wherever x appears, y also appears, then it may mean that x and y are actually related concepts. So this is the context. So the real question is this, people who buy x also tend to buy y. So this is what we want to formalize and try to do a preliminary round of calculations to see how one might determine. So everything, as we said, is going to be done by first abstracting out the\n",
            "Relevance Score: 0.86\n",
            "\n",
            "\n",
            "Document Rank: 3, Document Index: 2\n",
            "Source Document: {'source': '/content/audio_2.txt'}\n",
            "Document: hand side of the rule is always a singleton, and it corresponds to a special type of item which is a category, in this case a topic. So when you have a table like this, the rows may have different number of entries. Or if you want to think about it as a table, then some columns may be blank. So not all columns are filled, but there's one last column which is filled. And now you want to now construct these rules corresponding to the first k minus one columns, implying the last column. And this now becomes something where these rules that you get out of this give you a kind of classifier. So this is a kind of a simplistic way of doing supervised learning. So that kind of brings me to the end of this basic treatment of classes, this market basket analysis. So what you're really looking for is correlations between sets of items across transactions, which we formalize in terms of these association groups. X implies y. And what we saw is that as such, combinatorially, this is a very complicated thing to calculate, given the sizes of the sets involved. But we can use this appropriate principle both to identify the sets which are interesting, those which are frequent enough to be considered, and also to decompose them into rules. So you use the approoriate twice, once to find the frequent z, and second to decompose each frequent z into possible x implies y. And using this, we can build our first simple supervised learning model. So these rules now become a very simple way of prediction. Now, if you give me a document, I will look at what words are there in the document in this previous example. And if it contains the word student and school, I will predict education. If it contains game and team, I'll predict sports.\n",
            "Relevance Score: 0.82\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 8"
      ],
      "metadata": {
        "id": "BLxdsJe_lX5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_query8=[]\n",
        "for i in range(len(retrieved_docs_8)):\n",
        "  retrieved_docs_query8.append(retrieved_docs_8[i].page_content)\n",
        "\n",
        "print(query_8,\"\\n\")\n",
        "results = co.rerank(query=query_8, documents=retrieved_docs_query8, top_n=3, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
        "for idx, r in enumerate(results.results):\n",
        "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
        "  print(f\"Source Document: {retrieved_docs_8[r.index].metadata}\")\n",
        "  print(f\"Document: {retrieved_docs_query8[r.index]}\")\n",
        "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T15:01:17.034860Z",
          "iopub.execute_input": "2024-04-03T15:01:17.035533Z",
          "iopub.status.idle": "2024-04-03T15:01:17.192243Z",
          "shell.execute_reply.started": "2024-04-03T15:01:17.035502Z",
          "shell.execute_reply": "2024-04-03T15:01:17.191300Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3Jjw5rClX5L",
        "outputId": "6e94026c-9ce0-4404-ed64-16668cbaacf6"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How does the genie index contribute to decision tree construction, and why is it favored in practice, particularly in Python libraries? \n",
            "\n",
            "Document Rank: 1, Document Index: 0\n",
            "Source Document: {'source': '/content/audio_3.txt'}\n",
            "Document: squares of numbers. So genie index is actually easier to compute. And if you look at, as we will see, libraries in Python and all that, typically they use genie index. So although there is a very famous system built using entropy, which was one of the early decision tree systems, the ones that we will practically encounter in Python, libraries actually use the genie index. Basically. I'll stop with this right now. So what we are saying now is that we want to choose the next question to evaluate the next question to ask. So for the next question to ask, we have to say something about the quality of the impurity before and after. So if I just look at the raw ratio of the impurity, I have a motion of just that impurity percentage. But that gives me this linear relationship, which apparently is not that good in practice. So I want a nonlinear way of giving some impurity value to this ratio of yes or no, and then we can use either this entropy idea or this genie index, and it gives me a sharper curve, and this sharper practice turns out to be better. So that's how you build a decision tree. You build a decision tree by applying one of these two nonlinear things. If you come up to the third thing, it's also fine. It's just that these two happen to be standard things. They are both borrowed from other areas where they have a long and established history. So, as I said, genie index comes from economics and entropy comes from information. So we'll stop here. So, any questions? It okay, if there are no questions, then we'll continue with this discussion of decision trees on Thursday when we meet next.\n",
            "Relevance Score: 1.00\n",
            "\n",
            "\n",
            "Document Rank: 2, Document Index: 1\n",
            "Source Document: {'source': '/content/audio_4.txt'}\n",
            "Document: ratio. It's not just the information gain alone, which is what we had initially. It's information gain divided by the entropy of the attribute. So is this clear what role this is playing? We are just saying that if you have a scattered attribute, then you discount the gain that you get from it by the scattering value. So more scattered attributes will, the net effect will be less because of the denominator. So in terms of our heuristics, what we said is that nonlinear functions work better in practice. And the genie index is in fact the one that is used in most libraries. But we should not blindly use this because of this problem of these scattered attributes. So we will actually use this ratio of business to correct. So we have this information. So now let's get back to this question that we initially observed. That is that something like this age, it would be much more natural if this age were described in terms of numbers. So somehow this data has been arranged so that we get this data in terms of categories. But who does the arranging? So how do we realize that in this context? Sir, I have a question, sir. In this loan example, if we add attribute like is an indian citizen or not, then the attribute has only one class, that is Indians. Suppose a bank has, in this case, the impurity is low because we have only one attribute, but it does not give us. What will happen is if you use such a category, then it will split. As what? Because if everybody is indian, for example. Yes, sir. Then after I text, if I ask the question is the person indian? Then I will get two groups. One group will have zero rows and one group will have the same set. Yes, sir. So therefore the same set will have the same entropy, right. And so the entropy will actually not change. That question will be totally redundant. Right. So I will get the one times the entropy of the table plus zero times the entropy of zero, which is just the entropy of the table that I started with. So the difference\n",
            "Relevance Score: 0.99\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 9"
      ],
      "metadata": {
        "id": "hv1pxXaala3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_query9=[]\n",
        "for i in range(len(retrieved_docs_9)):\n",
        "  retrieved_docs_query9.append(retrieved_docs_9[i].page_content)\n",
        "\n",
        "print(query_9,\"\\n\")\n",
        "results = co.rerank(query=query_9, documents=retrieved_docs_query9, top_n=3, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
        "for idx, r in enumerate(results.results):\n",
        "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
        "  print(f\"Source Document: {retrieved_docs_9[r.index].metadata}\")\n",
        "  print(f\"Document: {retrieved_docs_query9[r.index]}\")\n",
        "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T15:01:17.034860Z",
          "iopub.execute_input": "2024-04-03T15:01:17.035533Z",
          "iopub.status.idle": "2024-04-03T15:01:17.192243Z",
          "shell.execute_reply.started": "2024-04-03T15:01:17.035502Z",
          "shell.execute_reply": "2024-04-03T15:01:17.191300Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huhaQQ_Ula3y",
        "outputId": "694f8676-07e6-4bc0-f550-a57715f8d54a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the harmonic mean, and how does it enable the combination of precision and recall into a single metric? \n",
            "\n",
            "Document Rank: 1, Document Index: 0\n",
            "Source Document: {'source': '/content/audio_4.txt'}\n",
            "Document: If you want the other one go up, the first one will come down. So now I just mentioned that these are not the only things so people also talk about. So there is, for instance, the corresponding thing to precision for the negative part is specificity. Right? So what is the ratio of correct negative results compared to all negative results as a true negative? So that's with respect to this column, then accuracy, of course, is just saying how many of the correct things are there with respect to the whole thing. So that also can be measured easily from this. And then there's something called thread score and so on. So I'm not going to talk about it. And just for completeness, I will mention that sometimes people want to say that we want a single number. It's not good to give me a precision separately and a recall separately and let me choose, I want a single number. There is this thing called harmonic mean. So the harmonic mean is one upon p plus one upon R. So remember, p is precision one and it's one upon that. So it's the reciprocal of the mean of the reciprocals. So if you work this out, you get two PR on two p plus. So this is sometimes called the f score. Right? So this is a way of combining the precision score and the recall score into a single number. But as I said, in most cases, it really depends on the application. So either you want something based on the context which has higher precision or higher recall, and you choose. So these are some of the ways in which we can actually. So, remember, our starting point was that we wanted to evaluate classifiers. So we said first we have a strategy for evaluating classifiers, so we can test them by using this test set and so on. But what is it that you want to measure on the test set? So that we have this wide variety of things, but most commonly this precision and recall are the ones that people are interested in, and they will choose which one to emphasize. Depending on the context. Depending on the context, you\n",
            "Relevance Score: 0.99\n",
            "\n",
            "\n",
            "Document Rank: 2, Document Index: 2\n",
            "Source Document: {'source': '/content/audio_4.txt'}\n",
            "Document: You don't mind losing out on some good candidates for whatever reason. So depending on the nature of your screening, you might want to do again, similarly, in medical diagnosis, if you want to make everybody immunized against something or if you want to give them. So if you have a disease for which there is a simple medication, then you may not be very specific about the diagnosis. Anybody who comes with certain symptoms, this is more or less what's happening now with this omicron, right? Anybody who has a cough, cold and fever is assumed to have omicron, and you just tell them to isolate. So this is something which is high recall. You want to catch everybody who's sick and make them less of a risk to people around them. On the other hand, if it's some really critical diagnosis, it's like pancreatic cancer or something where you're going to die in six months or three months. You don't want to tell somebody that unless you're very sure, because that whole person's life will be and their family's life will be uprooted by this. So then you will have a second opinion, third opinion before you actually make the decision. So you will not just go on an initial suspicion that, oh, this looks like pancreatic cancer, and blurt it out to the patient. You'll make them both. So in many situations, it's not clear, but what is generally clear is that these two things are usually not aligned. You cannot normally do both together. You cannot have normally. If you have to trade off either you want recall or you want precision. And one will go up.\n",
            "Relevance Score: 0.94\n",
            "\n",
            "\n",
            "Document Rank: 3, Document Index: 1\n",
            "Source Document: {'source': '/content/audio_4.txt'}\n",
            "Document: of things, but most commonly this precision and recall are the ones that people are interested in, and they will choose which one to emphasize. Depending on the context. Depending on the context, you may want precision predominant. You may want recall to be predominant. So there, it really becomes a domain specific. So I'll stop there. Any questions, sir? Like, given a precision or recall value, what we need to change in the algorithm so that we could come to the certain value of precision and recall. So that is a harder question. Right. So what is it that you need to change? So that's a very difficult question. All you can say is that given what we are doing now, this is what, then it becomes really an experimental issue. So this is the hardest question in machine learning, which is, how do I improve the quality of the classifier? So there is no answer to that question, which is obvious. So you have to start experimenting. You have to do something. So there is something called feature engineering. You might want to come out with some combination of features. So some of you may have heard of this thing, for example, which is used in medicine called body mass index. So, body mass index is some combination of some formula involving height and weight. So you might have a data set in which you have something like height and weight usually classify using height and weight. It doesn't work. But if you combine it in this strange way using body mass index, then you actually get a better class. So this is an example of what is called feature engineering. You make new features out of the old one, or you might need to change your model, maybe decision tree is not good, you need to use another model. So these are all very open questions, and I don't think there is any. If there were a clear answer to that, then life would be a lot simpler. But unfortunately, that is the real challenge of machine learning. What is it? If I am given a certain measure of performance, what is it\n",
            "Relevance Score: 0.73\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query 10"
      ],
      "metadata": {
        "id": "3a48V3jildv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_query10=[]\n",
        "for i in range(len(retrieved_docs_10)):\n",
        "  retrieved_docs_query10.append(retrieved_docs_10[i].page_content)\n",
        "\n",
        "print(query_10,\"\\n\")\n",
        "results = co.rerank(query=query_10, documents=retrieved_docs_query10, top_n=3, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
        "for idx, r in enumerate(results.results):\n",
        "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
        "  print(f\"Source Document: {retrieved_docs_10[r.index].metadata}\")\n",
        "  print(f\"Document: {retrieved_docs_query10[r.index]}\")\n",
        "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T15:01:17.034860Z",
          "iopub.execute_input": "2024-04-03T15:01:17.035533Z",
          "iopub.status.idle": "2024-04-03T15:01:17.192243Z",
          "shell.execute_reply.started": "2024-04-03T15:01:17.035502Z",
          "shell.execute_reply": "2024-04-03T15:01:17.191300Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpPdKdQnldwO",
        "outputId": "3a471558-fa28-4932-c708-ba3cd443cda7"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What are the implications of choosing a large or small learning rate? \n",
            "\n",
            "Document Rank: 1, Document Index: 3\n",
            "Source Document: {'source': '/content/audio_3.txt'}\n",
            "Document: you to take a battery of tests. So you come back, I'm sure many of you have done this. So you get some medical report and you say, this is this. So you will have some, depending on how many tests you have done, you will have anything ranging from five to 15 to 20 numbers on that piece of paper which the lab has produced. Right? Now, if the doctor were to say, okay, looking at this piece of paper, I declare that you have XYZ condition. It will be very hard for you to expect to believe that. What the doctor will say is that, look at this number. Look at that number. So those two numbers are outside the range that I would normally expect, and they typically indicate something. And therefore, I believe you have. So you narrow down the suspects. So it's the same as saying that you have a complex explanation which says, test whatever. Number one is this and number two is this, and number three is this, and number 15 is this, and therefore. Or you say, number two is this and number seven is that. And therefore, in any context, generally, a smaller model is usually better, but everything is. And this is a common feature of anything we do, machine learning. Nothing is. You cannot say it universally, as I said, like newtonian mechanics and quantum mechanics, you could always have a situation where the more complicated model is required. So, yes, certainly small trees are more explainable. It'll turn out that small trees also solve the other problem that we mentioned earlier about generalization. So, question is whether or not this model is better for the data you have not seen compared to the data that you are training on. So is it too specific for the training data that you are building. So if you ask more questions, then you are really focusing on more individual eto synchronicity or complications in the data that you see. That might be an isolated case, and it may not appear in general, so maybe you should ignore it. I will show you later on an example of this. But the\n",
            "Relevance Score: 0.06\n",
            "\n",
            "\n",
            "Document Rank: 2, Document Index: 2\n",
            "Source Document: {'source': '/content/audio_4.txt'}\n",
            "Document: data is both the entire data set that you are given to start with. And then it is the specific choice that you make about how to split this data into a large part, which you will use for building the model, and a small part that you will reserve for validating the model. So depending on how much data you have, the usual practice is about 20% to 30% of it you keep aside. So you use about 70% to 80% of the data that you are given to build a model, and you keep a random subset of 20% to 30% away from the model building process. So it is unknown as far as the model or unknown data. It's the same as getting new data from the real world in some sense, and you can use it to validate. So, of course, this is a little bit of a disadvantage, because one of the features I mentioned right at the beginning is that typically machine learning expects that the model improves in quality with the amount of training data you give it. So you are in some sense artificially, or because you have no choice, more or less. You are building a suboptimal model, you have 100% data, but you are forcing yourself to use only 70% to 80% of the data available to build the model, because you need to validate it against some unknown data. So, one other strategy where you don't want to, I mean, one of the other disadvantages of doing this is that maybe that there are some minority anomalies in this data, and the choice of test data that you have made might have hidden all these anomalies, so you never see them, and maybe it's important to see them. So there may be many situations, or maybe you just don't have enough data as a whole to build a good model by only looking at 80%. So another strategy is to systematically do this with different subsets.\n",
            "Relevance Score: 0.05\n",
            "\n",
            "\n",
            "Document Rank: 3, Document Index: 0\n",
            "Source Document: {'source': '/content/audio_5.txt'}\n",
            "Document: loss and all. But you don't do it predictably, you do it randomly. So you pick up a random data point, make an adjustment based on the value at that data point and so on. So this is called stochastic gradient descent. And another way in between, which we will see later, is you take small batches. So here we batch gradient descent. You take the full. So if you have, say, 1 million inputs, you will evaluate your function on all 1 million inputs before you make one step of adjustment. In stochastic gradient descent, you will look at one input and then do it. Now, the problem with that is that that one input that you get might be one, which is kind of very extreme, and so it might give you a wild swing, and then you might oscillate a lot more. So it might take you longer to converge, but faster to compute. So there's a trade off between doing each iteration. You are making a lot of adjustments fast, but those adjustments are less controlled. The other way in between is take small batches. If you have 1 million things, you break it up into 1000 batches. So you do 1000 points. So you get some averaging out of the behavior of the function, make an adjustment, then you do another 1000, make an adjustment and so on. So all these things are actually used, we will see later in neural networks and all that. But in this linear prediction also, that's where they arise. I mean, that's the origin of this. So this is, of course, as you have all probably read. So this strategy is conventionally called linear regression, but it's basically this iterative linear prediction, which you can do theoretically by a matrix operation, but computationally, it's more feasible to do it in this. So I'll stop here, and we'll continue next time to discuss why we are using some square error and also what to do when you have nonlinear things, how to deal with nonlinear approximations. Having done decision trees, I think at the next class, at the end of the class, the last 15 have a small moodle quiz\n",
            "Relevance Score: 0.04\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}